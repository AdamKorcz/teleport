{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction What is Teleport? Gravitational Teleport is a gateway for managing access to clusters of Linux servers via SSH or the Kubernetes API. It is intended to be used instead of traditional OpenSSH for organizations that need to: Secure their infrastructure and comply with security best-practices and regulatory requirements. Have complete visibility into activity happening across their infrastructure. Reduce the operational overhead of privileged access management across both traditional and cloud-native infrastructure. Teleport Demo Teleport Explainer Video Teleport aims to be a cloud-native SSH solution, i.e. it makes it natural to think of environments, not servers. Below is a list of the most popular Teleport features: Single SSH/Kubernetes access gateway for an entire organization. SSH certificate based authentication instead of static keys. Avoid key distribution and trust on first use issues by using auto-expiring keys signed by a cluster certificate authority (CA). Enforce 2nd factor authentication. Connect to clusters located behind firewalls without direct Internet access via SSH bastions. Collaboratively troubleshoot issues through session sharing . Discover online servers and Docker containers within a cluster with dynamic node labels . A single tool (\"pane of glass\") to manage RBAC for both SSH and Kubernetes. Audit log with session recording/replay. Kubernetes audit log, including the recording of interactive commands executed via kubectl . Ability to run in \"agentless\" mode, i.e. most Teleport features are available on clusters with pre-existing SSH daemons, usually sshd . See our OpenSSH Guide Teleport is available through the free, open source edition (\"Teleport Community Edition\") or a commercial edition (\"Teleport Enterprise Edition\"). Operating System Support Teleport is officially supported on the platforms listed below. It is worth noting that the open source community has been successful in building and running Teleport on UNIX variants other than Linux [2]. Operating System Teleport Client Teleport Server Linux v2.6+ yes yes MacOS v10.12+ yes yes Windows [1] yes [1] no [1] Teleport server does not run on Windows yet, but tsh (the Teleport client) can be used on Windows to execute tsh login to retrieve a user's SSH certificate and use it with ssh , the OpenSSH client, running on a Windows client machine. [2] Teleport is written in Go and it is theoretically possible to build it on any OS supported by the Golang toolchain . Teleport Community The Community Edition is on Github if you want to dive into the code. This documentation is also available in the Github repository , so feel free to create an issue or pull request if you have comments. Quickstart Guide - A quick tutorial to show off the basic capabilities of Teleport. A good place to start if you want to jump right in. Teleport Architecture - This section covers the underlying design principles of Teleport and provides a detailed description of Teleport's architecture. A good place to learn about Teleport's design and how it works. User Manual - This manual expands on the Quickstart and provides end users with all they need to know about how to use Teleport. Admin Manual - This manual covers installation and configuration of Teleport and the ongoing management of Teleport. FAQ - Common questions about Teleport. Teleport Enterprise Teleport Enterprise is built around the open-source core in Teleport Community, with the added benefits of role-based access control (RBAC) and easy integration with identity managers for single sign-on (SSO). Because the majority of documentation between the Community and Enterprise Editions overlap, we have separated out the documentation that is specific to Teleport Enterprise. Teleport Enterprise Introduction - Overview of the additional capabilities of Teleport Enterprise. Teleport Enterprise Quick Start - A quick tutorial to show off the basic capabilities of Teleport Enterprise. A good place to start if you want to jump right in. RBAC for SSH - Details on how Teleport Enterprise provides Role-based Access Controls (RBAC) for SSH. SSO for SSH - Overview on how Teleport Enterprise works with external identity providers for single sign-on (SSO). Support and Contributing We offer a few different options for support. First, we try to provide clear and comprehensive documentation. Documentation is also available in the Github repository , so feel free to create a PR or file an issue if you think improvements can be made. If you still have questions after reviewing our docs, you can also: Join the Teleport Community to ask questions. Our engineers are available there to help you. If you want to contribute to Teleport or file a bug report/issue, you can do so by creating an issue in Github . If you are interested in Teleport Enterprise or more responsive support during a POC, we can also create a dedicated Slack channel for you during your POC. You can reach out to us through our website or email us at sales@gravitational.com to arrange for a POC. Teleport is made by Gravitational , and we hope you enjoy using it. If you have comments or questions, feel free to reach out to the Gravitational Team: info@gravitational.com .","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#what-is-teleport","text":"Gravitational Teleport is a gateway for managing access to clusters of Linux servers via SSH or the Kubernetes API. It is intended to be used instead of traditional OpenSSH for organizations that need to: Secure their infrastructure and comply with security best-practices and regulatory requirements. Have complete visibility into activity happening across their infrastructure. Reduce the operational overhead of privileged access management across both traditional and cloud-native infrastructure. Teleport Demo Teleport Explainer Video Teleport aims to be a cloud-native SSH solution, i.e. it makes it natural to think of environments, not servers. Below is a list of the most popular Teleport features: Single SSH/Kubernetes access gateway for an entire organization. SSH certificate based authentication instead of static keys. Avoid key distribution and trust on first use issues by using auto-expiring keys signed by a cluster certificate authority (CA). Enforce 2nd factor authentication. Connect to clusters located behind firewalls without direct Internet access via SSH bastions. Collaboratively troubleshoot issues through session sharing . Discover online servers and Docker containers within a cluster with dynamic node labels . A single tool (\"pane of glass\") to manage RBAC for both SSH and Kubernetes. Audit log with session recording/replay. Kubernetes audit log, including the recording of interactive commands executed via kubectl . Ability to run in \"agentless\" mode, i.e. most Teleport features are available on clusters with pre-existing SSH daemons, usually sshd . See our OpenSSH Guide Teleport is available through the free, open source edition (\"Teleport Community Edition\") or a commercial edition (\"Teleport Enterprise Edition\").","title":"What is Teleport?"},{"location":"#operating-system-support","text":"Teleport is officially supported on the platforms listed below. It is worth noting that the open source community has been successful in building and running Teleport on UNIX variants other than Linux [2]. Operating System Teleport Client Teleport Server Linux v2.6+ yes yes MacOS v10.12+ yes yes Windows [1] yes [1] no [1] Teleport server does not run on Windows yet, but tsh (the Teleport client) can be used on Windows to execute tsh login to retrieve a user's SSH certificate and use it with ssh , the OpenSSH client, running on a Windows client machine. [2] Teleport is written in Go and it is theoretically possible to build it on any OS supported by the Golang toolchain .","title":"Operating System Support"},{"location":"#teleport-community","text":"The Community Edition is on Github if you want to dive into the code. This documentation is also available in the Github repository , so feel free to create an issue or pull request if you have comments. Quickstart Guide - A quick tutorial to show off the basic capabilities of Teleport. A good place to start if you want to jump right in. Teleport Architecture - This section covers the underlying design principles of Teleport and provides a detailed description of Teleport's architecture. A good place to learn about Teleport's design and how it works. User Manual - This manual expands on the Quickstart and provides end users with all they need to know about how to use Teleport. Admin Manual - This manual covers installation and configuration of Teleport and the ongoing management of Teleport. FAQ - Common questions about Teleport.","title":"Teleport Community"},{"location":"#teleport-enterprise","text":"Teleport Enterprise is built around the open-source core in Teleport Community, with the added benefits of role-based access control (RBAC) and easy integration with identity managers for single sign-on (SSO). Because the majority of documentation between the Community and Enterprise Editions overlap, we have separated out the documentation that is specific to Teleport Enterprise. Teleport Enterprise Introduction - Overview of the additional capabilities of Teleport Enterprise. Teleport Enterprise Quick Start - A quick tutorial to show off the basic capabilities of Teleport Enterprise. A good place to start if you want to jump right in. RBAC for SSH - Details on how Teleport Enterprise provides Role-based Access Controls (RBAC) for SSH. SSO for SSH - Overview on how Teleport Enterprise works with external identity providers for single sign-on (SSO).","title":"Teleport Enterprise"},{"location":"#support-and-contributing","text":"We offer a few different options for support. First, we try to provide clear and comprehensive documentation. Documentation is also available in the Github repository , so feel free to create a PR or file an issue if you think improvements can be made. If you still have questions after reviewing our docs, you can also: Join the Teleport Community to ask questions. Our engineers are available there to help you. If you want to contribute to Teleport or file a bug report/issue, you can do so by creating an issue in Github . If you are interested in Teleport Enterprise or more responsive support during a POC, we can also create a dedicated Slack channel for you during your POC. You can reach out to us through our website or email us at sales@gravitational.com to arrange for a POC. Teleport is made by Gravitational , and we hope you enjoy using it. If you have comments or questions, feel free to reach out to the Gravitational Team: info@gravitational.com .","title":"Support and Contributing"},{"location":"admin-guide/","text":"Teleport Admin Manual This manual covers the installation and configuration of Teleport and the ongoing management of a Teleport cluster. It assumes that the reader has good understanding of Linux administration. Installing Please visit our installation page for instructions on downloading and installing Teleport. Definitions Before diving into configuring and running Teleport, it helps to take a look at the Teleport Architecture and review the key concepts this document will be referring to: Concept Description Node Synonym to \"server\" or \"computer\", something one can \"SSH to\". A node must be running the teleport daemon with \"node\" role/service turned on. Certificate Authority (CA) A pair of public/private keys Teleport uses to manage access. A CA can sign a public key of a user or node, establishing their cluster membership. Teleport Cluster A Teleport Auth Service contains two CAs. One is used to sign user keys and the other signs node keys. A collection of nodes connected to the same CA is called a \"cluster\". Cluster Name Every Teleport cluster must have a name. If a name is not supplied via teleport.yaml configuration file, a GUID will be generated. IMPORTANT: renaming a cluster invalidates its keys and all certificates it had created. Trusted Cluster Teleport Auth Service can allow 3rd party users or nodes to connect if their public keys are signed by a trusted CA. A \"trusted cluster\" is a pair of public keys of the trusted CA. It can be configured via teleport.yaml file. Teleport Daemon The Teleport daemon is called teleport and it supports the following commands: Command Description start Starts the Teleport daemon. configure Dumps a sample configuration file in YAML format into standard output. version Shows the Teleport version. status Shows the status of a Teleport connection. This command is only available from inside of an active SSH session. help Shows help. When experimenting, you can quickly start teleport with verbose logging by typing teleport start -d . WARNING Teleport stores data in /var/lib/teleport . Make sure that regular/non-admin users do not have access to this folder on the Auth server. Systemd Unit File In production, we recommend starting teleport daemon via an init system like systemd . Here's the recommended Teleport service unit file for systemd: [Unit] Description=Teleport SSH Service After=network.target [Service] Type=simple Restart=on-failure ExecStart=/usr/local/bin/teleport start --config=/etc/teleport.yaml --pid-file=/run/teleport.pid ExecReload=/bin/kill -HUP $MAINPID PIDFile=/run/teleport.pid [Install] WantedBy=multi-user.target Graceful Restarts If using the systemd service unit file above, executing systemctl reload teleport will perform a graceful restart, i.e.the Teleport daemon will fork a new process to handle new incoming requests, leaving the old daemon process running until existing clients disconnect. Version warning Graceful restarts only work if Teleport is deployed using network-based storage like DynamoDB or etcd 3.3+. Future versions of Teleport will not have this limitation. You can also perform restarts/upgrades by sending kill signals to a Teleport daemon manually. Signal Teleport Daemon Behavior USR1 Dumps diagnostics/debugging information into syslog. TERM , INT or KILL Immediate non-graceful shutdown. All existing connections will be dropped. USR2 Forks a new Teleport daemon to serve new connections. HUP Forks a new Teleport daemon to serve new connections and initiates the graceful shutdown of the existing process when there are no more clients connected to it. Ports Teleport services listen on several ports. This table shows the default port numbers. Port Service Description 3022 Node SSH port. This is Teleport's equivalent of port #22 for SSH. 3023 Proxy SSH port clients connect to. A proxy will forward this connection to port #3022 on the destination node. 3024 Proxy SSH port used to create \"reverse SSH tunnels\" from behind-firewall environments into a trusted proxy server. 3025 Auth SSH port used by the Auth Service to serve its API to other nodes in a cluster. 3080 Proxy HTTPS connection to authenticate tsh users and web users into the cluster. The same connection is used to serve a Web UI. 3026 Kubernetes Proxy HTTPS Kubernetes proxy (if enabled) Filesystem Layout By default, a Teleport node has the following files present. The location of all of them is configurable. Full path Purpose /etc/teleport.yaml Teleport configuration file (optional). /usr/local/bin/teleport Teleport daemon binary. /usr/local/bin/tctl Teleport admin tool. It is only needed for auth servers. /var/lib/teleport Teleport data directory. Nodes keep their keys and certificates there. Auth servers store the audit log and the cluster keys there, but the audit log storage can be further configured via auth_service section in the config file. Configuration You should use a configuration file to configure the teleport daemon. For simple experimentation, you can use command line flags with the teleport start command. Read about all the allowed flags in the CLI Docs or run teleport start --help Configuration File Teleport uses the YAML file format for configuration. A sample configuration file is shown below. By default, it is stored in /etc/teleport.yaml , below is an expanded and commented version from teleport configure . The default path Teleport uses to look for a config file is /etc/teleport.yaml . You can override this path and set it explicitly using the -c or --config flag to teleport start : $ teleport start --config = /etc/teleport.yaml For a complete reference, see our Configuration Reference - teleport.yaml IMPORTANT When editing YAML configuration, please pay attention to how your editor handles white space. YAML requires consistent handling of tab characters. # # Sample Teleport configuration file # Creates a single proxy, auth and node server. # # Things to update: # 1. ca_pin: Obtain the CA pin hash for joining more nodes by running 'tctl status' # on the auth server once Teleport is running. # 2. license-if-using-teleport-enterprise.pem: If you are an Enterprise customer, # obtain this from https://dashboard.gravitational.com/web/login # teleport : # nodename allows to assign an alternative name this node can be reached by. # by default it's equal to hostname nodename : NODE_NAME data_dir : /var/lib/teleport # Invitation token used to join a cluster. it is not used on # subsequent starts auth_token : xxxx-token-xxxx # Optional CA pin of the auth server. This enables more secure way of adding new # nodes to a cluster. See \"Adding Nodes\" section above. ca_pin : \"sha256:ca-pin-hash-goes-here\" # list of auth servers in a cluster. you will have more than one auth server # if you configure teleport auth to run in HA configuration. # If adding a node located behind NAT, use the Proxy URL. e.g. # auth_servers: # - teleport-proxy.example.com:3080 auth_servers : - 10.1.0.5:3025 - 10.1.0.6:3025 # Logging configuration. Possible output values to disk via '/var/lib/teleport/teleport.log', # 'stdout', 'stderr' and 'syslog'. Possible severity values are INFO, WARN # and ERROR (default). log : output : stderr severity : INFO auth_service : enabled : \"yes\" # A cluster name is used as part of a signature in certificates # generated by this CA. # # We strongly recommend to explicitly set it to something meaningful as it # becomes important when configuring trust between multiple clusters. # # By default an automatically generated name is used (not recommended) # # IMPORTANT: if you change cluster_name, it will invalidate all generated # certificates and keys (may need to wipe out /var/lib/teleport directory) cluster_name : \"teleport-aws-us-east-1\" # IP and the port to bind to. Other Teleport nodes will be connecting to # this port (AKA \"Auth API\" or \"Cluster API\") to validate client # certificates listen_addr : 0.0.0.0:3025 tokens : - proxy,node:xxxx-token-xxxx # license_file: /path/to/license-if-using-teleport-enterprise.pem authentication : # default authentication type. possible values are 'local' and 'github' for OSS # and 'oidc', 'saml' and 'false' for Enterprise. type : local # second_factor can be off, otp, or u2f second_factor : otp ssh_service : enabled : \"yes\" labels : teleport : static-label-example commands : - name : hostname command : [ /usr/bin/hostname ] period : 1m0s - name : arch command : [ /usr/bin/uname , -p ] period : 1h0m0s proxy_service : enabled : \"yes\" listen_addr : 0.0.0.0:3023 web_listen_addr : 0.0.0.0:3080 tunnel_listen_addr : 0.0.0.0:3024 # The DNS name of the proxy HTTPS endpoint as accessible by cluster users. # Defaults to the proxy's hostname if not specified. If running multiple # proxies behind a load balancer, this name must point to the load balancer # (see public_addr section below) public_addr : TELEPORT_PUBLIC_DNS_NAME:3080 # TLS certificate for the HTTPS connection. Configuring these properly is # critical for Teleport security. https_key_file : /etc/letsencrypt/live/TELEPORT_PUBLIC_DNS_NAME/privkey.pem https_cert_file : /etc/letsencrypt/live/TELEPORT_PUBLIC_DNS_NAME/fullchain.pem Public Addr Notice that all three Teleport services (proxy, auth, node) have an optional public_addr property. The public address can take an IP or a DNS name. It can also be a list of values: public_addr : [ \"proxy-one.example.com\" , \"proxy-two.example.com\" ] Specifying a public address for a Teleport service may be useful in the following use cases: You have multiple identical services, like proxies, behind a load balancer. You want Teleport to issue SSH certificate for the service with the additional principals, e.g.host names. Authentication Teleport uses the concept of \"authentication connectors\" to authenticate users when they execute tsh login command. There are three types of authentication connectors: Local Connector Local authentication is used to authenticate against a local Teleport user database. This database is managed by tctl users command. Teleport also supports second factor authentication (2FA) for the local connector. There are three possible values (types) of 2FA: otp is the default. It implements TOTP standard. You can use Google Authenticator or Authy or any other TOTP client. u2f implements U2F standard for utilizing hardware (USB) keys for second factor. You can use YubiKeys , SoloKeys or any other hardware token which implements the FIDO U2F standard. off turns off second factor authentication. Here is an example of this setting in the teleport.yaml : auth_service : authentication : type : local second_factor : off Github OAuth 2.0 Connector This connector implements Github OAuth 2.0 authentication flow. Please refer to Github documentation on Creating an OAuth App to learn how to create and register an OAuth app. Here is an example of this setting in the teleport.yaml : auth_service : authentication : type : github See Github OAuth 2.0 for details on how to configure it. SAML This connector type implements SAML authentication. It can be configured against any external identity manager like Okta or Auth0. This feature is only available for Teleport Enterprise. Here is an example of this setting in the teleport.yaml : auth_service : authentication : type : saml OIDC Teleport implements OpenID Connect (OIDC) authentication, which is similar to SAML in principle. This feature is only available for Teleport Enterprise. Here is an example of this setting in the teleport.yaml : auth_service : authentication : type : oidc Hardware Keys - YubiKey FIDO U2F Teleport supports FIDO U2F hardware keys as a second authentication factor. By default U2F is disabled. To start using U2F: Enable U2F in Teleport configuration /etc/teleport.yaml . For CLI-based logins you have to install u2f-host utility. For web-based logins you have to use Google Chrome and Firefox 67 or greater, are the only supported U2F browsers at this time. # snippet from /etc/teleport.yaml to show an example configuration of U2F: auth_service : authentication : type : local second_factor : u2f # this section is needed only if second_factor is set to 'u2f' u2f : # app_id must point to the URL of the Teleport Web UI (proxy) accessible # by the end users app_id : https://localhost:3080 # facets must list all proxy servers if there are more than one deployed facets : - https://localhost:3080 For single-proxy setups, the app_id setting can be equal to the domain name of the proxy, but this will prevent you from adding more proxies without changing the app_id . For multi-proxy setups, the app_id should be an HTTPS URL pointing to a JSON file that mirrors facets in the auth config. Warning The app_id must never change in the lifetime of the cluster. If the App ID changes, all existing U2F key registrations will become invalid and all users who use U2F as the second factor will need to re-register. When adding a new proxy server, make sure to add it to the list of \"facets\" in the configuration file, but also to the JSON file referenced by app_id Logging in with U2F For logging in via the CLI, you must first install u2f-host . Installing: # OSX: $ brew install libu2f-host # Ubuntu 16.04 LTS: $ apt-get install u2f-host Then invoke tsh ssh as usual to authenticate: $ tsh --proxy <proxy-addr> ssh <hostname> Version Warning External user identities are only supported in Teleport Enterprise . Please reach out to sales@gravitational.com for more information. Adding and Deleting Users This section covers internal user identities, i.e. user accounts created and stored in Teleport's internal storage. Most production users of Teleport use external users via Github or Okta or any other SSO provider (Teleport Enterprise supports any SAML or OIDC compliant identity provider). A user identity in Teleport exists in the scope of a cluster. The member nodes of a cluster have multiple OS users on them. A Teleport administrator creates Teleport user accounts and maps them to the allowed OS user logins they can use. Let's look at this table: Teleport User Allowed OS Logins Description joe joe, root Teleport user 'joe' can login into member nodes as OS user 'joe' or 'root' bob bob Teleport user 'bob' can login into member nodes only as OS user 'bob' ross If no OS login is specified, it defaults to the same name as the Teleport user - 'ross'. To add a new user to Teleport, you have to use the tctl tool on the same node where the auth server is running, i.e. teleport was started with --roles=auth . $ tctl users add joe joe,root Teleport generates an auto-expiring token (with a TTL of 1 hour) and prints the token URL which must be used before the TTL expires. Signup token has been created. Share this URL with the user: https://<proxy>:3080/web/newuser/xxxxxxxxxxxx NOTE: make sure the <proxy> host is accessible. The user completes registration by visiting this URL in their web browser, picking a password and configuring the 2nd factor authentication. If the credentials are correct, the auth server generates and signs a new certificate and the client stores this key and will use it for subsequent logins. The key will automatically expire after 12 hours by default after which the user will need to log back in with her credentials. This TTL can be configured to a different value. Once authenticated, the account will become visible via tctl : $ tctl users ls User Allowed Logins ---- -------------- admin admin,root ross ross joe joe,root Joe would then use the tsh client tool to log in to member node \"luna\" via bastion \"work\" as root : $ tsh --proxy = work --user = joe root@luna To delete this user: $ tctl users rm joe Editing Users Users entries can be manipulated using the generic resource commands via tctl . For example, to see the full list of user records, an administrator can execute: $ tctl get users To edit the user \"joe\": # dump the user definition into a file: $ tctl get user/joe > joe.yaml # ... edit the contents of joe.yaml # update the user record: $ tctl create -f joe.yaml Some fields in the user record are reserved for internal use. Some of them will be finalized and documented in the future versions. Fields like is_locked or traits/logins can be used starting in version 2.3 Adding Nodes to the Cluster Teleport is a \"clustered\" system, meaning it only allows access to nodes (servers) that had been previously granted cluster membership. A cluster membership means that a node receives its own host certificate signed by the cluster's auth server. To receive a host certificate upon joining a cluster, a new Teleport host must present an \"invite token\". An invite token also defines which role a new host can assume within a cluster: auth , proxy or node . There are two ways to create invitation tokens: Static Tokens are easy to use and somewhat less secure. Dynamic Tokens are more secure but require more planning. Static Tokens Static tokens are defined ahead of time by an administrator and stored in the auth server's config file: # Config section in `/etc/teleport.yaml` file for the auth server auth_service : enabled : true tokens : # This static token allows new hosts to join the cluster as \"proxy\" or \"node\" - \"proxy,node:secret-token-value\" # A token can also be stored in a file. In this example the token for adding # new auth servers is stored in /path/to/tokenfile - \"auth:/path/to/tokenfile\" Short-lived Tokens A more secure way to add nodes to a cluster is to generate tokens as they are needed. Such token can be used multiple times until its time to live (TTL) expires. Use the tctl tool to register a new invitation token (or it can also generate a new token for you). In the following example a new token is created with a TTL of 5 minutes: $ tctl nodes add --ttl = 5m --roles = node,proxy --token = secret-value The invite token: secret-value If --token is not provided, tctl will generate one: # generate a short-lived invitation token for a new node: $ tctl nodes add --ttl = 5m --roles = node,proxy The invite token: e94d68a8a1e5821dbd79d03a960644f0 # you can also list all generated non-expired tokens: $ tctl tokens ls Token Type Expiry Time --------------- ----------- --------------- e94d68a8a1e5821dbd79d03a960644f0 Node 25 Sep 18 00 :21 UTC # ... or revoke an invitation before it's used: $ tctl tokens rm e94d68a8a1e5821dbd79d03a960644f0 Using Node Invitation Tokens Both static and short-lived tokens are used the same way. Execute the following command on a new node to add it to a cluster: # adding a new regular SSH node to the cluster: $ teleport start --roles = node --token = secret-token-value --auth-server = 10 .0.10.5 # adding a new regular SSH node using Teleport Node Tunneling: $ teleport start --roles = node --token = secret-token-value --auth-server = teleport-proxy.example.com:3080 # adding a new proxy service on the cluster: $ teleport start --roles = proxy --token = secret-token-value --auth-server = 10 .0.10.5 As new nodes come online, they start sending ping requests every few seconds to the CA of the cluster. This allows users to explore cluster membership and size: $ tctl nodes ls Node Name Node ID Address Labels --------- ------- ------- ------ turing d52527f9-b260-41d0-bb5a-e23b0cfe0f8f 10 .1.0.5:3022 distro:ubuntu dijkstra c9s93fd9-3333-91d3-9999-c9s93fd98f43 10 .1.0.6:3022 distro:debian Untrusted Auth Servers Teleport nodes use the HTTPS protocol to offer the join tokens to the auth server running on 10.0.10.5 in the example above. In a zero-trust environment, you must assume that an attacker can hijack the IP address of the auth server e.g. 10.0.10.5 . To prevent this from happening, you need to supply every new node with an additional bit of information about the auth server. This technique is called \"CA Pinning\". It works by asking the auth server to produce a \"CA Pin\", which is a hashed value of its public key, i.e. for which an attacker can't forge a matching private key. On the auth server: $ tctl status Cluster staging.example.com User CA never updated Host CA never updated CA pin sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1 The \"CA pin\" at the bottom needs to be passed to the new nodes when they're starting for the first time, i.e. when they join a cluster: Via CLI: $ teleport start \\ --roles = node \\ --token = 1ac590d36493acdaa2387bc1c492db1a \\ --ca-pin = sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1 \\ --auth-server = 10 .12.0.6:3025 or via /etc/teleport.yaml on a node: teleport : auth_token : \"1ac590d36493acdaa2387bc1c492db1a\" ca_pin : \"sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1\" auth_servers : - \"10.12.0.6:3025\" Warning If a CA pin is not provided, Teleport node will join a cluster but it will print a WARN message (warning) into its standard error output. Warning The CA pin becomes invalid if a Teleport administrator performs the CA rotation by executing tctl auth rotate . Revoking Invitations As you have seen above, Teleport uses tokens to invite users to a cluster (sign-up tokens) or to add new nodes to it (provisioning tokens). Both types of tokens can be revoked before they can be used. To see a list of outstanding tokens, run this command: $ tctl tokens ls Token Role Expiry Time ( UTC ) ----- ---- ----------------- eoKoh0caiw6weoGupahgh6Wuo7jaTee2 Proxy never 696c0471453e75882ff70a761c1a8bfa Node 17 May 16 03 :51 UTC 6fc5545ab78c2ea978caabef9dbd08a5 Signup 17 May 16 04 :24 UTC In this example, the first token has a \"never\" expiry date because it is a static token configured via a config file. The 2nd token with \"Node\" role was generated to invite a new node to this cluster. And the 3rd token was generated to invite a new user. The latter two tokens can be deleted (revoked) via tctl tokens del command: $ tctl tokens del 696c0471453e75882ff70a761c1a8bfa Token 696c0471453e75882ff70a761c1a8bfa has been deleted Adding a node located behind NAT Note This feature is sometimes called \"Teleport IoT\" or node tunneling. With the current setup, you've only been able to add nodes that have direct access to the auth server and within the internal IP range of the cluster. We recommend setting up a Trusted Cluster if you have workloads split across different networks/clouds. Teleport Node Tunneling lets you add a remote node to an existing Teleport Cluster via tunnel. This can be useful for IoT applications, or for managing a couple of servers in a different network. Similar to Adding Nodes to the Cluster , use tctl to create a single-use token for a node, but this time you'll replace the auth server IP with the URL of the proxy server. In the example below, we've replaced the auth server IP with the proxy web endpoint teleport-proxy.example.com:3080 . $ sudo tctl nodes add The invite token: n92bb958ce97f761da978d08c35c54a5c Run this on the new node to join the cluster: teleport start --roles = node --token = n92bb958ce97f761da978d08c35c54a5c --auth-server = teleport-proxy.example.com:3080 Using the ports in the default configuration, the node needs to be able to talk to ports 3080 and 3024 on the proxy. Port 3080 is used to initially fetch the credentials (SSH and TLS certificates) and for discovery (where is the reverse tunnel running, in this case 3024). Port 3024 is used to establish a connection to the auth server through the proxy. To enable multiplexing so only one port is used, simply set the tunnel_listen_addr the same as the web_listen_addr respectively within the proxy_service . Teleport will automatically recognize using the same port and enable multiplexing. If the log setting is set to DEBUG you will see multiplexing enabled in the server log. DEBU [ PROC:1 ] Setup Proxy: Reverse tunnel proxy and web proxy listen on the same port, multiplexing is on. service/service.go:1944 Load Balancers The setup above also works even if the cluster uses multiple proxies behind a load balancer (LB) or a DNS entry with multiple values. This works by the node establishing a tunnel to every proxy. This requires that an LB uses round-robin or a similar balancing algorithm. Do not use sticky load balancing algorithms (a.k.a. \"session affinity\") with Teleport proxies. Labeling Nodes In addition to specifying a custom nodename, Teleport also allows for the application of arbitrary key:value pairs to each node, called labels. There are two kinds of labels: static labels do not change over time, while teleport process is running. Examples of static labels are physical location of nodes, name of the environment (staging vs production), etc. dynamic labels also known as \"label commands\" allow to generate labels at runtime. Teleport will execute an external command on a node at a configurable frequency and the output of a command becomes the label value. Examples include reporting load averages, presence of a process, time after last reboot, etc. There are two ways to configure node labels. Via command line, by using --labels flag to teleport start command. Using /etc/teleport.yaml configuration file on the nodes. To define labels as command line arguments, use --labels flag like shown below. This method works well for static labels or simple commands: $ teleport start --labels uptime =[ 1m: \"uptime -p\" ] ,kernel =[ 1h: \"uname -r\" ] Alternatively, you can update labels via a configuration file: ssh_service : enabled : \"yes\" # Static labels are simple key/value pairs: labels : environment : test To configure dynamic labels via a configuration file, define a commands array as shown below: ssh_service : enabled : \"yes\" # Dynamic labels AKA \"commands\": commands : - name : hostname command : [ hostname ] period : 1m0s - name : arch command : [ uname , -p ] # this setting tells teleport to execute the command above # once an hour. this value cannot be less than one minute. period : 1h0m0s /path/to/executable must be a valid executable command (i.e. executable bit must be set) which also includes shell scripts with a proper shebang line . Important: notice that command setting is an array where the first element is a valid executable and each subsequent element is an argument, i.e: # valid syntax: command : [ \"/bin/uname\" , \"-m\" ] # INVALID syntax: command : [ \"/bin/uname -m\" ] # if you want to pipe several bash commands together, here's how to do it: # notice how ' and \" are interchangeable and you can use it for quoting: command : [ \"/bin/sh\" , \"-c\" , \"uname -a | egrep -o '[0-9]+\\\\.[0-9]+\\\\.[0-9]+'\" ] Audit Log Teleport logs every SSH event into its audit log. There are two components of the audit log: SSH Events: Teleport logs events like successful user logins along with the metadata like remote IP address, time and the session ID. Recorded Sessions: Every SSH shell session is recorded and can be replayed later. The recording is done by the nodes themselves, by default, but can be configured to be done by the proxy. Optional: Enhanced Session Recording Refer to the \"Audit Log\" chapter in the Teleport Architecture to learn more about how the audit log and session recording are designed. SSH Events Teleport supports multiple storage back-ends for storing the SSH events. The section below uses the dir backend as an example. dir backend uses the local filesystem of an auth server using the configurable data_dir directory. For highly available (HA) configurations, users can refer to our DynamoDB or Firestore chapters for information on how to configure the SSH events and recorded sessions to be stored on network storage. It is even possible to store the audit log in multiple places at the same time - see audit_events_uri setting in the sample configuration file above for how to do that. Let's examine the Teleport audit log using the dir backend. The event log is stored in data_dir under log directory, usually /var/lib/teleport/log . Each day is represented as a file: $ ls -l /var/lib/teleport/log/ total 104 -rw-r----- 1 root root 31638 Jan 22 20 :00 2017 -01-23.00:00:00.log -rw-r----- 1 root root 91256 Jan 31 21 :00 2017 -02-01.00:00:00.log -rw-r----- 1 root root 15815 Feb 32 22 :54 2017 -02-03.00:00:00.log The log files use JSON format. They are human-readable but can also be programmatically parsed. Each line represents an event and has the following format: { // Event type. See below for the list of all possible event types \"event\" : \"session.start\" , // Teleport user name \"user\" : \"ekontsevoy\" , // OS login \"login\" : \"root\" , // Server namespace. This field is reserved for future use. \"namespace\" : \"default\" , // Unique server ID. \"server_id\" : \"f84f7386-5e22-45ff-8f7d-b8079742e63f\" , // Session ID. Can be used to replay the session. \"sid\" : \"8d3895b6-e9dd-11e6-94de-40167e68e931\" , // Address of the SSH node \"addr.local\" : \"10.5.l.15:3022\" , // Address of the connecting client (user) \"addr.remote\" : \"73.223.221.14:42146\" , // Terminal size \"size\" : \"80:25\" , // Timestamp \"time\" : \"2017-02-03T06:54:05Z\" } The possible event types are: Event Type Description auth Authentication attempt. Adds the following fields: {\"success\": \"false\", \"error\": \"access denied\"} session.start Started an interactive shell session. session.end An interactive shell session has ended. session.join A new user has joined the existing interactive shell session. session.leave A user has left the session. session.disk A list of files opened during the session. Requires Enhanced Session Recording . session.network A list of network connections made during the session. Requires Enhanced Session Recording . session.command A list of commands ran during the session. Requires Enhanced Session Recording . exec Remote command has been executed via SSH, like tsh ssh root@node ls / . The following fields will be logged: {\"command\": \"ls /\", \"exitCode\": 0, \"exitError\": \"\"} scp Remote file copy has been executed. The following fields will be logged: {\"path\": \"/path/to/file.txt\", \"len\": 32344, \"action\": \"read\" } resize Terminal has been resized. user.login A user logged into web UI or via tsh. The following fields will be logged: {\"user\": \"alice@example.com\", \"method\": \"local\"} . Recorded Sessions In addition to logging session.start and session.end events, Teleport also records the entire stream of bytes going to/from standard input and standard output of an SSH session. Teleport can store the recorded sessions in an AWS S3 bucket or in a local filesystem (including NFS). The recorded sessions are stored as raw bytes in the sessions directory under log . Each session consists of two files, both are named after the session ID: .bytes file or .chunks.gz compressed format represents the raw session bytes and is somewhat human-readable, although you are better off using tsh play or the Web UI to replay it. .log file or .events.gz compressed file contains the copies of the event log entries that are related to this session. $ ls /var/lib/teleport/log/sessions/default -rw-r----- 1 root root 506192 Feb 4 00 :46 4c146ec8-eab6-11e6-b1b3-40167e68e931.session.bytes -rw-r----- 1 root root 44943 Feb 4 00 :46 4c146ec8-eab6-11e6-b1b3-40167e68e931.session.log To replay this session via CLI: $ tsh --proxy = proxy play 4c146ec8-eab6-11e6-b1b3-40167e68e931 Resources A Teleport administrator has two tools to configure a Teleport cluster: The configuration file is used for static configuration like the cluster name. The tctl admin tool is used for manipulating dynamic records like Teleport users. tctl has convenient subcommands for dynamic configuration, like tctl users or tctl nodes . However, for dealing with more advanced topics, like connecting clusters together or troubleshooting trust, tctl offers the more powerful, although lower-level CLI interface called resources . The concept is borrowed from the REST programming pattern. A cluster is composed of different objects (aka, resources) and there are just three common operations that can be performed on them: get , create , remove . A resource is defined as a YAML file. Every resource in Teleport has three required fields: Kind - The type of resource Name - A required field in the metadata to uniquely identify the resource Version - The version of the resource format Everything else is resource-specific and any component of a Teleport cluster can be manipulated with just 3 CLI commands: Command Description Examples tctl get Get one or multiple resources tctl get users or tctl get user/joe tctl rm Delete a resource by type/name tctl rm user/joe tctl create Create a new resource from a YAML file. Use -f to override / update tctl create -f joe.yaml YAML Format By default Teleport uses YAML format to describe resources. YAML is a wonderful and very human-readable alternative to JSON or XML, but it's sensitive to white space. Pay attention to spaces vs tabs! Here's an example how the YAML resource definition for a user Joe might look like. It can be retrieved by executing tctl get user/joe kind : user version : v2 metadata : name : joe spec : roles : admin status : # users can be temporarily locked in a Teleport system, but this # functionality is reserved for internal use for now. is_locked : false lock_expires : 0001-01-01T00:00:00Z locked_time : 0001-01-01T00:00:00Z traits : # these are \"allowed logins\" which are usually specified as the # last argument to `tctl users add` logins : - joe - root # any resource in Teleport can automatically expire. expires : 0001-01-01T00:00:00Z # for internal use only created_by : time : 0001-01-01T00:00:00Z user : name : builtin-Admin Note Some of the fields you will see when printing resources are used only internally and are not meant to be changed. Others are reserved for future use. Here's the list of resources currently exposed via tctl : Resource Kind Description user A user record in the internal Teleport user DB. node A registered SSH node. The same record is displayed via tctl nodes ls cluster A trusted cluster. See here for more details on connecting clusters together. role A role assumed by users. The open source Teleport only includes one role: \"admin\", but Enterprise teleport users can define their own roles. connector Authentication connectors for single sign-on (SSO) for SAML, OIDC and Github. Examples: # list all connectors: $ tctl get connectors # dump a SAML connector called \"okta\": $ tctl get saml/okta # delete a SAML connector called \"okta\": $ tctl rm saml/okta # delete an OIDC connector called \"gsuite\": $ tctl rm oidc/gsuite # delete a github connector called \"myteam\": $ tctl rm github/myteam # delete a local user called \"admin\": $ tctl rm users/admin Note Although tctl get connectors will show you every connector, when working with an individual connector you must use the correct kind , such as saml or oidc . You can see each connector's kind at the top of its YAML output from tctl get connectors . Trusted Clusters As explained in the architecture document , Teleport can partition compute infrastructure into multiple clusters. A cluster is a group of nodes connected to the cluster's auth server, acting as a certificate authority (CA) for all users and nodes. To retrieve an SSH certificate, users must authenticate with a cluster through a proxy server. So, if users want to connect to nodes belonging to different clusters, they would normally have to use a different --proxy flag for each cluster. This is not always convenient. The concept of trusted clusters allows Teleport administrators to connect multiple clusters together and establish trust between them. Trusted clusters allow users of one cluster to seamlessly SSH into the nodes of another cluster without having to \"hop\" between proxy servers. Moreover, users don't even need to have a direct connection to other clusters' proxy servers. Trusted clusters also have their own restrictions on user access. To learn more about Trusted Clusters please visit our Trusted Cluster Guide Github OAuth 2.0 Teleport supports authentication and authorization via external identity providers such as Github. You can watch the video for how to configure Github as an SSO provider , or you can follow the documentation below. First, the Teleport auth service must be configured to use Github for authentication: # snippet from /etc/teleport.yaml auth_service : authentication : type : github Next step is to define a Github connector: # Create a file called github.yaml: kind : github version : v3 metadata : # connector name that will be used with `tsh --auth=github login` name : github spec : # client ID of Github OAuth app client_id : <client-id> # client secret of Github OAuth app client_secret : <client-secret> # connector display name that will be shown on web UI login screen display : Github # callback URL that will be called after successful authentication redirect_url : https://<proxy-address>/v1/webapi/github/callback # mapping of org/team memberships onto allowed logins and roles teams_to_logins : - organization : octocats # Github organization name team : admins # Github team name within that organization # allowed logins for users in this org/team logins : - root # List of Kubernetes groups this Github team is allowed to connect to # (see Kubernetes integration for more information) kubernetes_groups : [ \"system:masters\" ] Note For open-source Teleport the logins field contains a list of allowed OS logins. For the commercial Teleport Enterprise offering, which supports role-based access control, the same field is treated as a list of roles that users from the matching org/team assume after going through the authorization flow. To obtain client ID and client secret, please follow Github documentation on how to create and register an OAuth app . Be sure to set the \"Authorization callback URL\" to the same value as redirect_url in the resource spec. Teleport will request only the read:org OAuth scope, you can read more about Github OAuth scopes . Finally, create the connector using tctl resource management command: $ tctl create github.yaml Tip When going through the Github authentication flow for the first time, the application must be granted the access to all organizations that are present in the \"teams to logins\" mapping, otherwise Teleport will not be able to determine team memberships for these orgs. HTTP CONNECT Proxies Some networks funnel all connections through a proxy server where they can be audited and access control rules are applied. For these scenarios Teleport supports HTTP CONNECT tunneling. To use HTTP CONNECT tunneling, simply set either the HTTPS_PROXY or HTTP_PROXY environment variables and when Teleport builds and establishes the reverse tunnel to the main cluster, it will funnel all traffic though the proxy. Specifically, if using the default configuration, Teleport will tunnel ports 3024 (SSH, reverse tunnel) and 3080 (HTTPS, establishing trust) through the proxy. The value of HTTPS_PROXY or HTTP_PROXY should be in the format scheme://host:port where scheme is either https or http . If the value is host:port , Teleport will prepend http . It's important to note that in order for Teleport to use HTTP CONNECT tunnelling, the HTTP_PROXY and HTTPS_PROXY environment variables must be set within Teleport's environment. You can also optionally set the NO_PROXY environment variable to avoid use of the proxy when accessing specified hosts/netmasks. When launching Teleport with systemd, this will probably involve adding some lines to your systemd unit file: [Service] Environment=\"HTTP_PROXY=http://proxy.example.com:8080/\" Environment=\"HTTPS_PROXY=http://proxy.example.com:8080/\" Environment=\"NO_PROXY=localhost,127.0.0.1,192.168.0.0/16,172.16.0.0/12,10.0.0.0/8\" Note localhost and 127.0.0.1 are invalid values for the proxy host. If for some reason your proxy runs locally, you'll need to provide some other DNS name or a private IP address for it. PAM Integration Teleport node service can be configured to integrate with PAM . This allows Teleport to create user sessions using PAM session profiles. To enable PAM on a given Linux machine, update /etc/teleport.yaml with: teleport : ssh_service : pam : # \"no\" by default enabled : yes # use /etc/pam.d/sshd configuration (the default) service_name : \"sshd\" Please note that most Linux distributions come with a number of PAM services in /etc/pam.d and Teleport will try to use sshd by default, which will be removed if you uninstall openssh-server package. We recommend creating your own PAM service file like /etc/pam.d/teleport and specifying it as service_name above. Note Teleport only supports the account and session stack. The auth PAM module is currently not supported with Teleport. Using Teleport with OpenSSH Review our dedicated Using Teleport with OpenSSH guide. Certificate Rotation Take a look at the Certificates chapter in the architecture document to learn how the certificate rotation works. This section will show you how to implement certificate rotation in practice. The easiest way to start the rotation is to execute this command on a cluster's auth server : $ tctl auth rotate This will trigger a rotation process for both hosts and users with a grace period of 48 hours. This can be customized, i.e. # rotate only user certificates with a grace period of 200 hours: $ tctl auth rotate --type = user --grace-period = 200h # rotate only host certificates with a grace period of 8 hours: $ tctl auth rotate --type = host --grace-period = 8h The rotation takes time, especially for hosts, because each node in a cluster needs to be notified that a rotation is taking place and request a new certificate for itself before the grace period ends. Warning Be careful when choosing a grace period when rotating host certificates. The grace period needs to be long enough for all nodes in a cluster to request a new certificate. If some nodes go offline during the rotation and come back only after the grace period has ended, they will be forced to leave the cluster, i.e. users will no longer be allowed to SSH into them. To check the status of certificate rotation: $ tctl status CA Pinning Warning If you are using CA Pinning when adding new nodes, the CA pin will changes after the rotation. Make sure you use the new CA pin when adding nodes after rotation. Ansible Integration Ansible uses the OpenSSH client by default. This makes it compatible with Teleport without any extra work, except configuring OpenSSH client to work with Teleport Proxy: configure your OpenSSH to connect to Teleport proxy and use ssh-agent socket enable scp mode in the Ansible config file (default is /etc/ansible/ansible.cfg ): scp_if_ssh = True Kubernetes Integration Teleport can be configured as a compliance gateway for Kubernetes clusters. This allows users to authenticate against a Teleport proxy using tsh login command to retrieve credentials for both SSH and Kubernetes API. Follow our Kubernetes guide which contains some more specific examples and instructions. High Availability Tip Before continuing, please make sure to take a look at the Cluster State section in the Teleport Architecture documentation. Usually there are two ways to achieve high availability. You can \"outsource\" this function to the infrastructure. For example, using a highly available network-based disk volumes (similar to AWS EBS) and by migrating a failed VM to a new host. In this scenario, there's nothing Teleport-specific to be done. If high availability cannot be provided by the infrastructure (perhaps you're running Teleport on a bare metal cluster), you can still configure Teleport to run in a highly available fashion. Auth Server HA In order to run multiple instances of Teleport Auth Server, you must switch to a highly available secrets back-end first. Also, you must tell each node in a cluster that there is more than one auth server available. There are two ways to do this: Use a load balancer to create a single auth API access point (AP) and specify this AP in auth_servers section of Teleport configuration for all nodes in a cluster. This load balancer should do TCP level forwarding. If a load balancer is not an option, you must specify each instance of an auth server in auth_servers section of Teleport configuration. IMPORTANT: with multiple instances of the auth servers running, special attention needs to be paid to keeping their configuration identical. Settings like cluster_name , tokens , storage , etc must be the same. Teleport Proxy HA The Teleport Proxy is stateless which makes running multiple instances trivial. If using the default configuration , configure your load balancer to forward ports 3023 and 3080 to the servers that run the Teleport proxy. If you have configured your proxy to use non-default ports, you will need to configure your load balancer to forward the ports you specified for listen_addr and web_listen_addr in teleport.yaml . The load balancer for web_listen_addr can terminate TLS with your own certificate that is valid for your users, while the remaining ports should do TCP level forwarding, since Teleport will handle its own SSL on top of that with its own certificates. NOTE If you terminate TLS with your own certificate at a load balancer you'll need to run Teleport with --insecure-no-tls If your load balancer supports HTTP health checks, configure it to hit the /readyz diagnostics endpoint on machines running Teleport. This endpoint must be enabled by using the --diag-addr flag to teleport start: teleport start --diag-addr=127.0.0.1:3000 The http://127.0.0.1:3000/readyz endpoint will reply {\"status\":\"ok\"} if the Teleport service is running without problems. NOTE As the new auth servers get added to the cluster and the old servers get decommissioned, nodes and proxies will refresh the list of available auth servers and store it in their local cache /var/lib/teleport/authservers.json - the values from the cache file will take precedence over the configuration file. We'll cover how to use etcd , DynamoDB and Firestore storage back-ends to make Teleport highly available below. Teleport Scalability Tweaks When running Teleport at scale (for example in the case where there are 10,000+ nodes connected to a cluster via node tunnelling mode , the following settings should be set on Teleport auth and proxies: Proxy Servers These settings alter Teleport's default connection limit from 15000 to 65000. # Teleport Proxy teleport : cache : # use an in-memory cache to speed up the connection of many teleport nodes # back to proxy type : in-memory # set up connection limits to prevent throttling of many IoT nodes connecting to proxies connection_limits : max_connections : 65000 max_users : 1000 Auth Servers # Teleport Auth teleport : connection_limits : max_connections : 65000 max_users : 1000 Using etcd Teleport can use etcd as a storage backend to achieve highly available deployments. You must take steps to protect access to etcd in this configuration because that is where Teleport secrets like keys and user records will be stored. IMPORTANT etcd can only currently be used to store Teleport's internal database in a highly-available way. This will allow you to have multiple auth servers in your cluster for an HA deployment, but it will not also store Teleport audit events for you in the same way that DynamoDB or Firestore will. To configure Teleport for using etcd as a storage back-end: Make sure you are using etcd version 3.3 or newer. Install etcd and configure peer and client TLS authentication using the etcd security guide . You can use this script provided by etcd if you don't already have a TLS setup. Configure all Teleport Auth servers to use etcd in the \"storage\" section of the config file as shown below. Deploy several auth servers connected to etcd back-end. Deploy several proxy nodes that have auth_servers pointed to list of auth servers to connect to. teleport : storage : type : etcd # list of etcd peers to connect to: peers : [ \"https://172.17.0.1:4001\" , \"https://172.17.0.2:4001\" ] # required path to TLS client certificate and key files to connect to etcd # # to create these, follow # https://coreos.com/os/docs/latest/generate-self-signed-certificates.html # or use the etcd-provided script # https://github.com/etcd-io/etcd/tree/master/hack/tls-setup tls_cert_file : /var/lib/teleport/etcd-cert.pem tls_key_file : /var/lib/teleport/etcd-key.pem # optional file with trusted CA authority # file to authenticate etcd nodes # # if you used the script above to generate the client TLS certificate, # this CA certificate should be one of the other generated files tls_ca_file : /var/lib/teleport/etcd-ca.pem # alternative password based authentication, if not using TLS client # certificate # # See https://etcd.io/docs/v3.4.0/op-guide/authentication/ for setting # up a new user username : username password_file : /mnt/secrets/etcd-pass # etcd key (location) where teleport will be storing its state under. # make sure it ends with a '/'! prefix : /teleport/ # NOT RECOMMENDED: enables insecure etcd mode in which self-signed # certificate will be accepted insecure : false Using Amazon S3 Tip Before continuing, please make sure to take a look at the cluster state section in Teleport Architecture documentation. AWS Authentication The configuration examples below contain AWS access keys and secret keys. They are optional, they exist for your convenience but we DO NOT RECOMMEND using them in production. If Teleport is running on an AWS instance it will automatically use the instance IAM role. Teleport also will pick up AWS credentials from the ~/.aws folder, just like the AWS CLI tool. S3 buckets can only be used as a storage for the recorded sessions. S3 cannot store the audit log or the cluster state. Below is an example of how to configure a Teleport auth server to store the recorded sessions in an S3 bucket. teleport : storage : # The region setting sets the default AWS region for all AWS services # Teleport may consume (DynamoDB, S3) region : us-east-1 # Path to S3 bucket to store the recorded sessions in. audit_sessions_uri : \"s3://Example_TELEPORT_S3_BUCKET/records\" # Teleport assumes credentials. Using provider chains, assuming IAM role or # standard .aws/credentials in the home folder. The AWS authentication settings above can be omitted if the machine itself is running on an EC2 instance with an IAM role. Using DynamoDB Tip Before continuing, please make sure to take a look at the cluster state section in Teleport Architecture documentation. If you are running Teleport on AWS, you can use DynamoDB as a storage back-end to achieve high availability. DynamoDB back-end supports two types of Teleport data: Cluster state Audit log events DynamoDB cannot store the recorded sessions. You are advised to use AWS S3 for that as shown above. To configure Teleport to use DynamoDB: Make sure you have AWS access key and a secret key which give you access to DynamoDB account. If you're using (as recommended) an IAM role for this, the policy with necessary permissions is listed below. Configure all Teleport Auth servers to use DynamoDB back-end in the \"storage\" section of teleport.yaml as shown below. Deploy several auth servers connected to DynamoDB storage back-end. Deploy several proxy nodes. Make sure that all Teleport nodes have auth_servers configuration setting populated with the auth servers. teleport : storage : type : dynamodb # Region location of dynamodb instance, https://docs.aws.amazon.com/en_pv/general/latest/gr/rande.html#ddb_region region : us-east-1 # Name of the DynamoDB table. If it does not exist, Teleport will create it. table_name : Example_TELEPORT_DYNAMO_TABLE_NAME # This setting configures Teleport to send the audit events to three places: # To keep a copy in DynamoDB, a copy on a local filesystem, and also output the events to stdout. # NOTE: The DynamoDB events table has a different schema to the regular Teleport # database table, so attempting to use same table for both will result in errors. # When using highly available storage like DynamoDB, you should make sure that the list always specifies # the HA storage method first, as this is what the Teleport web UI uses as its source of events to display. audit_events_uri : [ 'dynamodb://events_table_name' , 'file:///var/lib/teleport/audit/events' , 'stdout://' ] # This setting configures Teleport to save the recorded sessions in an S3 bucket: audit_sessions_uri : s3://Example_TELEPORT_S3_BUCKET/records Replace us-east-1 and Example_TELEPORT_DYNAMO_TABLE_NAME with your own settings. Teleport will create the table automatically. Example_TELEPORT_DYNAMO_TABLE_NAME and events_table_name must be different DynamoDB tables. The schema is different for each. Using the same table name for both will result in errors. The AWS authentication setting above can be omitted if the machine itself is running on an EC2 instance with an IAM role. Audit log settings above are optional. If specified, Teleport will store the audit log in DynamoDB and the session recordings must be stored in an S3 bucket, i.e. both audit_xxx settings must be present. If they are not set, Teleport will default to a local file system for the audit log, i.e. /var/lib/teleport/log on an auth server. If DynamoDB is used for the audit log, the logged events will be stored with a TTL of 1 year. Currently this TTL is not configurable. Access to DynamoDB Make sure that the IAM role assigned to Teleport is configured with the sufficient access to DynamoDB. Below is the example of the IAM policy you can use: { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"AllAPIActionsOnTeleportAuth\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:eu-west-1:123456789012:table/prod.teleport.auth\" }, { \"Sid\" : \"AllAPIActionsOnTeleportStreams\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:eu-west-1:123456789012:table/prod.teleport.auth/stream/*\" } ] } Using GCS Tip Before continuing, please make sure to take a look at the cluster state section in Teleport Architecture documentation. Google Cloud Storage (GCS) can only be used as a storage for the recorded sessions. GCS cannot store the audit log or the cluster state. Below is an example of how to configure a Teleport auth server to store the recorded sessions in a GCS bucket. teleport : storage : # Path to GCS to store the recorded sessions in. audit_sessions_uri : \"gs://Example_TELEPORT_STORAGE/records\" credentials_path : /var/lib/teleport/gcs_creds Using Firestore Tip Before continuing, please make sure to take a look at the cluster state section in Teleport Architecture documentation. If you are running Teleport on GCP, you can use Firestore as a storage back-end to achieve high availability. Firestore back-end supports two types of Teleport data: Cluster state Audit log events Firestore cannot store the recorded sessions. You are advised to use Google Cloud Storage (GCS) for that as shown above. To configure Teleport to use Firestore: Configure all Teleport Auth servers to use Firestore back-end in the \"storage\" section of teleport.yaml as shown below. Deploy several auth servers connected to Firestore storage back-end. Deploy several proxy nodes. Make sure that all Teleport nodes have auth_servers configuration setting populated with the auth servers or use a load balancer for the auth servers in high availability mode. teleport : storage : type : firestore # Project ID https://support.google.com/googleapi/answer/7014113?hl=en project_id : Example_GCP_Project_Name # Name of the Firestore table. If it does not exist, Teleport won't start collection_name : Example_TELEPORT_FIRESTORE_TABLE_NAME credentials_path : /var/lib/teleport/gcs_creds # This setting configures Teleport to send the audit events to three places: # To keep a copy in Firestore, a copy on a local filesystem, and also write the events to stdout. # NOTE: The Firestore events table has a different schema to the regular Teleport # database table, so attempting to use same table for both will result in errors. # When using highly available storage like Firestore, you should make sure that the list always specifies # the HA storage method first, as this is what the Teleport web UI uses as its source of events to display. audit_events_uri : [ 'firestore://Example_TELEPORT_FIRESTORE_EVENTS_TABLE_NAME' , 'file:///var/lib/teleport/audit/events' , 'stdout://' ] # This setting configures Teleport to save the recorded sessions in GCP storage: audit_sessions_uri : gs://Example_TELEPORT_S3_BUCKET/records Replace Example_GCP_Project_Name and Example_TELEPORT_FIRESTORE_TABLE_NAME with your own settings. Teleport will create the table automatically. Example_TELEPORT_FIRESTORE_TABLE_NAME and Example_TELEPORT_FIRESTORE_EVENTS_TABLE_NAME must be different Firestore tables. The schema is different for each. Using the same table name for both will result in errors. The GCP authentication setting above can be omitted if the machine itself is running on a GCE instance with a Service Account that has access to the Firestore table. Audit log settings above are optional. If specified, Teleport will store the audit log in Firestore and the session recordings must be stored in a GCP bucket, i.e.both audit_xxx settings must be present. If they are not set, Teleport will default to a local file system for the audit log, i.e. /var/lib/teleport/log on an auth server. Upgrading Teleport Teleport is always a critical component of the infrastructure it runs on. This is why upgrading to a new version must be performed with caution. Teleport is a much more capable system than a bare bones SSH server. While it offers significant benefits on a cluster level, it also adds some complexity to cluster upgrades. To ensure robust operation Teleport administrators must follow the upgrade rules listed below. Production Releases First of all, avoid running pre-releases (release candidates) in production environments. Teleport development team uses Semantic Versioning which makes it easy to tell if a specific version is recommended for production use. Component Compatibility When running multiple binaries of Teleport within a cluster (nodes, proxies, clients, etc), the following rules apply: Patch versions are always compatible, for example any 4.0.1 component will work with any 4.0.3 component. Other versions are always compatible with their previous release. This means you must not attempt to upgrade from 4.1 straight to 4.3. You must upgrade to 4.2 first. Teleport clients tsh for users and tctl for admins may not be compatible with different versions of the teleport service. As an extra precaution you might want to backup your application prior to upgrading. We provide more instructions in Backup before upgrading . Upgrading to Teleport 4.0+ Teleport 4.0+ switched to GRPC and HTTP/2 as an API protocol. The HTTP/2 spec bans two previously recommended ciphers. tls-rsa-with-aes-128-gcm-sha256 & tls-rsa-with-aes-256-gcm-sha384 , make sure these are removed from teleport.yaml Visit our community for more details If upgrading you might want to consider rotating CA to SHA-256 or SHA-512 for RSA SSH certificate signatures. The previous default was SHA-1, which is now considered weak against brute-force attacks. SHA-1 certificate signatures are also no longer accepted by OpenSSH versions 8.2 and above. All new Teleport clusters will default to SHA-512 based signatures. To upgrade an existing cluster, set the following in your teleport.yaml: teleport: ca_signature_algo: \"rsa-sha2-512\" After updating to 4.3+ rotate the cluster CA following these docs . Backup Before Upgrading As an extra precaution you might want to backup your application prior to upgrading. We have more instructions in Backing up Teleport . Upgrade Sequence When upgrading a single Teleport cluster: Upgrade the auth server first . The auth server keeps the cluster state and if there are data format changes introduced in the new version this will perform necessary migrations. Then, upgrade the proxy servers. The proxy servers are stateless and can be upgraded in any sequence or at the same time. Finally, upgrade the SSH nodes in any sequence or at the same time. Warning If several auth servers are running in HA configuration (for example, in AWS auto-scaling group) you have to shrink the group to just one auth server prior to performing an upgrade. While Teleport will attempt to perform any necessary migrations, we recommend users create a backup of their backend before upgrading the Auth Server, as a precaution. This allows for a safe rollback in case the migration itself fails. When upgrading multiple clusters: First, upgrade the main cluster, i.e. the one which other clusters trust. Upgrade the trusted clusters. Backing Up Teleport When planning a backup of Teleport, it's important to know what is where and the importance of each component. Teleport's Proxies and Nodes are stateless, and thus only teleport.yaml should be backed up. The Auth server is Teleport's brains, and depending on the backend should be backed up regularly. For example a customer running Teleport on AWS with DynamoDB have these key items of data: What Where ( Example AWS Customer ) Local Users ( not SSO ) DynamoDB Certificate Authorities DynamoDB Trusted Clusters DynamoDB Connectors: SSO DynamoDB / File System RBAC DynamoDB / File System teleport.yaml File System teleport.service File System license.pem File System TLS key/certificate ( File System / Outside Scope ) Audit log DynamoDB Session recordings S3 For this customer, we would recommend using AWS best practices for backing up DynamoDB. If DynamoDB is used for the audit log, logged events have a TTL of 1 year. Backend Recommended backup strategy dir ( local filesystem ) Backup /var/lib/teleport/storage directory and the output of tctl get all . DynamoDB Follow AWS Guidelines for Backup & Restore etcd Follow etcD Guidleines for Disaster Recovery Firestore Follow GCP Guidlines for Automated Backups Teleport Resources Teleport uses YAML resources for roles, trusted clusters, local users and auth connectors. These could be created via tctl or via the UI. GitOps If running Teleport at scale, it's important for teams to have an automated way to restore Teleport. At a high level, this is our recommended approach: Persist and backup your backend Share that backend among auth servers Store your configs as discrete files in VCS Have your CI run tctl create -f *.yaml from that git directory Migrating Backends. As of version v4.1 you can now quickly export a collection of resources from Teleport. This feature was designed to help customers migrate from local storage to etcd. Using tctl get all will retrieve the below items: Users Certificate Authorities Trusted Clusters Connectors: Github SAML [Teleport Enterprise] OIDC [Teleport Enterprise] Roles [Teleport Enterprise] When migrating backends, you should back up your auth server's data_dir/storage directly. Example of backing up and restoring a cluster. # export dynamic configuration state from old cluster $ tctl get all > state.yaml # prepare a new uninitialized backend (make sure to port # any non-default config values from the old config file) $ mkdir fresh && cat > fresh.yaml << EOF teleport: data_dir: fresh EOF # bootstrap fresh server (kill the old one first!) $ teleport start --config fresh.yaml --bootstrap state.yaml # from another terminal, verify state transferred correctly $ tctl --config fresh.yaml get all # <your state here!> The --bootstrap flag has no effect, except during backend initialization (performed by auth server on first start), so it is safe for use in supervised/HA contexts. Limitations All the same limitations around modifying the config file of an existing cluster also apply to a new cluster being bootstrapped from the state of an old cluster. Of particular note: Changing cluster name will break your CAs (this will be caught and teleport will refuse to start). Some user authentication mechanisms (e.g. u2f) require that the public endpoint of the web ui remains the same (this can't be caught by teleport, be careful!). Any node whose invite token is defined statically (in the config file of the auth server) will be able to join automatically, but nodes that were added dynamically will need to be re-invited Daemon Restarts As covered in the Graceful Restarts section, Teleport supports graceful restarts. To upgrade a host to a newer Teleport version, an administrator must: Replace the Teleport binaries, usually teleport and tctl Execute systemctl restart teleport This will perform a graceful restart, i.e.the Teleport daemon will fork a new process to handle new incoming requests, leaving the old daemon process running until existing clients disconnect. License File Commercial Teleport subscriptions require a valid license. The license file can be downloaded from the Teleport Customer Portal . The Teleport license file contains a X.509 certificate and the corresponding private key in PEM format. Place the downloaded file on Auth servers and set the license_file configuration parameter of your teleport.yaml to point to the file location: auth_service : license_file : /var/lib/teleport/license.pem The license_file path can be either absolute or relative to the configured data_dir . If license file path is not set, Teleport will look for the license.pem file in the configured data_dir . NOTE Only Auth servers require the license. Proxies and Nodes that do not also have Auth role enabled do not need the license. Troubleshooting To diagnose problems you can configure teleport to run with verbose logging enabled by passing it -d flag. NOTE It is not recommended to run Teleport in production with verbose logging as it generates a substantial amount of data. Sometimes you may want to reset teleport to a clean state. This can be accomplished by erasing everything under \"data_dir\" directory. Assuming the default location, rm -rf /var/lib/teleport/* will do. Teleport also supports HTTP endpoints for monitoring purposes. They are disabled by default, but you can enable them: $ teleport start --diag-addr = 127 .0.0.1:3000 Now you can see the monitoring information by visiting several endpoints: http://127.0.0.1:3000/metrics is the list of internal metrics Teleport is tracking. It is compatible with Prometheus collectors. For a full list of metrics review our metrics reference . http://127.0.0.1:3000/healthz returns \"OK\" if the process is healthy or 503 otherwise. http://127.0.0.1:3000/readyz is similar to /healthz , but it returns \"OK\" only after the node successfully joined the cluster, i.e.it draws the difference between \"healthy\" and \"ready\". http://127.0.0.1:3000/debug/pprof/ is Golang's standard profiler. It's only available when -d flag is given in addition to --diag-addr Getting Help If you need help, please ask on our community forum . You can also open an issue on Github . For commercial support, you can create a ticket through the customer dashboard . For more information about custom features, or to try our Enterprise edition of Teleport, please reach out to us at sales@gravitational.com .","title":"Admin Manual"},{"location":"admin-guide/#teleport-admin-manual","text":"This manual covers the installation and configuration of Teleport and the ongoing management of a Teleport cluster. It assumes that the reader has good understanding of Linux administration.","title":"Teleport Admin Manual"},{"location":"admin-guide/#installing","text":"Please visit our installation page for instructions on downloading and installing Teleport.","title":"Installing"},{"location":"admin-guide/#definitions","text":"Before diving into configuring and running Teleport, it helps to take a look at the Teleport Architecture and review the key concepts this document will be referring to: Concept Description Node Synonym to \"server\" or \"computer\", something one can \"SSH to\". A node must be running the teleport daemon with \"node\" role/service turned on. Certificate Authority (CA) A pair of public/private keys Teleport uses to manage access. A CA can sign a public key of a user or node, establishing their cluster membership. Teleport Cluster A Teleport Auth Service contains two CAs. One is used to sign user keys and the other signs node keys. A collection of nodes connected to the same CA is called a \"cluster\". Cluster Name Every Teleport cluster must have a name. If a name is not supplied via teleport.yaml configuration file, a GUID will be generated. IMPORTANT: renaming a cluster invalidates its keys and all certificates it had created. Trusted Cluster Teleport Auth Service can allow 3rd party users or nodes to connect if their public keys are signed by a trusted CA. A \"trusted cluster\" is a pair of public keys of the trusted CA. It can be configured via teleport.yaml file.","title":"Definitions"},{"location":"admin-guide/#teleport-daemon","text":"The Teleport daemon is called teleport and it supports the following commands: Command Description start Starts the Teleport daemon. configure Dumps a sample configuration file in YAML format into standard output. version Shows the Teleport version. status Shows the status of a Teleport connection. This command is only available from inside of an active SSH session. help Shows help. When experimenting, you can quickly start teleport with verbose logging by typing teleport start -d . WARNING Teleport stores data in /var/lib/teleport . Make sure that regular/non-admin users do not have access to this folder on the Auth server.","title":"Teleport Daemon"},{"location":"admin-guide/#systemd-unit-file","text":"In production, we recommend starting teleport daemon via an init system like systemd . Here's the recommended Teleport service unit file for systemd: [Unit] Description=Teleport SSH Service After=network.target [Service] Type=simple Restart=on-failure ExecStart=/usr/local/bin/teleport start --config=/etc/teleport.yaml --pid-file=/run/teleport.pid ExecReload=/bin/kill -HUP $MAINPID PIDFile=/run/teleport.pid [Install] WantedBy=multi-user.target","title":"Systemd Unit File"},{"location":"admin-guide/#graceful-restarts","text":"If using the systemd service unit file above, executing systemctl reload teleport will perform a graceful restart, i.e.the Teleport daemon will fork a new process to handle new incoming requests, leaving the old daemon process running until existing clients disconnect. Version warning Graceful restarts only work if Teleport is deployed using network-based storage like DynamoDB or etcd 3.3+. Future versions of Teleport will not have this limitation. You can also perform restarts/upgrades by sending kill signals to a Teleport daemon manually. Signal Teleport Daemon Behavior USR1 Dumps diagnostics/debugging information into syslog. TERM , INT or KILL Immediate non-graceful shutdown. All existing connections will be dropped. USR2 Forks a new Teleport daemon to serve new connections. HUP Forks a new Teleport daemon to serve new connections and initiates the graceful shutdown of the existing process when there are no more clients connected to it.","title":"Graceful Restarts"},{"location":"admin-guide/#ports","text":"Teleport services listen on several ports. This table shows the default port numbers. Port Service Description 3022 Node SSH port. This is Teleport's equivalent of port #22 for SSH. 3023 Proxy SSH port clients connect to. A proxy will forward this connection to port #3022 on the destination node. 3024 Proxy SSH port used to create \"reverse SSH tunnels\" from behind-firewall environments into a trusted proxy server. 3025 Auth SSH port used by the Auth Service to serve its API to other nodes in a cluster. 3080 Proxy HTTPS connection to authenticate tsh users and web users into the cluster. The same connection is used to serve a Web UI. 3026 Kubernetes Proxy HTTPS Kubernetes proxy (if enabled)","title":"Ports"},{"location":"admin-guide/#filesystem-layout","text":"By default, a Teleport node has the following files present. The location of all of them is configurable. Full path Purpose /etc/teleport.yaml Teleport configuration file (optional). /usr/local/bin/teleport Teleport daemon binary. /usr/local/bin/tctl Teleport admin tool. It is only needed for auth servers. /var/lib/teleport Teleport data directory. Nodes keep their keys and certificates there. Auth servers store the audit log and the cluster keys there, but the audit log storage can be further configured via auth_service section in the config file.","title":"Filesystem Layout"},{"location":"admin-guide/#configuration","text":"You should use a configuration file to configure the teleport daemon. For simple experimentation, you can use command line flags with the teleport start command. Read about all the allowed flags in the CLI Docs or run teleport start --help","title":"Configuration"},{"location":"admin-guide/#configuration-file","text":"Teleport uses the YAML file format for configuration. A sample configuration file is shown below. By default, it is stored in /etc/teleport.yaml , below is an expanded and commented version from teleport configure . The default path Teleport uses to look for a config file is /etc/teleport.yaml . You can override this path and set it explicitly using the -c or --config flag to teleport start : $ teleport start --config = /etc/teleport.yaml For a complete reference, see our Configuration Reference - teleport.yaml IMPORTANT When editing YAML configuration, please pay attention to how your editor handles white space. YAML requires consistent handling of tab characters. # # Sample Teleport configuration file # Creates a single proxy, auth and node server. # # Things to update: # 1. ca_pin: Obtain the CA pin hash for joining more nodes by running 'tctl status' # on the auth server once Teleport is running. # 2. license-if-using-teleport-enterprise.pem: If you are an Enterprise customer, # obtain this from https://dashboard.gravitational.com/web/login # teleport : # nodename allows to assign an alternative name this node can be reached by. # by default it's equal to hostname nodename : NODE_NAME data_dir : /var/lib/teleport # Invitation token used to join a cluster. it is not used on # subsequent starts auth_token : xxxx-token-xxxx # Optional CA pin of the auth server. This enables more secure way of adding new # nodes to a cluster. See \"Adding Nodes\" section above. ca_pin : \"sha256:ca-pin-hash-goes-here\" # list of auth servers in a cluster. you will have more than one auth server # if you configure teleport auth to run in HA configuration. # If adding a node located behind NAT, use the Proxy URL. e.g. # auth_servers: # - teleport-proxy.example.com:3080 auth_servers : - 10.1.0.5:3025 - 10.1.0.6:3025 # Logging configuration. Possible output values to disk via '/var/lib/teleport/teleport.log', # 'stdout', 'stderr' and 'syslog'. Possible severity values are INFO, WARN # and ERROR (default). log : output : stderr severity : INFO auth_service : enabled : \"yes\" # A cluster name is used as part of a signature in certificates # generated by this CA. # # We strongly recommend to explicitly set it to something meaningful as it # becomes important when configuring trust between multiple clusters. # # By default an automatically generated name is used (not recommended) # # IMPORTANT: if you change cluster_name, it will invalidate all generated # certificates and keys (may need to wipe out /var/lib/teleport directory) cluster_name : \"teleport-aws-us-east-1\" # IP and the port to bind to. Other Teleport nodes will be connecting to # this port (AKA \"Auth API\" or \"Cluster API\") to validate client # certificates listen_addr : 0.0.0.0:3025 tokens : - proxy,node:xxxx-token-xxxx # license_file: /path/to/license-if-using-teleport-enterprise.pem authentication : # default authentication type. possible values are 'local' and 'github' for OSS # and 'oidc', 'saml' and 'false' for Enterprise. type : local # second_factor can be off, otp, or u2f second_factor : otp ssh_service : enabled : \"yes\" labels : teleport : static-label-example commands : - name : hostname command : [ /usr/bin/hostname ] period : 1m0s - name : arch command : [ /usr/bin/uname , -p ] period : 1h0m0s proxy_service : enabled : \"yes\" listen_addr : 0.0.0.0:3023 web_listen_addr : 0.0.0.0:3080 tunnel_listen_addr : 0.0.0.0:3024 # The DNS name of the proxy HTTPS endpoint as accessible by cluster users. # Defaults to the proxy's hostname if not specified. If running multiple # proxies behind a load balancer, this name must point to the load balancer # (see public_addr section below) public_addr : TELEPORT_PUBLIC_DNS_NAME:3080 # TLS certificate for the HTTPS connection. Configuring these properly is # critical for Teleport security. https_key_file : /etc/letsencrypt/live/TELEPORT_PUBLIC_DNS_NAME/privkey.pem https_cert_file : /etc/letsencrypt/live/TELEPORT_PUBLIC_DNS_NAME/fullchain.pem","title":"Configuration File"},{"location":"admin-guide/#authentication","text":"Teleport uses the concept of \"authentication connectors\" to authenticate users when they execute tsh login command. There are three types of authentication connectors:","title":"Authentication"},{"location":"admin-guide/#local-connector","text":"Local authentication is used to authenticate against a local Teleport user database. This database is managed by tctl users command. Teleport also supports second factor authentication (2FA) for the local connector. There are three possible values (types) of 2FA: otp is the default. It implements TOTP standard. You can use Google Authenticator or Authy or any other TOTP client. u2f implements U2F standard for utilizing hardware (USB) keys for second factor. You can use YubiKeys , SoloKeys or any other hardware token which implements the FIDO U2F standard. off turns off second factor authentication. Here is an example of this setting in the teleport.yaml : auth_service : authentication : type : local second_factor : off","title":"Local Connector"},{"location":"admin-guide/#github-oauth-20-connector","text":"This connector implements Github OAuth 2.0 authentication flow. Please refer to Github documentation on Creating an OAuth App to learn how to create and register an OAuth app. Here is an example of this setting in the teleport.yaml : auth_service : authentication : type : github See Github OAuth 2.0 for details on how to configure it.","title":"Github OAuth 2.0 Connector"},{"location":"admin-guide/#saml","text":"This connector type implements SAML authentication. It can be configured against any external identity manager like Okta or Auth0. This feature is only available for Teleport Enterprise. Here is an example of this setting in the teleport.yaml : auth_service : authentication : type : saml","title":"SAML"},{"location":"admin-guide/#oidc","text":"Teleport implements OpenID Connect (OIDC) authentication, which is similar to SAML in principle. This feature is only available for Teleport Enterprise. Here is an example of this setting in the teleport.yaml : auth_service : authentication : type : oidc","title":"OIDC"},{"location":"admin-guide/#hardware-keys-yubikey-fido-u2f","text":"Teleport supports FIDO U2F hardware keys as a second authentication factor. By default U2F is disabled. To start using U2F: Enable U2F in Teleport configuration /etc/teleport.yaml . For CLI-based logins you have to install u2f-host utility. For web-based logins you have to use Google Chrome and Firefox 67 or greater, are the only supported U2F browsers at this time. # snippet from /etc/teleport.yaml to show an example configuration of U2F: auth_service : authentication : type : local second_factor : u2f # this section is needed only if second_factor is set to 'u2f' u2f : # app_id must point to the URL of the Teleport Web UI (proxy) accessible # by the end users app_id : https://localhost:3080 # facets must list all proxy servers if there are more than one deployed facets : - https://localhost:3080 For single-proxy setups, the app_id setting can be equal to the domain name of the proxy, but this will prevent you from adding more proxies without changing the app_id . For multi-proxy setups, the app_id should be an HTTPS URL pointing to a JSON file that mirrors facets in the auth config. Warning The app_id must never change in the lifetime of the cluster. If the App ID changes, all existing U2F key registrations will become invalid and all users who use U2F as the second factor will need to re-register. When adding a new proxy server, make sure to add it to the list of \"facets\" in the configuration file, but also to the JSON file referenced by app_id Logging in with U2F For logging in via the CLI, you must first install u2f-host . Installing: # OSX: $ brew install libu2f-host # Ubuntu 16.04 LTS: $ apt-get install u2f-host Then invoke tsh ssh as usual to authenticate: $ tsh --proxy <proxy-addr> ssh <hostname> Version Warning External user identities are only supported in Teleport Enterprise . Please reach out to sales@gravitational.com for more information.","title":"Hardware Keys - YubiKey FIDO U2F"},{"location":"admin-guide/#adding-and-deleting-users","text":"This section covers internal user identities, i.e. user accounts created and stored in Teleport's internal storage. Most production users of Teleport use external users via Github or Okta or any other SSO provider (Teleport Enterprise supports any SAML or OIDC compliant identity provider). A user identity in Teleport exists in the scope of a cluster. The member nodes of a cluster have multiple OS users on them. A Teleport administrator creates Teleport user accounts and maps them to the allowed OS user logins they can use. Let's look at this table: Teleport User Allowed OS Logins Description joe joe, root Teleport user 'joe' can login into member nodes as OS user 'joe' or 'root' bob bob Teleport user 'bob' can login into member nodes only as OS user 'bob' ross If no OS login is specified, it defaults to the same name as the Teleport user - 'ross'. To add a new user to Teleport, you have to use the tctl tool on the same node where the auth server is running, i.e. teleport was started with --roles=auth . $ tctl users add joe joe,root Teleport generates an auto-expiring token (with a TTL of 1 hour) and prints the token URL which must be used before the TTL expires. Signup token has been created. Share this URL with the user: https://<proxy>:3080/web/newuser/xxxxxxxxxxxx NOTE: make sure the <proxy> host is accessible. The user completes registration by visiting this URL in their web browser, picking a password and configuring the 2nd factor authentication. If the credentials are correct, the auth server generates and signs a new certificate and the client stores this key and will use it for subsequent logins. The key will automatically expire after 12 hours by default after which the user will need to log back in with her credentials. This TTL can be configured to a different value. Once authenticated, the account will become visible via tctl : $ tctl users ls User Allowed Logins ---- -------------- admin admin,root ross ross joe joe,root Joe would then use the tsh client tool to log in to member node \"luna\" via bastion \"work\" as root : $ tsh --proxy = work --user = joe root@luna To delete this user: $ tctl users rm joe","title":"Adding and Deleting Users"},{"location":"admin-guide/#editing-users","text":"Users entries can be manipulated using the generic resource commands via tctl . For example, to see the full list of user records, an administrator can execute: $ tctl get users To edit the user \"joe\": # dump the user definition into a file: $ tctl get user/joe > joe.yaml # ... edit the contents of joe.yaml # update the user record: $ tctl create -f joe.yaml Some fields in the user record are reserved for internal use. Some of them will be finalized and documented in the future versions. Fields like is_locked or traits/logins can be used starting in version 2.3","title":"Editing Users"},{"location":"admin-guide/#adding-nodes-to-the-cluster","text":"Teleport is a \"clustered\" system, meaning it only allows access to nodes (servers) that had been previously granted cluster membership. A cluster membership means that a node receives its own host certificate signed by the cluster's auth server. To receive a host certificate upon joining a cluster, a new Teleport host must present an \"invite token\". An invite token also defines which role a new host can assume within a cluster: auth , proxy or node . There are two ways to create invitation tokens: Static Tokens are easy to use and somewhat less secure. Dynamic Tokens are more secure but require more planning.","title":"Adding Nodes to the Cluster"},{"location":"admin-guide/#static-tokens","text":"Static tokens are defined ahead of time by an administrator and stored in the auth server's config file: # Config section in `/etc/teleport.yaml` file for the auth server auth_service : enabled : true tokens : # This static token allows new hosts to join the cluster as \"proxy\" or \"node\" - \"proxy,node:secret-token-value\" # A token can also be stored in a file. In this example the token for adding # new auth servers is stored in /path/to/tokenfile - \"auth:/path/to/tokenfile\"","title":"Static Tokens"},{"location":"admin-guide/#short-lived-tokens","text":"A more secure way to add nodes to a cluster is to generate tokens as they are needed. Such token can be used multiple times until its time to live (TTL) expires. Use the tctl tool to register a new invitation token (or it can also generate a new token for you). In the following example a new token is created with a TTL of 5 minutes: $ tctl nodes add --ttl = 5m --roles = node,proxy --token = secret-value The invite token: secret-value If --token is not provided, tctl will generate one: # generate a short-lived invitation token for a new node: $ tctl nodes add --ttl = 5m --roles = node,proxy The invite token: e94d68a8a1e5821dbd79d03a960644f0 # you can also list all generated non-expired tokens: $ tctl tokens ls Token Type Expiry Time --------------- ----------- --------------- e94d68a8a1e5821dbd79d03a960644f0 Node 25 Sep 18 00 :21 UTC # ... or revoke an invitation before it's used: $ tctl tokens rm e94d68a8a1e5821dbd79d03a960644f0","title":"Short-lived Tokens"},{"location":"admin-guide/#using-node-invitation-tokens","text":"Both static and short-lived tokens are used the same way. Execute the following command on a new node to add it to a cluster: # adding a new regular SSH node to the cluster: $ teleport start --roles = node --token = secret-token-value --auth-server = 10 .0.10.5 # adding a new regular SSH node using Teleport Node Tunneling: $ teleport start --roles = node --token = secret-token-value --auth-server = teleport-proxy.example.com:3080 # adding a new proxy service on the cluster: $ teleport start --roles = proxy --token = secret-token-value --auth-server = 10 .0.10.5 As new nodes come online, they start sending ping requests every few seconds to the CA of the cluster. This allows users to explore cluster membership and size: $ tctl nodes ls Node Name Node ID Address Labels --------- ------- ------- ------ turing d52527f9-b260-41d0-bb5a-e23b0cfe0f8f 10 .1.0.5:3022 distro:ubuntu dijkstra c9s93fd9-3333-91d3-9999-c9s93fd98f43 10 .1.0.6:3022 distro:debian","title":"Using Node Invitation Tokens"},{"location":"admin-guide/#untrusted-auth-servers","text":"Teleport nodes use the HTTPS protocol to offer the join tokens to the auth server running on 10.0.10.5 in the example above. In a zero-trust environment, you must assume that an attacker can hijack the IP address of the auth server e.g. 10.0.10.5 . To prevent this from happening, you need to supply every new node with an additional bit of information about the auth server. This technique is called \"CA Pinning\". It works by asking the auth server to produce a \"CA Pin\", which is a hashed value of its public key, i.e. for which an attacker can't forge a matching private key. On the auth server: $ tctl status Cluster staging.example.com User CA never updated Host CA never updated CA pin sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1 The \"CA pin\" at the bottom needs to be passed to the new nodes when they're starting for the first time, i.e. when they join a cluster: Via CLI: $ teleport start \\ --roles = node \\ --token = 1ac590d36493acdaa2387bc1c492db1a \\ --ca-pin = sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1 \\ --auth-server = 10 .12.0.6:3025 or via /etc/teleport.yaml on a node: teleport : auth_token : \"1ac590d36493acdaa2387bc1c492db1a\" ca_pin : \"sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1\" auth_servers : - \"10.12.0.6:3025\" Warning If a CA pin is not provided, Teleport node will join a cluster but it will print a WARN message (warning) into its standard error output. Warning The CA pin becomes invalid if a Teleport administrator performs the CA rotation by executing tctl auth rotate .","title":"Untrusted Auth Servers"},{"location":"admin-guide/#revoking-invitations","text":"As you have seen above, Teleport uses tokens to invite users to a cluster (sign-up tokens) or to add new nodes to it (provisioning tokens). Both types of tokens can be revoked before they can be used. To see a list of outstanding tokens, run this command: $ tctl tokens ls Token Role Expiry Time ( UTC ) ----- ---- ----------------- eoKoh0caiw6weoGupahgh6Wuo7jaTee2 Proxy never 696c0471453e75882ff70a761c1a8bfa Node 17 May 16 03 :51 UTC 6fc5545ab78c2ea978caabef9dbd08a5 Signup 17 May 16 04 :24 UTC In this example, the first token has a \"never\" expiry date because it is a static token configured via a config file. The 2nd token with \"Node\" role was generated to invite a new node to this cluster. And the 3rd token was generated to invite a new user. The latter two tokens can be deleted (revoked) via tctl tokens del command: $ tctl tokens del 696c0471453e75882ff70a761c1a8bfa Token 696c0471453e75882ff70a761c1a8bfa has been deleted","title":"Revoking Invitations"},{"location":"admin-guide/#adding-a-node-located-behind-nat","text":"Note This feature is sometimes called \"Teleport IoT\" or node tunneling. With the current setup, you've only been able to add nodes that have direct access to the auth server and within the internal IP range of the cluster. We recommend setting up a Trusted Cluster if you have workloads split across different networks/clouds. Teleport Node Tunneling lets you add a remote node to an existing Teleport Cluster via tunnel. This can be useful for IoT applications, or for managing a couple of servers in a different network. Similar to Adding Nodes to the Cluster , use tctl to create a single-use token for a node, but this time you'll replace the auth server IP with the URL of the proxy server. In the example below, we've replaced the auth server IP with the proxy web endpoint teleport-proxy.example.com:3080 . $ sudo tctl nodes add The invite token: n92bb958ce97f761da978d08c35c54a5c Run this on the new node to join the cluster: teleport start --roles = node --token = n92bb958ce97f761da978d08c35c54a5c --auth-server = teleport-proxy.example.com:3080 Using the ports in the default configuration, the node needs to be able to talk to ports 3080 and 3024 on the proxy. Port 3080 is used to initially fetch the credentials (SSH and TLS certificates) and for discovery (where is the reverse tunnel running, in this case 3024). Port 3024 is used to establish a connection to the auth server through the proxy. To enable multiplexing so only one port is used, simply set the tunnel_listen_addr the same as the web_listen_addr respectively within the proxy_service . Teleport will automatically recognize using the same port and enable multiplexing. If the log setting is set to DEBUG you will see multiplexing enabled in the server log. DEBU [ PROC:1 ] Setup Proxy: Reverse tunnel proxy and web proxy listen on the same port, multiplexing is on. service/service.go:1944 Load Balancers The setup above also works even if the cluster uses multiple proxies behind a load balancer (LB) or a DNS entry with multiple values. This works by the node establishing a tunnel to every proxy. This requires that an LB uses round-robin or a similar balancing algorithm. Do not use sticky load balancing algorithms (a.k.a. \"session affinity\") with Teleport proxies.","title":"Adding a node located behind NAT"},{"location":"admin-guide/#labeling-nodes","text":"In addition to specifying a custom nodename, Teleport also allows for the application of arbitrary key:value pairs to each node, called labels. There are two kinds of labels: static labels do not change over time, while teleport process is running. Examples of static labels are physical location of nodes, name of the environment (staging vs production), etc. dynamic labels also known as \"label commands\" allow to generate labels at runtime. Teleport will execute an external command on a node at a configurable frequency and the output of a command becomes the label value. Examples include reporting load averages, presence of a process, time after last reboot, etc. There are two ways to configure node labels. Via command line, by using --labels flag to teleport start command. Using /etc/teleport.yaml configuration file on the nodes. To define labels as command line arguments, use --labels flag like shown below. This method works well for static labels or simple commands: $ teleport start --labels uptime =[ 1m: \"uptime -p\" ] ,kernel =[ 1h: \"uname -r\" ] Alternatively, you can update labels via a configuration file: ssh_service : enabled : \"yes\" # Static labels are simple key/value pairs: labels : environment : test To configure dynamic labels via a configuration file, define a commands array as shown below: ssh_service : enabled : \"yes\" # Dynamic labels AKA \"commands\": commands : - name : hostname command : [ hostname ] period : 1m0s - name : arch command : [ uname , -p ] # this setting tells teleport to execute the command above # once an hour. this value cannot be less than one minute. period : 1h0m0s /path/to/executable must be a valid executable command (i.e. executable bit must be set) which also includes shell scripts with a proper shebang line . Important: notice that command setting is an array where the first element is a valid executable and each subsequent element is an argument, i.e: # valid syntax: command : [ \"/bin/uname\" , \"-m\" ] # INVALID syntax: command : [ \"/bin/uname -m\" ] # if you want to pipe several bash commands together, here's how to do it: # notice how ' and \" are interchangeable and you can use it for quoting: command : [ \"/bin/sh\" , \"-c\" , \"uname -a | egrep -o '[0-9]+\\\\.[0-9]+\\\\.[0-9]+'\" ]","title":"Labeling Nodes"},{"location":"admin-guide/#audit-log","text":"Teleport logs every SSH event into its audit log. There are two components of the audit log: SSH Events: Teleport logs events like successful user logins along with the metadata like remote IP address, time and the session ID. Recorded Sessions: Every SSH shell session is recorded and can be replayed later. The recording is done by the nodes themselves, by default, but can be configured to be done by the proxy. Optional: Enhanced Session Recording Refer to the \"Audit Log\" chapter in the Teleport Architecture to learn more about how the audit log and session recording are designed.","title":"Audit Log"},{"location":"admin-guide/#ssh-events","text":"Teleport supports multiple storage back-ends for storing the SSH events. The section below uses the dir backend as an example. dir backend uses the local filesystem of an auth server using the configurable data_dir directory. For highly available (HA) configurations, users can refer to our DynamoDB or Firestore chapters for information on how to configure the SSH events and recorded sessions to be stored on network storage. It is even possible to store the audit log in multiple places at the same time - see audit_events_uri setting in the sample configuration file above for how to do that. Let's examine the Teleport audit log using the dir backend. The event log is stored in data_dir under log directory, usually /var/lib/teleport/log . Each day is represented as a file: $ ls -l /var/lib/teleport/log/ total 104 -rw-r----- 1 root root 31638 Jan 22 20 :00 2017 -01-23.00:00:00.log -rw-r----- 1 root root 91256 Jan 31 21 :00 2017 -02-01.00:00:00.log -rw-r----- 1 root root 15815 Feb 32 22 :54 2017 -02-03.00:00:00.log The log files use JSON format. They are human-readable but can also be programmatically parsed. Each line represents an event and has the following format: { // Event type. See below for the list of all possible event types \"event\" : \"session.start\" , // Teleport user name \"user\" : \"ekontsevoy\" , // OS login \"login\" : \"root\" , // Server namespace. This field is reserved for future use. \"namespace\" : \"default\" , // Unique server ID. \"server_id\" : \"f84f7386-5e22-45ff-8f7d-b8079742e63f\" , // Session ID. Can be used to replay the session. \"sid\" : \"8d3895b6-e9dd-11e6-94de-40167e68e931\" , // Address of the SSH node \"addr.local\" : \"10.5.l.15:3022\" , // Address of the connecting client (user) \"addr.remote\" : \"73.223.221.14:42146\" , // Terminal size \"size\" : \"80:25\" , // Timestamp \"time\" : \"2017-02-03T06:54:05Z\" } The possible event types are: Event Type Description auth Authentication attempt. Adds the following fields: {\"success\": \"false\", \"error\": \"access denied\"} session.start Started an interactive shell session. session.end An interactive shell session has ended. session.join A new user has joined the existing interactive shell session. session.leave A user has left the session. session.disk A list of files opened during the session. Requires Enhanced Session Recording . session.network A list of network connections made during the session. Requires Enhanced Session Recording . session.command A list of commands ran during the session. Requires Enhanced Session Recording . exec Remote command has been executed via SSH, like tsh ssh root@node ls / . The following fields will be logged: {\"command\": \"ls /\", \"exitCode\": 0, \"exitError\": \"\"} scp Remote file copy has been executed. The following fields will be logged: {\"path\": \"/path/to/file.txt\", \"len\": 32344, \"action\": \"read\" } resize Terminal has been resized. user.login A user logged into web UI or via tsh. The following fields will be logged: {\"user\": \"alice@example.com\", \"method\": \"local\"} .","title":"SSH Events"},{"location":"admin-guide/#recorded-sessions","text":"In addition to logging session.start and session.end events, Teleport also records the entire stream of bytes going to/from standard input and standard output of an SSH session. Teleport can store the recorded sessions in an AWS S3 bucket or in a local filesystem (including NFS). The recorded sessions are stored as raw bytes in the sessions directory under log . Each session consists of two files, both are named after the session ID: .bytes file or .chunks.gz compressed format represents the raw session bytes and is somewhat human-readable, although you are better off using tsh play or the Web UI to replay it. .log file or .events.gz compressed file contains the copies of the event log entries that are related to this session. $ ls /var/lib/teleport/log/sessions/default -rw-r----- 1 root root 506192 Feb 4 00 :46 4c146ec8-eab6-11e6-b1b3-40167e68e931.session.bytes -rw-r----- 1 root root 44943 Feb 4 00 :46 4c146ec8-eab6-11e6-b1b3-40167e68e931.session.log To replay this session via CLI: $ tsh --proxy = proxy play 4c146ec8-eab6-11e6-b1b3-40167e68e931","title":"Recorded Sessions"},{"location":"admin-guide/#resources","text":"A Teleport administrator has two tools to configure a Teleport cluster: The configuration file is used for static configuration like the cluster name. The tctl admin tool is used for manipulating dynamic records like Teleport users. tctl has convenient subcommands for dynamic configuration, like tctl users or tctl nodes . However, for dealing with more advanced topics, like connecting clusters together or troubleshooting trust, tctl offers the more powerful, although lower-level CLI interface called resources . The concept is borrowed from the REST programming pattern. A cluster is composed of different objects (aka, resources) and there are just three common operations that can be performed on them: get , create , remove . A resource is defined as a YAML file. Every resource in Teleport has three required fields: Kind - The type of resource Name - A required field in the metadata to uniquely identify the resource Version - The version of the resource format Everything else is resource-specific and any component of a Teleport cluster can be manipulated with just 3 CLI commands: Command Description Examples tctl get Get one or multiple resources tctl get users or tctl get user/joe tctl rm Delete a resource by type/name tctl rm user/joe tctl create Create a new resource from a YAML file. Use -f to override / update tctl create -f joe.yaml YAML Format By default Teleport uses YAML format to describe resources. YAML is a wonderful and very human-readable alternative to JSON or XML, but it's sensitive to white space. Pay attention to spaces vs tabs! Here's an example how the YAML resource definition for a user Joe might look like. It can be retrieved by executing tctl get user/joe kind : user version : v2 metadata : name : joe spec : roles : admin status : # users can be temporarily locked in a Teleport system, but this # functionality is reserved for internal use for now. is_locked : false lock_expires : 0001-01-01T00:00:00Z locked_time : 0001-01-01T00:00:00Z traits : # these are \"allowed logins\" which are usually specified as the # last argument to `tctl users add` logins : - joe - root # any resource in Teleport can automatically expire. expires : 0001-01-01T00:00:00Z # for internal use only created_by : time : 0001-01-01T00:00:00Z user : name : builtin-Admin Note Some of the fields you will see when printing resources are used only internally and are not meant to be changed. Others are reserved for future use. Here's the list of resources currently exposed via tctl : Resource Kind Description user A user record in the internal Teleport user DB. node A registered SSH node. The same record is displayed via tctl nodes ls cluster A trusted cluster. See here for more details on connecting clusters together. role A role assumed by users. The open source Teleport only includes one role: \"admin\", but Enterprise teleport users can define their own roles. connector Authentication connectors for single sign-on (SSO) for SAML, OIDC and Github. Examples: # list all connectors: $ tctl get connectors # dump a SAML connector called \"okta\": $ tctl get saml/okta # delete a SAML connector called \"okta\": $ tctl rm saml/okta # delete an OIDC connector called \"gsuite\": $ tctl rm oidc/gsuite # delete a github connector called \"myteam\": $ tctl rm github/myteam # delete a local user called \"admin\": $ tctl rm users/admin Note Although tctl get connectors will show you every connector, when working with an individual connector you must use the correct kind , such as saml or oidc . You can see each connector's kind at the top of its YAML output from tctl get connectors .","title":"Resources"},{"location":"admin-guide/#trusted-clusters","text":"As explained in the architecture document , Teleport can partition compute infrastructure into multiple clusters. A cluster is a group of nodes connected to the cluster's auth server, acting as a certificate authority (CA) for all users and nodes. To retrieve an SSH certificate, users must authenticate with a cluster through a proxy server. So, if users want to connect to nodes belonging to different clusters, they would normally have to use a different --proxy flag for each cluster. This is not always convenient. The concept of trusted clusters allows Teleport administrators to connect multiple clusters together and establish trust between them. Trusted clusters allow users of one cluster to seamlessly SSH into the nodes of another cluster without having to \"hop\" between proxy servers. Moreover, users don't even need to have a direct connection to other clusters' proxy servers. Trusted clusters also have their own restrictions on user access. To learn more about Trusted Clusters please visit our Trusted Cluster Guide","title":"Trusted Clusters"},{"location":"admin-guide/#github-oauth-20","text":"Teleport supports authentication and authorization via external identity providers such as Github. You can watch the video for how to configure Github as an SSO provider , or you can follow the documentation below. First, the Teleport auth service must be configured to use Github for authentication: # snippet from /etc/teleport.yaml auth_service : authentication : type : github Next step is to define a Github connector: # Create a file called github.yaml: kind : github version : v3 metadata : # connector name that will be used with `tsh --auth=github login` name : github spec : # client ID of Github OAuth app client_id : <client-id> # client secret of Github OAuth app client_secret : <client-secret> # connector display name that will be shown on web UI login screen display : Github # callback URL that will be called after successful authentication redirect_url : https://<proxy-address>/v1/webapi/github/callback # mapping of org/team memberships onto allowed logins and roles teams_to_logins : - organization : octocats # Github organization name team : admins # Github team name within that organization # allowed logins for users in this org/team logins : - root # List of Kubernetes groups this Github team is allowed to connect to # (see Kubernetes integration for more information) kubernetes_groups : [ \"system:masters\" ] Note For open-source Teleport the logins field contains a list of allowed OS logins. For the commercial Teleport Enterprise offering, which supports role-based access control, the same field is treated as a list of roles that users from the matching org/team assume after going through the authorization flow. To obtain client ID and client secret, please follow Github documentation on how to create and register an OAuth app . Be sure to set the \"Authorization callback URL\" to the same value as redirect_url in the resource spec. Teleport will request only the read:org OAuth scope, you can read more about Github OAuth scopes . Finally, create the connector using tctl resource management command: $ tctl create github.yaml Tip When going through the Github authentication flow for the first time, the application must be granted the access to all organizations that are present in the \"teams to logins\" mapping, otherwise Teleport will not be able to determine team memberships for these orgs.","title":"Github OAuth 2.0"},{"location":"admin-guide/#http-connect-proxies","text":"Some networks funnel all connections through a proxy server where they can be audited and access control rules are applied. For these scenarios Teleport supports HTTP CONNECT tunneling. To use HTTP CONNECT tunneling, simply set either the HTTPS_PROXY or HTTP_PROXY environment variables and when Teleport builds and establishes the reverse tunnel to the main cluster, it will funnel all traffic though the proxy. Specifically, if using the default configuration, Teleport will tunnel ports 3024 (SSH, reverse tunnel) and 3080 (HTTPS, establishing trust) through the proxy. The value of HTTPS_PROXY or HTTP_PROXY should be in the format scheme://host:port where scheme is either https or http . If the value is host:port , Teleport will prepend http . It's important to note that in order for Teleport to use HTTP CONNECT tunnelling, the HTTP_PROXY and HTTPS_PROXY environment variables must be set within Teleport's environment. You can also optionally set the NO_PROXY environment variable to avoid use of the proxy when accessing specified hosts/netmasks. When launching Teleport with systemd, this will probably involve adding some lines to your systemd unit file: [Service] Environment=\"HTTP_PROXY=http://proxy.example.com:8080/\" Environment=\"HTTPS_PROXY=http://proxy.example.com:8080/\" Environment=\"NO_PROXY=localhost,127.0.0.1,192.168.0.0/16,172.16.0.0/12,10.0.0.0/8\" Note localhost and 127.0.0.1 are invalid values for the proxy host. If for some reason your proxy runs locally, you'll need to provide some other DNS name or a private IP address for it.","title":"HTTP CONNECT Proxies"},{"location":"admin-guide/#pam-integration","text":"Teleport node service can be configured to integrate with PAM . This allows Teleport to create user sessions using PAM session profiles. To enable PAM on a given Linux machine, update /etc/teleport.yaml with: teleport : ssh_service : pam : # \"no\" by default enabled : yes # use /etc/pam.d/sshd configuration (the default) service_name : \"sshd\" Please note that most Linux distributions come with a number of PAM services in /etc/pam.d and Teleport will try to use sshd by default, which will be removed if you uninstall openssh-server package. We recommend creating your own PAM service file like /etc/pam.d/teleport and specifying it as service_name above. Note Teleport only supports the account and session stack. The auth PAM module is currently not supported with Teleport.","title":"PAM Integration"},{"location":"admin-guide/#using-teleport-with-openssh","text":"Review our dedicated Using Teleport with OpenSSH guide.","title":"Using Teleport with OpenSSH"},{"location":"admin-guide/#certificate-rotation","text":"Take a look at the Certificates chapter in the architecture document to learn how the certificate rotation works. This section will show you how to implement certificate rotation in practice. The easiest way to start the rotation is to execute this command on a cluster's auth server : $ tctl auth rotate This will trigger a rotation process for both hosts and users with a grace period of 48 hours. This can be customized, i.e. # rotate only user certificates with a grace period of 200 hours: $ tctl auth rotate --type = user --grace-period = 200h # rotate only host certificates with a grace period of 8 hours: $ tctl auth rotate --type = host --grace-period = 8h The rotation takes time, especially for hosts, because each node in a cluster needs to be notified that a rotation is taking place and request a new certificate for itself before the grace period ends. Warning Be careful when choosing a grace period when rotating host certificates. The grace period needs to be long enough for all nodes in a cluster to request a new certificate. If some nodes go offline during the rotation and come back only after the grace period has ended, they will be forced to leave the cluster, i.e. users will no longer be allowed to SSH into them. To check the status of certificate rotation: $ tctl status CA Pinning Warning If you are using CA Pinning when adding new nodes, the CA pin will changes after the rotation. Make sure you use the new CA pin when adding nodes after rotation.","title":"Certificate Rotation"},{"location":"admin-guide/#ansible-integration","text":"Ansible uses the OpenSSH client by default. This makes it compatible with Teleport without any extra work, except configuring OpenSSH client to work with Teleport Proxy: configure your OpenSSH to connect to Teleport proxy and use ssh-agent socket enable scp mode in the Ansible config file (default is /etc/ansible/ansible.cfg ): scp_if_ssh = True","title":"Ansible Integration"},{"location":"admin-guide/#kubernetes-integration","text":"Teleport can be configured as a compliance gateway for Kubernetes clusters. This allows users to authenticate against a Teleport proxy using tsh login command to retrieve credentials for both SSH and Kubernetes API. Follow our Kubernetes guide which contains some more specific examples and instructions.","title":"Kubernetes Integration"},{"location":"admin-guide/#high-availability","text":"Tip Before continuing, please make sure to take a look at the Cluster State section in the Teleport Architecture documentation. Usually there are two ways to achieve high availability. You can \"outsource\" this function to the infrastructure. For example, using a highly available network-based disk volumes (similar to AWS EBS) and by migrating a failed VM to a new host. In this scenario, there's nothing Teleport-specific to be done. If high availability cannot be provided by the infrastructure (perhaps you're running Teleport on a bare metal cluster), you can still configure Teleport to run in a highly available fashion.","title":"High Availability"},{"location":"admin-guide/#auth-server-ha","text":"In order to run multiple instances of Teleport Auth Server, you must switch to a highly available secrets back-end first. Also, you must tell each node in a cluster that there is more than one auth server available. There are two ways to do this: Use a load balancer to create a single auth API access point (AP) and specify this AP in auth_servers section of Teleport configuration for all nodes in a cluster. This load balancer should do TCP level forwarding. If a load balancer is not an option, you must specify each instance of an auth server in auth_servers section of Teleport configuration. IMPORTANT: with multiple instances of the auth servers running, special attention needs to be paid to keeping their configuration identical. Settings like cluster_name , tokens , storage , etc must be the same.","title":"Auth Server HA"},{"location":"admin-guide/#teleport-proxy-ha","text":"The Teleport Proxy is stateless which makes running multiple instances trivial. If using the default configuration , configure your load balancer to forward ports 3023 and 3080 to the servers that run the Teleport proxy. If you have configured your proxy to use non-default ports, you will need to configure your load balancer to forward the ports you specified for listen_addr and web_listen_addr in teleport.yaml . The load balancer for web_listen_addr can terminate TLS with your own certificate that is valid for your users, while the remaining ports should do TCP level forwarding, since Teleport will handle its own SSL on top of that with its own certificates. NOTE If you terminate TLS with your own certificate at a load balancer you'll need to run Teleport with --insecure-no-tls If your load balancer supports HTTP health checks, configure it to hit the /readyz diagnostics endpoint on machines running Teleport. This endpoint must be enabled by using the --diag-addr flag to teleport start: teleport start --diag-addr=127.0.0.1:3000 The http://127.0.0.1:3000/readyz endpoint will reply {\"status\":\"ok\"} if the Teleport service is running without problems. NOTE As the new auth servers get added to the cluster and the old servers get decommissioned, nodes and proxies will refresh the list of available auth servers and store it in their local cache /var/lib/teleport/authservers.json - the values from the cache file will take precedence over the configuration file. We'll cover how to use etcd , DynamoDB and Firestore storage back-ends to make Teleport highly available below.","title":"Teleport Proxy HA"},{"location":"admin-guide/#teleport-scalability-tweaks","text":"When running Teleport at scale (for example in the case where there are 10,000+ nodes connected to a cluster via node tunnelling mode , the following settings should be set on Teleport auth and proxies:","title":"Teleport Scalability Tweaks"},{"location":"admin-guide/#using-etcd","text":"Teleport can use etcd as a storage backend to achieve highly available deployments. You must take steps to protect access to etcd in this configuration because that is where Teleport secrets like keys and user records will be stored. IMPORTANT etcd can only currently be used to store Teleport's internal database in a highly-available way. This will allow you to have multiple auth servers in your cluster for an HA deployment, but it will not also store Teleport audit events for you in the same way that DynamoDB or Firestore will. To configure Teleport for using etcd as a storage back-end: Make sure you are using etcd version 3.3 or newer. Install etcd and configure peer and client TLS authentication using the etcd security guide . You can use this script provided by etcd if you don't already have a TLS setup. Configure all Teleport Auth servers to use etcd in the \"storage\" section of the config file as shown below. Deploy several auth servers connected to etcd back-end. Deploy several proxy nodes that have auth_servers pointed to list of auth servers to connect to. teleport : storage : type : etcd # list of etcd peers to connect to: peers : [ \"https://172.17.0.1:4001\" , \"https://172.17.0.2:4001\" ] # required path to TLS client certificate and key files to connect to etcd # # to create these, follow # https://coreos.com/os/docs/latest/generate-self-signed-certificates.html # or use the etcd-provided script # https://github.com/etcd-io/etcd/tree/master/hack/tls-setup tls_cert_file : /var/lib/teleport/etcd-cert.pem tls_key_file : /var/lib/teleport/etcd-key.pem # optional file with trusted CA authority # file to authenticate etcd nodes # # if you used the script above to generate the client TLS certificate, # this CA certificate should be one of the other generated files tls_ca_file : /var/lib/teleport/etcd-ca.pem # alternative password based authentication, if not using TLS client # certificate # # See https://etcd.io/docs/v3.4.0/op-guide/authentication/ for setting # up a new user username : username password_file : /mnt/secrets/etcd-pass # etcd key (location) where teleport will be storing its state under. # make sure it ends with a '/'! prefix : /teleport/ # NOT RECOMMENDED: enables insecure etcd mode in which self-signed # certificate will be accepted insecure : false","title":"Using etcd"},{"location":"admin-guide/#using-amazon-s3","text":"Tip Before continuing, please make sure to take a look at the cluster state section in Teleport Architecture documentation. AWS Authentication The configuration examples below contain AWS access keys and secret keys. They are optional, they exist for your convenience but we DO NOT RECOMMEND using them in production. If Teleport is running on an AWS instance it will automatically use the instance IAM role. Teleport also will pick up AWS credentials from the ~/.aws folder, just like the AWS CLI tool. S3 buckets can only be used as a storage for the recorded sessions. S3 cannot store the audit log or the cluster state. Below is an example of how to configure a Teleport auth server to store the recorded sessions in an S3 bucket. teleport : storage : # The region setting sets the default AWS region for all AWS services # Teleport may consume (DynamoDB, S3) region : us-east-1 # Path to S3 bucket to store the recorded sessions in. audit_sessions_uri : \"s3://Example_TELEPORT_S3_BUCKET/records\" # Teleport assumes credentials. Using provider chains, assuming IAM role or # standard .aws/credentials in the home folder. The AWS authentication settings above can be omitted if the machine itself is running on an EC2 instance with an IAM role.","title":"Using Amazon S3"},{"location":"admin-guide/#using-dynamodb","text":"Tip Before continuing, please make sure to take a look at the cluster state section in Teleport Architecture documentation. If you are running Teleport on AWS, you can use DynamoDB as a storage back-end to achieve high availability. DynamoDB back-end supports two types of Teleport data: Cluster state Audit log events DynamoDB cannot store the recorded sessions. You are advised to use AWS S3 for that as shown above. To configure Teleport to use DynamoDB: Make sure you have AWS access key and a secret key which give you access to DynamoDB account. If you're using (as recommended) an IAM role for this, the policy with necessary permissions is listed below. Configure all Teleport Auth servers to use DynamoDB back-end in the \"storage\" section of teleport.yaml as shown below. Deploy several auth servers connected to DynamoDB storage back-end. Deploy several proxy nodes. Make sure that all Teleport nodes have auth_servers configuration setting populated with the auth servers. teleport : storage : type : dynamodb # Region location of dynamodb instance, https://docs.aws.amazon.com/en_pv/general/latest/gr/rande.html#ddb_region region : us-east-1 # Name of the DynamoDB table. If it does not exist, Teleport will create it. table_name : Example_TELEPORT_DYNAMO_TABLE_NAME # This setting configures Teleport to send the audit events to three places: # To keep a copy in DynamoDB, a copy on a local filesystem, and also output the events to stdout. # NOTE: The DynamoDB events table has a different schema to the regular Teleport # database table, so attempting to use same table for both will result in errors. # When using highly available storage like DynamoDB, you should make sure that the list always specifies # the HA storage method first, as this is what the Teleport web UI uses as its source of events to display. audit_events_uri : [ 'dynamodb://events_table_name' , 'file:///var/lib/teleport/audit/events' , 'stdout://' ] # This setting configures Teleport to save the recorded sessions in an S3 bucket: audit_sessions_uri : s3://Example_TELEPORT_S3_BUCKET/records Replace us-east-1 and Example_TELEPORT_DYNAMO_TABLE_NAME with your own settings. Teleport will create the table automatically. Example_TELEPORT_DYNAMO_TABLE_NAME and events_table_name must be different DynamoDB tables. The schema is different for each. Using the same table name for both will result in errors. The AWS authentication setting above can be omitted if the machine itself is running on an EC2 instance with an IAM role. Audit log settings above are optional. If specified, Teleport will store the audit log in DynamoDB and the session recordings must be stored in an S3 bucket, i.e. both audit_xxx settings must be present. If they are not set, Teleport will default to a local file system for the audit log, i.e. /var/lib/teleport/log on an auth server. If DynamoDB is used for the audit log, the logged events will be stored with a TTL of 1 year. Currently this TTL is not configurable. Access to DynamoDB Make sure that the IAM role assigned to Teleport is configured with the sufficient access to DynamoDB. Below is the example of the IAM policy you can use: { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"AllAPIActionsOnTeleportAuth\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:eu-west-1:123456789012:table/prod.teleport.auth\" }, { \"Sid\" : \"AllAPIActionsOnTeleportStreams\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:eu-west-1:123456789012:table/prod.teleport.auth/stream/*\" } ] }","title":"Using DynamoDB"},{"location":"admin-guide/#using-gcs","text":"Tip Before continuing, please make sure to take a look at the cluster state section in Teleport Architecture documentation. Google Cloud Storage (GCS) can only be used as a storage for the recorded sessions. GCS cannot store the audit log or the cluster state. Below is an example of how to configure a Teleport auth server to store the recorded sessions in a GCS bucket. teleport : storage : # Path to GCS to store the recorded sessions in. audit_sessions_uri : \"gs://Example_TELEPORT_STORAGE/records\" credentials_path : /var/lib/teleport/gcs_creds","title":"Using GCS"},{"location":"admin-guide/#using-firestore","text":"Tip Before continuing, please make sure to take a look at the cluster state section in Teleport Architecture documentation. If you are running Teleport on GCP, you can use Firestore as a storage back-end to achieve high availability. Firestore back-end supports two types of Teleport data: Cluster state Audit log events Firestore cannot store the recorded sessions. You are advised to use Google Cloud Storage (GCS) for that as shown above. To configure Teleport to use Firestore: Configure all Teleport Auth servers to use Firestore back-end in the \"storage\" section of teleport.yaml as shown below. Deploy several auth servers connected to Firestore storage back-end. Deploy several proxy nodes. Make sure that all Teleport nodes have auth_servers configuration setting populated with the auth servers or use a load balancer for the auth servers in high availability mode. teleport : storage : type : firestore # Project ID https://support.google.com/googleapi/answer/7014113?hl=en project_id : Example_GCP_Project_Name # Name of the Firestore table. If it does not exist, Teleport won't start collection_name : Example_TELEPORT_FIRESTORE_TABLE_NAME credentials_path : /var/lib/teleport/gcs_creds # This setting configures Teleport to send the audit events to three places: # To keep a copy in Firestore, a copy on a local filesystem, and also write the events to stdout. # NOTE: The Firestore events table has a different schema to the regular Teleport # database table, so attempting to use same table for both will result in errors. # When using highly available storage like Firestore, you should make sure that the list always specifies # the HA storage method first, as this is what the Teleport web UI uses as its source of events to display. audit_events_uri : [ 'firestore://Example_TELEPORT_FIRESTORE_EVENTS_TABLE_NAME' , 'file:///var/lib/teleport/audit/events' , 'stdout://' ] # This setting configures Teleport to save the recorded sessions in GCP storage: audit_sessions_uri : gs://Example_TELEPORT_S3_BUCKET/records Replace Example_GCP_Project_Name and Example_TELEPORT_FIRESTORE_TABLE_NAME with your own settings. Teleport will create the table automatically. Example_TELEPORT_FIRESTORE_TABLE_NAME and Example_TELEPORT_FIRESTORE_EVENTS_TABLE_NAME must be different Firestore tables. The schema is different for each. Using the same table name for both will result in errors. The GCP authentication setting above can be omitted if the machine itself is running on a GCE instance with a Service Account that has access to the Firestore table. Audit log settings above are optional. If specified, Teleport will store the audit log in Firestore and the session recordings must be stored in a GCP bucket, i.e.both audit_xxx settings must be present. If they are not set, Teleport will default to a local file system for the audit log, i.e. /var/lib/teleport/log on an auth server.","title":"Using Firestore"},{"location":"admin-guide/#upgrading-teleport","text":"Teleport is always a critical component of the infrastructure it runs on. This is why upgrading to a new version must be performed with caution. Teleport is a much more capable system than a bare bones SSH server. While it offers significant benefits on a cluster level, it also adds some complexity to cluster upgrades. To ensure robust operation Teleport administrators must follow the upgrade rules listed below.","title":"Upgrading Teleport"},{"location":"admin-guide/#production-releases","text":"First of all, avoid running pre-releases (release candidates) in production environments. Teleport development team uses Semantic Versioning which makes it easy to tell if a specific version is recommended for production use.","title":"Production Releases"},{"location":"admin-guide/#component-compatibility","text":"When running multiple binaries of Teleport within a cluster (nodes, proxies, clients, etc), the following rules apply: Patch versions are always compatible, for example any 4.0.1 component will work with any 4.0.3 component. Other versions are always compatible with their previous release. This means you must not attempt to upgrade from 4.1 straight to 4.3. You must upgrade to 4.2 first. Teleport clients tsh for users and tctl for admins may not be compatible with different versions of the teleport service. As an extra precaution you might want to backup your application prior to upgrading. We provide more instructions in Backup before upgrading . Upgrading to Teleport 4.0+ Teleport 4.0+ switched to GRPC and HTTP/2 as an API protocol. The HTTP/2 spec bans two previously recommended ciphers. tls-rsa-with-aes-128-gcm-sha256 & tls-rsa-with-aes-256-gcm-sha384 , make sure these are removed from teleport.yaml Visit our community for more details If upgrading you might want to consider rotating CA to SHA-256 or SHA-512 for RSA SSH certificate signatures. The previous default was SHA-1, which is now considered weak against brute-force attacks. SHA-1 certificate signatures are also no longer accepted by OpenSSH versions 8.2 and above. All new Teleport clusters will default to SHA-512 based signatures. To upgrade an existing cluster, set the following in your teleport.yaml: teleport: ca_signature_algo: \"rsa-sha2-512\" After updating to 4.3+ rotate the cluster CA following these docs .","title":"Component Compatibility"},{"location":"admin-guide/#backup-before-upgrading","text":"As an extra precaution you might want to backup your application prior to upgrading. We have more instructions in Backing up Teleport .","title":"Backup Before Upgrading"},{"location":"admin-guide/#upgrade-sequence","text":"When upgrading a single Teleport cluster: Upgrade the auth server first . The auth server keeps the cluster state and if there are data format changes introduced in the new version this will perform necessary migrations. Then, upgrade the proxy servers. The proxy servers are stateless and can be upgraded in any sequence or at the same time. Finally, upgrade the SSH nodes in any sequence or at the same time. Warning If several auth servers are running in HA configuration (for example, in AWS auto-scaling group) you have to shrink the group to just one auth server prior to performing an upgrade. While Teleport will attempt to perform any necessary migrations, we recommend users create a backup of their backend before upgrading the Auth Server, as a precaution. This allows for a safe rollback in case the migration itself fails. When upgrading multiple clusters: First, upgrade the main cluster, i.e. the one which other clusters trust. Upgrade the trusted clusters.","title":"Upgrade Sequence"},{"location":"admin-guide/#backing-up-teleport","text":"When planning a backup of Teleport, it's important to know what is where and the importance of each component. Teleport's Proxies and Nodes are stateless, and thus only teleport.yaml should be backed up. The Auth server is Teleport's brains, and depending on the backend should be backed up regularly. For example a customer running Teleport on AWS with DynamoDB have these key items of data: What Where ( Example AWS Customer ) Local Users ( not SSO ) DynamoDB Certificate Authorities DynamoDB Trusted Clusters DynamoDB Connectors: SSO DynamoDB / File System RBAC DynamoDB / File System teleport.yaml File System teleport.service File System license.pem File System TLS key/certificate ( File System / Outside Scope ) Audit log DynamoDB Session recordings S3 For this customer, we would recommend using AWS best practices for backing up DynamoDB. If DynamoDB is used for the audit log, logged events have a TTL of 1 year. Backend Recommended backup strategy dir ( local filesystem ) Backup /var/lib/teleport/storage directory and the output of tctl get all . DynamoDB Follow AWS Guidelines for Backup & Restore etcd Follow etcD Guidleines for Disaster Recovery Firestore Follow GCP Guidlines for Automated Backups","title":"Backing Up Teleport"},{"location":"admin-guide/#teleport-resources","text":"Teleport uses YAML resources for roles, trusted clusters, local users and auth connectors. These could be created via tctl or via the UI.","title":"Teleport Resources"},{"location":"admin-guide/#gitops","text":"If running Teleport at scale, it's important for teams to have an automated way to restore Teleport. At a high level, this is our recommended approach: Persist and backup your backend Share that backend among auth servers Store your configs as discrete files in VCS Have your CI run tctl create -f *.yaml from that git directory","title":"GitOps"},{"location":"admin-guide/#migrating-backends","text":"As of version v4.1 you can now quickly export a collection of resources from Teleport. This feature was designed to help customers migrate from local storage to etcd. Using tctl get all will retrieve the below items: Users Certificate Authorities Trusted Clusters Connectors: Github SAML [Teleport Enterprise] OIDC [Teleport Enterprise] Roles [Teleport Enterprise] When migrating backends, you should back up your auth server's data_dir/storage directly. Example of backing up and restoring a cluster. # export dynamic configuration state from old cluster $ tctl get all > state.yaml # prepare a new uninitialized backend (make sure to port # any non-default config values from the old config file) $ mkdir fresh && cat > fresh.yaml << EOF teleport: data_dir: fresh EOF # bootstrap fresh server (kill the old one first!) $ teleport start --config fresh.yaml --bootstrap state.yaml # from another terminal, verify state transferred correctly $ tctl --config fresh.yaml get all # <your state here!> The --bootstrap flag has no effect, except during backend initialization (performed by auth server on first start), so it is safe for use in supervised/HA contexts. Limitations All the same limitations around modifying the config file of an existing cluster also apply to a new cluster being bootstrapped from the state of an old cluster. Of particular note: Changing cluster name will break your CAs (this will be caught and teleport will refuse to start). Some user authentication mechanisms (e.g. u2f) require that the public endpoint of the web ui remains the same (this can't be caught by teleport, be careful!). Any node whose invite token is defined statically (in the config file of the auth server) will be able to join automatically, but nodes that were added dynamically will need to be re-invited","title":"Migrating Backends."},{"location":"admin-guide/#daemon-restarts","text":"As covered in the Graceful Restarts section, Teleport supports graceful restarts. To upgrade a host to a newer Teleport version, an administrator must: Replace the Teleport binaries, usually teleport and tctl Execute systemctl restart teleport This will perform a graceful restart, i.e.the Teleport daemon will fork a new process to handle new incoming requests, leaving the old daemon process running until existing clients disconnect.","title":"Daemon Restarts"},{"location":"admin-guide/#license-file","text":"Commercial Teleport subscriptions require a valid license. The license file can be downloaded from the Teleport Customer Portal . The Teleport license file contains a X.509 certificate and the corresponding private key in PEM format. Place the downloaded file on Auth servers and set the license_file configuration parameter of your teleport.yaml to point to the file location: auth_service : license_file : /var/lib/teleport/license.pem The license_file path can be either absolute or relative to the configured data_dir . If license file path is not set, Teleport will look for the license.pem file in the configured data_dir . NOTE Only Auth servers require the license. Proxies and Nodes that do not also have Auth role enabled do not need the license.","title":"License File"},{"location":"admin-guide/#troubleshooting","text":"To diagnose problems you can configure teleport to run with verbose logging enabled by passing it -d flag. NOTE It is not recommended to run Teleport in production with verbose logging as it generates a substantial amount of data. Sometimes you may want to reset teleport to a clean state. This can be accomplished by erasing everything under \"data_dir\" directory. Assuming the default location, rm -rf /var/lib/teleport/* will do. Teleport also supports HTTP endpoints for monitoring purposes. They are disabled by default, but you can enable them: $ teleport start --diag-addr = 127 .0.0.1:3000 Now you can see the monitoring information by visiting several endpoints: http://127.0.0.1:3000/metrics is the list of internal metrics Teleport is tracking. It is compatible with Prometheus collectors. For a full list of metrics review our metrics reference . http://127.0.0.1:3000/healthz returns \"OK\" if the process is healthy or 503 otherwise. http://127.0.0.1:3000/readyz is similar to /healthz , but it returns \"OK\" only after the node successfully joined the cluster, i.e.it draws the difference between \"healthy\" and \"ready\". http://127.0.0.1:3000/debug/pprof/ is Golang's standard profiler. It's only available when -d flag is given in addition to --diag-addr","title":"Troubleshooting"},{"location":"admin-guide/#getting-help","text":"If you need help, please ask on our community forum . You can also open an issue on Github . For commercial support, you can create a ticket through the customer dashboard . For more information about custom features, or to try our Enterprise edition of Teleport, please reach out to us at sales@gravitational.com .","title":"Getting Help"},{"location":"api-reference/","text":"Teleport API Reference Teleport is currently working on documenting our API. Warning We are currently working on this project. If you have an API suggestion, please complete our survey . Authentication In order to interact with the Access Request API, you will need to provision appropriate TLS certificates. In order to provision certificates, you will need to create a user with appropriate permissions: $ cat > rscs.yaml <<EOF kind: user metadata: name: access-plugin spec: roles: ['access-plugin'] version: v2 --- kind: role metadata: name: access-plugin spec: allow: rules: - resources: ['access_request'] verbs: ['list','read','update'] # teleport currently refuses to issue certs for a user with 0 logins, # this restriction may be lifted in future versions. logins: ['access-plugin'] version: v3 EOF # ... $ tctl create rscs.yaml # ... $ tctl auth sign --format = tls --user = access-plugin --out = auth # ... The above sequence should result in three PEM encoded files being generated: auth.crt , auth.key , and auth.cas (certificate, private key, and CA certs respectively). Note: by default, tctl auth sign produces certificates with a relatively short lifetime. For production deployments, the --ttl flag can be used to ensure a more practical certificate lifetime. gRPC APIs Audit Events API Coming Soon Certificate Generation API Coming Soon Tokens API Coming Soon Workflow API Coming Soon","title":"API"},{"location":"api-reference/#teleport-api-reference","text":"Teleport is currently working on documenting our API. Warning We are currently working on this project. If you have an API suggestion, please complete our survey .","title":"Teleport API Reference"},{"location":"api-reference/#authentication","text":"In order to interact with the Access Request API, you will need to provision appropriate TLS certificates. In order to provision certificates, you will need to create a user with appropriate permissions: $ cat > rscs.yaml <<EOF kind: user metadata: name: access-plugin spec: roles: ['access-plugin'] version: v2 --- kind: role metadata: name: access-plugin spec: allow: rules: - resources: ['access_request'] verbs: ['list','read','update'] # teleport currently refuses to issue certs for a user with 0 logins, # this restriction may be lifted in future versions. logins: ['access-plugin'] version: v3 EOF # ... $ tctl create rscs.yaml # ... $ tctl auth sign --format = tls --user = access-plugin --out = auth # ... The above sequence should result in three PEM encoded files being generated: auth.crt , auth.key , and auth.cas (certificate, private key, and CA certs respectively). Note: by default, tctl auth sign produces certificates with a relatively short lifetime. For production deployments, the --ttl flag can be used to ensure a more practical certificate lifetime.","title":"Authentication"},{"location":"api-reference/#grpc-apis","text":"","title":"gRPC APIs"},{"location":"api-reference/#audit-events-api","text":"Coming Soon","title":"Audit Events API"},{"location":"api-reference/#certificate-generation-api","text":"Coming Soon","title":"Certificate Generation API"},{"location":"api-reference/#tokens-api","text":"Coming Soon","title":"Tokens API"},{"location":"api-reference/#workflow-api","text":"Coming Soon","title":"Workflow API"},{"location":"aws-oss-guide/","text":"Running Teleport on AWS We've created this guide to give customers a high level overview of how to use Teleport on Amazon Web Services (AWS). This guide provides a high level introduction leading to a deep dive into how to setup and run Teleport in production. We have split this guide into: Teleport on AWS FAQ Authenticating to EKS Using GitHub Credentials with Teleport Community Edition Setting up Teleport Enterprise on AWS Teleport AWS Tips & Tricks AWS HA with Terraform Teleport on AWS FAQ Why would you want to use Teleport with AWS? At some point you'll want to log into the system using SSH to help test, debug and troubleshoot a problem box. For EC2, AWS recommends creating 'Key Pairs' and has a range of other tips for securing EC2 instances . This approach has a number of limitations: As your organization grows, keeping track of end users' public/private keys becomes an administrative nightmare. Using SSH public/private keys has a number of limitations. Read why SSH Certificates are better . Once a machine has been bootstrapped with SSH Keys, there isn't an easy way to add new keys and delegate access. Which Services can I use Teleport with? You can use Teleport for all the services that you would SSH into. This guide is focused on EC2. We have a short blog post on using Teleport with EKS . We plan to expand the guide based on feedback but will plan to add instructions for the below. RDS Detailed EKS Lightsail Fargate AWS ECS Teleport Introduction This guide will cover how to setup, configure and run Teleport on AWS . AWS Services required to run Teleport in HA EC2 / Autoscale DynamoDB S3 Route53 NLB IAM ACM SSM We recommend setting up Teleport in high availability mode (HA). In HA mode DynamoDB stores the state of the system and S3 will store audit logs. EC2 / Autoscale To run Teleport in a HA configuration we recommend using m4.large instances. It's best practice to separate the proxy and authentication server, using autoscaling groups for both machines. We have pre-built AMIs for both Teleport OSS and Enterprise editions. Instructions on using these AMIs are below . DynamoDB DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. For large clusters you can provision usage but for smaller deployments you can leverage DynamoDB's autoscaling. Teleport 4.0 leverages DynamoDB's streaming feature . When turning this on, you'll need to specify New Image from the streaming options. DynamoDB back-end supports two types of Teleport data: Cluster state Cluster events See DynamoDB Admin Guide for more information Setting Stream to NEW IMAGE For maintainability and ease of use, we recommend following our Terraform example but below are high level definitions for the tables required to run Teleport. DynamoDB Table A - Teleport Cluster State: Table name teleport-cluster-name Primary partition key HashKey (String) Primary sort key FullPath (String) DynamoDB Table B - Teleport Cluster Events: Table name teleport-cluster-name-events Primary partition key SessionID (String) Primary sort key EventIndex (Number) Indexes - timesearch teleport-cluster-name-events Primary partition key EventNamespace (String) Primary sort key CreatedAt (Number) Attributes ALL S3 Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. In this Teleport setup, S3 will provide storage for recorded sessions. We recommend using Amazon S3 Standard. Tip S3 provides Amazon S3 Object Lock , which is useful for customers deploying Teleport in regulated environments. Route53 Route53 is a highly available Domain Name System (DNS) provided by AWS. It'll be needed to setup a URL for the proxy - we recommend using a subdomain. e.g. teleport.acmeinc.com NLB: Network Load Balancer AWS provides many different load balancers. To setup Teleport, we recommend using a Network Load Balancer. Network Load Balancers provides TLS for the Teleport proxy and provides the TCP connections needed for Teleport proxy SSH connections. IAM IAM is the recommended tool for creating service access. This guide will follow the best practice of principle of least privilege (PoLP). IAM for Amazon S3 In order to grant an IAM user in your AWS account access to one of your buckets, example.s3.bucket you will need to grant the following permissions: s3:ListBucket , s3:ListBucketVersions , s3:PutObject , s3:GetObject , s3:GetObjectVersion An example policy is shown below: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" , \"s3:ListBucketVersions\" ], \"Resource\" : [ \"arn:aws:s3:::example.s3.bucket\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:GetObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::example.s3.bucket/*\" ] } ] } Note example.s3.bucket will need to be replaced with your bucket name. IAM for DynamoDB In order to grant an IAM user access to DynamoDB make sure that the IAM role assigned to Teleport is configured with proper permissions. An example policy is shown below: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllAPIActionsOnTeleportClusterState\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:REGION:ACCOUNTID:table/DYNAMODB-RESOURCE-A\" }, { \"Sid\" : \"AllAPIActionsOnTeleportClusterStateStreams\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:REGION:ACCOUNTID:table/DYNAMODB-RESOURCE-A/stream/*\" }, { \"Sid\" : \"AllAPIActionsOnTeleportClusterEvents\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:REGION:ACCOUNTID:table/DYNAMODB-RESOURCE-B\" }, { \"Sid\" : \"AllAPIActionsOnTeleportClusterEventsStreams\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:REGION:ACCOUNTID:table/DYNAMODB-RESOURCE-B/stream/*\" }, { \"Sid\" : \"AllAPIActionsOnTeleportClusterEventIndex\" , \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : \"arn:aws:dynamodb:REGION:ACCOUNTID:table/DYNAMODB-RESOURCE-B/index/*\" }, ] } Note arn:aws:dynamodb:REGION:ACCOUNTID:table/DYNAMODB-RESOURCE-A and RESOURCE-B will need to be replaced with your two DynamoDB tables. ACM With AWS Certificate Manager, you can quickly request SSL/TLS certificates. TLS Cert: Used to provide SSL for the proxy. SSH Certs (not in ACM): Created and self signed by the authentication server and are used to delegate access to Teleport nodes. AWS Systems Manager Parameter Store To add new nodes to a Teleport Cluster, we recommend using a strong static token . SSM can be also used to store the enterprise licence. Setting up a HA Teleport Cluster Teleport's config based setup offers a wide range of customization for customers. This guide offers a range of setup options for AWS. If you have a very large account, multiple accounts, or over 10k users we would recommend getting in touch. We are more than happy to help you architect, setup and deploy Teleport into your environment. We have these options for you. Deploying with CloudFormation Deploying with Terraform HA + Monitoring Deploying with CloudFormation We are currently working on an updated CloudFormation guide but you can start with our AWS AMI example . It requires a VPC, but we expect customers to deploy within an already existing VPC. Deploying with Terraform To deploy Teleport in AWS using Terraform look at our AWS Guide . Installing Teleport to EC2 Server Customers run many workloads within EC2 and depending on how you work there are many ways to integrate Teleport onto your servers. We recommend looking at our Admin manual . In short, to add new nodes / EC2 servers that you can \"SSH into\" you'll need to Install the Teleport Binary on the Server Run Teleport, we recommend using SystemD Set the correct settings in /etc/teleport.yaml Add EC2 nodes to the Teleport cluster Upgrading To upgrade to a newer version of Teleport: Back up /etc/teleport.yaml , /etc/teleport.d/ and the contents of /var/lib/teleport Launch a new instance with the correct AMI for a newer version of Teleport Copy /etc/teleport.yaml , /etc/teleport.d/ and /var/lib/teleport to the new instance, overwriting anything that already exists Backup the contents and copy them over to the new instance. Either restart the instance, or log in via SSH and run sudo systemctl restart teleport.service Using Teleport with EKS In this section, we will set Kubernetes RBAC permissions needed by Teleport and configure the EC2 instance running Teleport to map to those permissions. Prerequisites You\u2019ll need a functioning EKS cluster, we recommend version 1.16. If you\u2019re unfamiliar with creating an EKS cluster, view AWS EKS getting started guide . EKS Version: The below guide has been tested with Kubernetes 1.16 eksctl . Make sure you have eksctl version 0.22.0. jq installed on your local machine. An AWS Account with Root Access Create cluster role and role binding The first step is to create a cluster role and role binding that will allow the Teleport EC2 instance to relay user requests to the Kubernetes API. The below command is ran on a machine with kubectl access to the cluster. $ cat << 'EOF' | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: teleport-impersonation rules: - apiGroups: - \"\" resources: - users - groups - serviceaccounts verbs: - impersonate - apiGroups: - \"authorization.k8s.io\" resources: - selfsubjectaccessreviews verbs: - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: teleport-crb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: teleport-impersonation subjects: - kind: Group name: teleport-group EOF If successful the terminal should output: # clusterrole.rbac.authorization.k8s.io/teleport-impersonation created # clusterrolebinding.rbac.authorization.k8s.io/teleport-crb created Create IAM trust policy document This is the trust policy that allows the Teleport EC2 instance to assume a role. $ cat > teleport_assume_role.json << 'EOF' { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } EOF This will create a .json file locally, the below command should then be run to create the role. $ aws iam create-role --role-name teleport-role --assume-role-policy-document file://teleport_assume_role.json Create IAM policy granting list-clusters and describe-cluster permissions (optional) This policy is necessary to create a kubeconfig file using the aws eks update-kubeconfig command. If you have another mechanism to create a kubeconfig file on the instance that runs Teleport, this step is not required. cat > teleport_eks_desc_and_list.json << 'EOF' { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"eks:DescribeCluster\", \"eks:ListClusters\" ], \"Resource\": \"*\" } ] } EOF POLICY_ARN = $( aws iam create-policy --policy-name teleport-policy --policy-document file://teleport_eks_desc_and_list.json | jq -r '.Policy.Arn' ) aws iam attach-role-policy --role-name teleport-role --policy-arn $POLICY_ARN Map IAM role to Kubernetes group This maps the IAM role teleport-role to the Kubernetes group teleport-group . Note If you used eksctl to create your cluster, you may need to add the mapUsers section to the aws-auth ConfigMap before executing these commands. eksctl create iamidentitymapping --cluster [cluster-name] --arn arn:aws:iam::[account-id]:role/teleport-role --group teleport-group --username teleport ROLE = \" - userarn: arn:aws:iam::[account-id]:role/teleport-role\\n username: teleport\\n groups:\\n - teleport-group\" kubectl get -n kube-system configmap/aws-auth -o yaml | awk \"/mapRoles: \\|/{print;print \\\" $ROLE \\\";next}1\" > /tmp/aws-auth-patch.yml kubectl patch configmap/aws-auth -n kube-system --patch \" $( cat /tmp/aws-auth-patch.yml ) \" To check, run kubectl describe configmap -n kube-system aws-auth Installing Teleport Create EC2 instance Create an EC2 instance using the Teleport Community AMI on a public subnet in your VPC. Modify the security group associated with that instance to allow port 22 inbound, so you can SSH to the instance after it\u2019s running. You will need security group rules to allow access to ports 3080 and 3022-3026 so that users can access the Teleport server from the Internet. This will allow GitHub to post a response back to the Teleport server. You\u2019ll need to open port 80 to allow Let\u2019s Encrypt to complete HTTP validation when using SSL certificates/unless you provide pre-provisioned TLS certificates. Type Protocol Port Range Source Custom TCP 3022-3026 0.0.0.0/0 Custom TCP 3080 0.0.0.0/0 HTTP TCP 80 0.0.0.0/0 SSH TCP 22 your IP You must modify the EKS control plane security group to allow port 443 inbound from the Teleport security group, to allow your Teleport instance to communicate with the Kubernetes API. Assign role to instance: aws iam create-instance-profile --instance-profile-name teleport-role aws iam add-role-to-instance-profile --instance-profile-name teleport-role --role-name teleport-role aws ec2 associate-iam-instance-profile --iam-instance-profile Name = teleport-role --instance-id [ instance_id ] # instance_id should be replaced with the instance id of the instance where you intend to install Teleport. Install Teleport. Download and Install Teleport Setup systemd Setup Teleport config file. sudo cp teleport.yaml /etc/teleport.yaml . An example is below. {!examples/aws/eks/teleport.yaml!} Download Kubectl on Teleport Server Kubectl is required to obtain the correct kubeconfig, so Teleport can access the EKS Cluster. Instructions are below, or can be found on AWS Docs . # Download the Amazon EKS-vended kubectl binary for your cluster's Kubernetes version from Amazon S3: curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/linux/amd64/kubectl # (Optional) Verify the downloaded binary with the SHA-256 sum for your binary. curl -o kubectl.sha256 https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/linux/amd64/kubectl.sha256 # Check the SHA-256 sum for your downloaded binary. openssl sha256 kubectl # Apply execute permissions to the binary. chmod +x ./kubectl # Copy the binary to a folder in your PATH. If you have already installed a version of kubectl, then we recommend creating a $HOME/bin/kubectl and ensuring that $HOME/bin comes first in your $PATH. sudo mv ./kubectl /usr/local/bin # After you install kubectl, you can verify its version with the following command: kubectl version --short --client Create kubeconfig: aws eks update-kubeconfig --name [ teleport-cluster ] --region us-west-2 # Added new context arn:aws:eks:us-west-2:480176057099:cluster/teleport-eks-cluster to /home/ec2-user/.kube/config Creating TLS certificate for Teleport Create TLS certificate for HTTPs. It is absolutely crucial to properly configure TLS for HTTPS when you use Teleport Proxy in production. For simplicity, we are using Let\u2019s Encrypt to issue certificates and simple DNS resolution. However, using an Elastic IP and a Route53 domain name would be appropriate for production use cases. Install certbot: # Install the EPEL release package for RHEL 7 and enable the EPEL repository. sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum install -y certbot python-certbot-nginx Run Certbot export TELEPORT_PUBLIC_DNS_NAME =[ teleport-proxy-url ] echo $TELEPORT_PUBLIC_DNS_NAME export EMAIL =[ email-for-letsencrypt ] sudo certbot certonly --standalone \\ --preferred-challenges http \\ -d $TELEPORT_PUBLIC_DNS_NAME \\ -n \\ --agree-tos \\ --email = $EMAIL Setup Github Auth Run this on the Teleport EC2 Host, see Github Auth for more info. export TELEPORT_PUBLIC_DNS_NAME = \"[teleport-proxy-url]\" export GH_CLIENT_ID = \"[github-client-id]\" export GH_SECRET = \"[github-oauth-secret]\" export GH_ORG = \"[github-org]\" export GH_TEAM = \"[github-team]\" cat > github.yaml << EOF kind: github version: v3 metadata: # connector name that will be used with 'tsh --auth=github login' name: github spec: # client ID of Github OAuth app client_id: $GH_CLIENT_ID # client secret of Github OAuth app client_secret: $GH_SECRET # connector display name that will be shown on web UI login screen display: Github # callback URL that will be called after successful authentication redirect_url: https://$TELEPORT_PUBLIC_DNS_NAME:3080/v1/webapi/github/callback # mapping of org/team memberships onto allowed logins and roles teams_to_logins: - kubernetes_groups: # change this to restrict the access of users logging in via GitHub; # system:masters gives everyone full access to the entire cluster - system:masters logins: - github - ec2-user organization: $GH_ORG team: $GH_TEAM EOF Use tctl to create the github auth connector. sudo /usr/local/bin/tctl create -f ./github.yaml Testing Teleport & EKS Setup \u279c ~ tsh login --proxy=[teleport-proxy-url].practice.io:3080 If browser window does not open automatically, open it by clicking on the link: http://127.0.0.1:64467/54e5d06a-c509-4077-bf54-fb27fd1b8d50 > Profile URL: https://[teleport-proxy-url].practice.io:3080 Logged in as: benarent Cluster: teleport-eks Roles: admin* Logins: github, ec2-user Valid until: 2020-06-30 02:12:54 -0700 PDT [valid for 12h0m0s] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty * RBAC is only available in Teleport Enterprise https://gravitational.com/teleport/docs/enterprise On your local machine test using kubectl get pods --all-namespaces \u279c ~ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-56p6g 1 /1 Running 0 5h28m kube-system aws-node-7dv5j 1 /1 Running 0 5h28m kube-system coredns-5c97f79574-69m6k 1 /1 Running 0 5h36m kube-system coredns-5c97f79574-dq54w 1 /1 Running 0 5h36m kube-system kube-proxy-7w4z4 1 /1 Running 0 5h28m kube-system kube-proxy-c5nv2 1 /1 Running 0 5h28m Running Teleport Enterprise on AWS Most of this guide has been designed for OSS Teleport. Most of this guide also applies to Teleport Enterprise with a few extra notes around adding a license and getting the correct binary. We've a few other resources for Enterprise customers such as our Running Teleport Enterprise in HA mode on AWS using Terraform Teleport Enterprise Quickstart If you would like help setting up Teleport Enterprise on AWS, please mail us at info@gravitational.com Teleport AWS Tips & Tricks Generating labels from AWS tags Labels can be a useful addition to the Teleport UI. Simply add some or all of the below to Teleport Nodes in etc/teleport.yaml to have helpful labels in the Teleport UI. commands : - name : arch command : [ uname , -p ] period : 1h0m0s - name : kernel command : [ uname , -r ] period : 1h0m0s - name : uptime command : [ uptime , -p ] period : 1h0m0s - name : internal command : [ curl , \"http://169.254.169.254/latest/meta-data/local-ipv4\" ] period : 1h0m0s - name : external command : [ curl , \"http://169.254.169.254/latest/meta-data/public-ipv4\" ] period : 1h0m0s Create labels based on EC2 Tags .","title":"AWS"},{"location":"aws-oss-guide/#running-teleport-on-aws","text":"We've created this guide to give customers a high level overview of how to use Teleport on Amazon Web Services (AWS). This guide provides a high level introduction leading to a deep dive into how to setup and run Teleport in production. We have split this guide into: Teleport on AWS FAQ Authenticating to EKS Using GitHub Credentials with Teleport Community Edition Setting up Teleport Enterprise on AWS Teleport AWS Tips & Tricks AWS HA with Terraform","title":"Running Teleport on AWS"},{"location":"aws-oss-guide/#teleport-on-aws-faq","text":"Why would you want to use Teleport with AWS? At some point you'll want to log into the system using SSH to help test, debug and troubleshoot a problem box. For EC2, AWS recommends creating 'Key Pairs' and has a range of other tips for securing EC2 instances . This approach has a number of limitations: As your organization grows, keeping track of end users' public/private keys becomes an administrative nightmare. Using SSH public/private keys has a number of limitations. Read why SSH Certificates are better . Once a machine has been bootstrapped with SSH Keys, there isn't an easy way to add new keys and delegate access. Which Services can I use Teleport with? You can use Teleport for all the services that you would SSH into. This guide is focused on EC2. We have a short blog post on using Teleport with EKS . We plan to expand the guide based on feedback but will plan to add instructions for the below. RDS Detailed EKS Lightsail Fargate AWS ECS","title":"Teleport on AWS FAQ"},{"location":"aws-oss-guide/#teleport-introduction","text":"This guide will cover how to setup, configure and run Teleport on AWS .","title":"Teleport Introduction"},{"location":"aws-oss-guide/#ec2-autoscale","text":"To run Teleport in a HA configuration we recommend using m4.large instances. It's best practice to separate the proxy and authentication server, using autoscaling groups for both machines. We have pre-built AMIs for both Teleport OSS and Enterprise editions. Instructions on using these AMIs are below .","title":"EC2 / Autoscale"},{"location":"aws-oss-guide/#dynamodb","text":"DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. For large clusters you can provision usage but for smaller deployments you can leverage DynamoDB's autoscaling. Teleport 4.0 leverages DynamoDB's streaming feature . When turning this on, you'll need to specify New Image from the streaming options. DynamoDB back-end supports two types of Teleport data: Cluster state Cluster events See DynamoDB Admin Guide for more information Setting Stream to NEW IMAGE For maintainability and ease of use, we recommend following our Terraform example but below are high level definitions for the tables required to run Teleport. DynamoDB Table A - Teleport Cluster State: Table name teleport-cluster-name Primary partition key HashKey (String) Primary sort key FullPath (String) DynamoDB Table B - Teleport Cluster Events: Table name teleport-cluster-name-events Primary partition key SessionID (String) Primary sort key EventIndex (Number) Indexes - timesearch teleport-cluster-name-events Primary partition key EventNamespace (String) Primary sort key CreatedAt (Number) Attributes ALL","title":"DynamoDB"},{"location":"aws-oss-guide/#s3","text":"Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. In this Teleport setup, S3 will provide storage for recorded sessions. We recommend using Amazon S3 Standard. Tip S3 provides Amazon S3 Object Lock , which is useful for customers deploying Teleport in regulated environments.","title":"S3"},{"location":"aws-oss-guide/#route53","text":"Route53 is a highly available Domain Name System (DNS) provided by AWS. It'll be needed to setup a URL for the proxy - we recommend using a subdomain. e.g. teleport.acmeinc.com","title":"Route53"},{"location":"aws-oss-guide/#nlb-network-load-balancer","text":"AWS provides many different load balancers. To setup Teleport, we recommend using a Network Load Balancer. Network Load Balancers provides TLS for the Teleport proxy and provides the TCP connections needed for Teleport proxy SSH connections.","title":"NLB: Network Load Balancer"},{"location":"aws-oss-guide/#iam","text":"IAM is the recommended tool for creating service access. This guide will follow the best practice of principle of least privilege (PoLP).","title":"IAM"},{"location":"aws-oss-guide/#acm","text":"With AWS Certificate Manager, you can quickly request SSL/TLS certificates. TLS Cert: Used to provide SSL for the proxy. SSH Certs (not in ACM): Created and self signed by the authentication server and are used to delegate access to Teleport nodes.","title":"ACM"},{"location":"aws-oss-guide/#aws-systems-manager-parameter-store","text":"To add new nodes to a Teleport Cluster, we recommend using a strong static token . SSM can be also used to store the enterprise licence.","title":"AWS Systems Manager Parameter Store"},{"location":"aws-oss-guide/#setting-up-a-ha-teleport-cluster","text":"Teleport's config based setup offers a wide range of customization for customers. This guide offers a range of setup options for AWS. If you have a very large account, multiple accounts, or over 10k users we would recommend getting in touch. We are more than happy to help you architect, setup and deploy Teleport into your environment. We have these options for you. Deploying with CloudFormation Deploying with Terraform HA + Monitoring","title":"Setting up a HA Teleport Cluster"},{"location":"aws-oss-guide/#deploying-with-cloudformation","text":"We are currently working on an updated CloudFormation guide but you can start with our AWS AMI example . It requires a VPC, but we expect customers to deploy within an already existing VPC.","title":"Deploying with CloudFormation"},{"location":"aws-oss-guide/#deploying-with-terraform","text":"To deploy Teleport in AWS using Terraform look at our AWS Guide .","title":"Deploying with Terraform"},{"location":"aws-oss-guide/#installing-teleport-to-ec2-server","text":"Customers run many workloads within EC2 and depending on how you work there are many ways to integrate Teleport onto your servers. We recommend looking at our Admin manual . In short, to add new nodes / EC2 servers that you can \"SSH into\" you'll need to Install the Teleport Binary on the Server Run Teleport, we recommend using SystemD Set the correct settings in /etc/teleport.yaml Add EC2 nodes to the Teleport cluster","title":"Installing Teleport to EC2 Server"},{"location":"aws-oss-guide/#upgrading","text":"To upgrade to a newer version of Teleport: Back up /etc/teleport.yaml , /etc/teleport.d/ and the contents of /var/lib/teleport Launch a new instance with the correct AMI for a newer version of Teleport Copy /etc/teleport.yaml , /etc/teleport.d/ and /var/lib/teleport to the new instance, overwriting anything that already exists Backup the contents and copy them over to the new instance. Either restart the instance, or log in via SSH and run sudo systemctl restart teleport.service","title":"Upgrading"},{"location":"aws-oss-guide/#using-teleport-with-eks","text":"In this section, we will set Kubernetes RBAC permissions needed by Teleport and configure the EC2 instance running Teleport to map to those permissions.","title":"Using Teleport with EKS"},{"location":"aws-oss-guide/#prerequisites","text":"You\u2019ll need a functioning EKS cluster, we recommend version 1.16. If you\u2019re unfamiliar with creating an EKS cluster, view AWS EKS getting started guide . EKS Version: The below guide has been tested with Kubernetes 1.16 eksctl . Make sure you have eksctl version 0.22.0. jq installed on your local machine. An AWS Account with Root Access","title":"Prerequisites"},{"location":"aws-oss-guide/#create-cluster-role-and-role-binding","text":"The first step is to create a cluster role and role binding that will allow the Teleport EC2 instance to relay user requests to the Kubernetes API. The below command is ran on a machine with kubectl access to the cluster. $ cat << 'EOF' | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: teleport-impersonation rules: - apiGroups: - \"\" resources: - users - groups - serviceaccounts verbs: - impersonate - apiGroups: - \"authorization.k8s.io\" resources: - selfsubjectaccessreviews verbs: - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: teleport-crb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: teleport-impersonation subjects: - kind: Group name: teleport-group EOF If successful the terminal should output: # clusterrole.rbac.authorization.k8s.io/teleport-impersonation created # clusterrolebinding.rbac.authorization.k8s.io/teleport-crb created","title":"Create cluster role and role binding"},{"location":"aws-oss-guide/#create-iam-trust-policy-document","text":"This is the trust policy that allows the Teleport EC2 instance to assume a role. $ cat > teleport_assume_role.json << 'EOF' { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } EOF This will create a .json file locally, the below command should then be run to create the role. $ aws iam create-role --role-name teleport-role --assume-role-policy-document file://teleport_assume_role.json","title":"Create IAM trust policy document"},{"location":"aws-oss-guide/#create-iam-policy-granting-list-clusters-and-describe-cluster-permissions-optional","text":"This policy is necessary to create a kubeconfig file using the aws eks update-kubeconfig command. If you have another mechanism to create a kubeconfig file on the instance that runs Teleport, this step is not required. cat > teleport_eks_desc_and_list.json << 'EOF' { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"eks:DescribeCluster\", \"eks:ListClusters\" ], \"Resource\": \"*\" } ] } EOF POLICY_ARN = $( aws iam create-policy --policy-name teleport-policy --policy-document file://teleport_eks_desc_and_list.json | jq -r '.Policy.Arn' ) aws iam attach-role-policy --role-name teleport-role --policy-arn $POLICY_ARN","title":"Create IAM policy granting list-clusters and describe-cluster permissions (optional)"},{"location":"aws-oss-guide/#map-iam-role-to-kubernetes-group","text":"This maps the IAM role teleport-role to the Kubernetes group teleport-group . Note If you used eksctl to create your cluster, you may need to add the mapUsers section to the aws-auth ConfigMap before executing these commands. eksctl create iamidentitymapping --cluster [cluster-name] --arn arn:aws:iam::[account-id]:role/teleport-role --group teleport-group --username teleport ROLE = \" - userarn: arn:aws:iam::[account-id]:role/teleport-role\\n username: teleport\\n groups:\\n - teleport-group\" kubectl get -n kube-system configmap/aws-auth -o yaml | awk \"/mapRoles: \\|/{print;print \\\" $ROLE \\\";next}1\" > /tmp/aws-auth-patch.yml kubectl patch configmap/aws-auth -n kube-system --patch \" $( cat /tmp/aws-auth-patch.yml ) \" To check, run kubectl describe configmap -n kube-system aws-auth","title":"Map IAM role to Kubernetes group"},{"location":"aws-oss-guide/#installing-teleport","text":"","title":"Installing Teleport"},{"location":"aws-oss-guide/#running-teleport-enterprise-on-aws","text":"Most of this guide has been designed for OSS Teleport. Most of this guide also applies to Teleport Enterprise with a few extra notes around adding a license and getting the correct binary. We've a few other resources for Enterprise customers such as our Running Teleport Enterprise in HA mode on AWS using Terraform Teleport Enterprise Quickstart If you would like help setting up Teleport Enterprise on AWS, please mail us at info@gravitational.com","title":"Running Teleport Enterprise on AWS"},{"location":"aws-oss-guide/#teleport-aws-tips-tricks","text":"","title":"Teleport AWS Tips &amp; Tricks"},{"location":"aws-oss-guide/#generating-labels-from-aws-tags","text":"Labels can be a useful addition to the Teleport UI. Simply add some or all of the below to Teleport Nodes in etc/teleport.yaml to have helpful labels in the Teleport UI. commands : - name : arch command : [ uname , -p ] period : 1h0m0s - name : kernel command : [ uname , -r ] period : 1h0m0s - name : uptime command : [ uptime , -p ] period : 1h0m0s - name : internal command : [ curl , \"http://169.254.169.254/latest/meta-data/local-ipv4\" ] period : 1h0m0s - name : external command : [ curl , \"http://169.254.169.254/latest/meta-data/public-ipv4\" ] period : 1h0m0s Create labels based on EC2 Tags .","title":"Generating labels from AWS tags"},{"location":"aws-terraform-guide/","text":"Running Teleport Enterprise in HA mode on AWS This guide is designed to accompany our reference Terraform code and describe how to use and administrate the resulting Teleport deployment. Running Teleport Enterprise in HA mode on AWS Prerequisites Get the Terraform code Set up variables region cluster_name ami_name key_name license_path route53_zone route53_domain s3_bucket_name email grafana_pass use_acm Reference deployment defaults Instances Cluster state database storage Audit event storage Recorded session storage Cluster domain Deploying with Terraform Accessing the cluster after Terraform setup Adding an admin user to the Teleport cluster Logging into the cluster with tsh Restarting/checking Teleport services LetsEncrypt ACM Adding EC2 instances to your Teleport cluster Getting the CA pin hash Getting the node join token Joining nodes via the Teleport auth server Joining nodes via Teleport IoT/node tunnelling Trusted clusters Script to quickly connect to instances Prerequisites Our code requires Terraform 0.12+. You can download Terraform here . We will assume that you have terraform installed and available on your path. $ which terraform /usr/local/bin/terraform $ terraform version Terraform v0.12.20 You will also require the aws command line tool. This is available in Ubuntu/Debian/Fedora/CentOS and MacOS Homebrew as the awscli package. Fedora/CentOS: yum -y install awscli Ubuntu/Debian: apt-get -y install awscli MacOS (with Homebrew ): brew install awscli When possible, installing via a package is always preferable. If you can't find a package available for your distribution, you can also download the tool from https://aws.amazon.com/cli/ We will assume that you have configured your AWS cli access with credentials available at ~/.aws/credentials : $ cat ~/.aws/credentials [ default ] aws_access_key_id = AKIA.... aws_secret_access_key = 8ZRxy.... You should also have a default region set under ~/.aws/config : $ cat ~/.aws/config [ default ] region = us-east-1 As a result, you should be able to run a command like aws ec2 describe-instances to list running EC2 instances. If you get an \"access denied\", \"403 Forbidden\" or similar message, you will need to grant additional permissions to the AWS IAM user that your aws_access_key_id and aws_secret_access_key refers to. As a general rule, we assume that any user running Terraform has administrator-level permissions for the following AWS services: EC2 S3 Route 53 DynamoDB Elastic Load Balancing IAM SSM Parameter Store The Terraform deployment itself will create new IAM roles to be used by Teleport instances which have appropriately limited permission scopes for AWS services. However, the initial cluster setup must be done by a user with a high level of AWS permissions. Get the Terraform code Firstly, you'll need to clone the Teleport repo to get the Terraform code available on your system: $ git clone https://github.com/gravitational/teleport Cloning into 'teleport' ... remote: Enumerating objects: 106 , done . remote: Counting objects: 100 % ( 106 /106 ) , done . remote: Compressing objects: 100 % ( 95 /95 ) , done . remote: Total 61144 ( delta 33 ) , reused 35 ( delta 11 ) , pack-reused 61038 Receiving objects: 100 % ( 61144 /61144 ) , 85 .17 MiB | 4 .66 MiB/s, done . Resolving deltas: 100 % ( 39141 /39141 ) , done . Once this is done, you can change into the directory where the Terraform code is checked out and run terraform init : $ cd teleport/examples/aws/terraform/ha-autoscale-cluster $ terraform init Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"template\" ( hashicorp/template ) 2 .1.2... - Downloading plugin for provider \"aws\" ( hashicorp/aws ) 2 .51.0... - Downloading plugin for provider \"random\" ( hashicorp/random ) 2 .2.1... Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. This will download the appropriate Terraform plugins needed to spin up Teleport using our reference code. Set up variables Terraform modules use variables to pass in input. You can do this in a few ways: on the command line to terraform apply by editing the vars.tf file by setting environment variables For this guide, we are going to make extensive use of environment variables. This is because it makes it easier for us to reference values from our configuration when running Teleport commands after the cluster has been created. Any set environment variable starting with TF_VAR_ is automatically processed and stripped down by Terraform, so TF_VAR_test_variable becomes test_variable . We maintain an up-to-date list of the variables and what they do in the README.md file under the examples/aws/terraform/ha-autoscale-cluster section of the Teleport repo but we'll run through an example list here. Things you will need to decide on: region Setting export TF_VAR_region=\"us-west-2\" The AWS region to run in. You should pick from the supported list as detailed in the README . These are regions which support DynamoDB encryption at rest . cluster_name Setting export TF_VAR_cluster_name=\"example-cluster\" This is the internal Teleport cluster name to use. This should be unique, and not contain spaces, dots (.) or other special characters. Some AWS services will not allow you to use dots in a name, so this should not be set to a domain name. This will appear in the web UI for your cluster and cannot be changed after creation without rebuilding your cluster from scratch, so choose carefully. ami_name Setting export TF_VAR_ami_name=\"gravitational-teleport-ami-ent-{{ teleport.version }}\" Gravitational automatically builds and publishes OSS, Enterprise and Enterprise FIPS 140-2 AMIs when we release a new version of Teleport. The AMI names follow the format: gravitational-teleport-ami-<type>-<version> where <type> is either oss or ent (Enterprise) and version is the version of Teleport e.g. {{ teleport.version }} . FIPS 140-2 compatible AMIs (which deploy Teleport in FIPS 140-2 mode by default) have the -fips suffix. The AWS account ID which publishes these AMIs is 126027368216 . You can list the available AMIs with the example awscli commands below. The output is in JSON format by default. List Gravitational AMIs OSS AMIs aws ec2 describe-images --owners 126027368216 --filters 'Name=name,Values=gravitational-teleport-ami-oss*' Enterprise AMIs aws ec2 describe-images --owners 126027368216 --filters 'Name=name,Values=gravitational-teleport-ami-ent*' List Enterprise FIPS 140-2 AMIs aws ec2 describe-images --owners 126027368216 --filters 'Name=name,Values=gravitational-teleport-ami-ent*-fips' key_name Setting export TF_VAR_key_name=\"exampleuser\" The AWS keypair name to use when deploying EC2 instances. This must exist in the same region as you specify in the region variable, and you will need a copy of this keypair available to connect to the deployed EC2 instances. Do not use a keypair that you do not have access to. license_path Setting export TF_VAR_license_path=\"/home/user/teleport-license.pem\" The full local path to your Teleport license file, which customers can download from the Gravitational dashboard . This license will be uploaded to AWS SSM and automatically downloaded to Teleport auth nodes in order to enable Teleport Enterprise/Pro functionality. (OSS users can provide any valid local file path here - it isn't used by the auth server in a Teleport OSS install) route53_zone Setting export TF_VAR_route53_zone=\"example.com\" Our Terraform setup requires you to have your domain provisioned in AWS Route 53 - it will automatically add DNS records for route53_domain as set up below. You can list these with this command: $ aws route53 list-hosted-zones --query \"HostedZones[*].Name\" --output text [ \"example.com.\" , \"testing.net.\" , \"subdomain.wow.org.\" ] You should use the appropriate domain without the trailing dot. route53_domain Setting export TF_VAR_route53_domain=\"teleport.example.com\" A subdomain to set up as a CNAME to the Teleport load balancer for web access. This will be the public-facing domain that people use to connect to your Teleport cluster, so choose wisely. This must be a subdomain of the domain you chose for route53_zone above. s3_bucket_name Setting export TF_VAR_s3_bucket_name=\"example-cluster\" The Terraform example also provisions an S3 bucket to hold certificates provisioned by LetsEncrypt and distribute these to EC2 instances. This can be any S3-compatible name, and will be generated in the same region as set above. This bucket is still provisioned when using ACM, as it is also used to store Teleport session logs. email Setting export TF_VAR_email=\"support@example.com\" LetsEncrypt requires an email address for every certificate registered which can be used to send notifications and useful information. We recommend a generic ops/support email address which the team deploying Teleport has access to. grafana_pass Setting export TF_VAR_grafana_pass=\"CHANGE_THIS_VALUE\" We deploy Grafana along with every Terraform deployment and automatically make stats on cluster usage available in a custom dashboard. This variable sets up the password for the Grafana admin user. The Grafana web UI is served on the same subdomain as specified above in route53_domain on port 8443. With the variables set in this example, it would be available on https://teleport.example.com:8443 If you do not change this from the default ( CHANGE_THIS_VALUE ), then it will be set to a random value for security and you will need to log into the monitoring instance to discover this manually. As such, we recommend setting this to a known value at the outset. use_acm Setting export TF_VAR_use_acm=\"false\" If set to the string \"false\" , Terraform will use LetsEncrypt to provision the public-facing web UI certificate for the Teleport cluster ( route53_domain - so https://teleport.example.com in this example). This uses an AWS network load balancer to load-balance connections to the Teleport cluster's web UI, and its SSL termination is handled by Teleport itself. If set to the string \"true\" , Terraform will use AWS ACM to provision the public-facing web UI certificate for the cluster. This uses an AWS application load balancer to load-balance connections to the Teleport cluster's web UI, and its SSL termination is handled by the load balancer. If you wish to use a pre-existing ACM certificate rather than having Terraform generate one for you, you can make Terraform use it by running this command before terraform apply : terraform import aws_acm_certificate.cert <certificate_arn> Reference deployment defaults Instances Our reference deployment will provision the following instances for your cluster by default: 2 x m4.large Teleport auth instances in an ASG, behind an internal network load balancer, configured using DynamoDB for shared storage. The desired size of the ASG is configured here 2 x m4.large Teleport proxy instances in an ASG, behind a public-facing load balancer - NLB for LetsEncrypt, ALB for ACM. The desired size of the ASG is configured here 1 x m4.large Teleport node instance in an ASG. The desired size of the ASG is configured here 1 x m4.large monitoring server in an ASG which hosts the Grafana instance and receives monitoring data from each service in the cluster. The desired size of the ASG is configured here 1 x t2.medium bastion server which is the only permitted source for inbound SSH traffic to the instances. This is done to avoid exposing each instance to the internet directly. The instance types used for each ASG can be configured here If you don't wish to set up a node or the monitoring services, you can set the desired_size and min_size for an ASG to 0 and Terraform will not provision it. Cluster state database storage The reference Terraform deployment sets Teleport up to store its cluster state database in DynamoDB. The name of the table for cluster state will be the same as the cluster name configured in the cluster_name variable above. In our example, the DynamoDB table would be called example-cluster . More information about how Teleport works with DynamoDB can be found in our DynamoDB Admin Guide . Audit event storage The reference Terraform deployment sets Teleport up to store cluster audit logs in DynamoDB. The name of the table for audit event storage will be the same as the cluster name configured in the cluster_name variable above with -events appended to the end. In our example, the DynamoDB table would be called example-cluster-events . More information about how Teleport works with DynamoDB can be found in our DynamoDB Admin Guide . Recorded session storage The reference Terraform deployment sets Teleport up to store recorded session logs in the same S3 bucket configured in the s3_bucket_name variable, under the records directory. In our example this would be s3://example-cluster/records Tip S3 provides Amazon S3 Object Lock , which is useful for customers deploying Teleport in regulated environments. Configuration of object lock is out of the scope of this guide. Cluster domain The reference Terraform deployment sets the Teleport cluster up to be available on a domain defined in Route53, referenced by the route53_domain variable. In our example this would be teleport.example.com Teleport's web interface will be available on port 443 - https://teleport.example.com - this is via a configured CNAME to the AWS load balancer. Teleport's proxy SSH interface will be available via a network load balancer with an AWS-controlled hostname on port 3023. This is the default port used when connecting with the tsh client and will not require any additional configuration. Teleport's tunnel interface will be available via the same network load balancer with an AWS-controlled hostname on port 3024. This allows trusted clusters and nodes connected via node tunnelling to access the cluster. After deploying, you can get the hostname of the proxy/tunnel network load balancer if needed with this command: $ aws elbv2 describe-load-balancers --names \" ${ TF_VAR_cluster_name } -proxy\" --query \"LoadBalancers[*].DNSName\" --output text example-cluster-proxy-7c97b76593d6bf21.elb.us-east-1.amazonaws.com Teleport's auth server interface will be available via an internal load balancer with an AWS-controlled hostname on port 3025. After deploying, you can get the hostname of the internal auth load balancer if needed with this command: $ aws elbv2 describe-load-balancers --names \" ${ TF_VAR_cluster_name } -auth\" --query \"LoadBalancers[*].DNSName\" --output text example-cluster-auth-c5b0fc2764ee015b.elb.us-east-1.amazonaws.com Deploying with Terraform Once you have set values for and exported all the variables detailed above, you should run terraform plan to validate the configuration. $ terraform plan Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.template_file.monitor_user_data: Refreshing state... data.aws_kms_alias.ssm: Refreshing state... data.aws_caller_identity.current: Refreshing state... data.aws_ami.base: Refreshing state... data.aws_availability_zones.available: Refreshing state... data.aws_route53_zone.proxy: Refreshing state... data.aws_region.current: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create < = read ( data resources ) Terraform will perform the following actions: <output trimmed> Plan: 121 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn 't specify an \"-out\" parameter to save this plan, so Terraform can' t guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. This looks good (no errors produced by Terraform) so we can run terraform apply : $ terraform apply <output trimmed> Plan: 121 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Entering yes here will start the Terraform deployment. It takes around 8-10 minutes to deploy in full. Destroy/shutdown a Terraform deployment If you need to tear down a running deployment for any reason, you can run terraform destroy . Accessing the cluster after Terraform setup Once the Terraform setup is finished, your Teleport cluster's web UI should be available on https:// route53_domain - this is https://teleport.example.com in our example. Adding an admin user to the Teleport cluster To add users to the Teleport cluster, you will need to connect to a Teleport auth server via SSH and run the tctl command. 1 - Use the AWS cli to get the IP of the bastion server: $ export BASTION_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=bastion\" --query \"Reservations[*].Instances[*].PublicIpAddress\" --output text ) $ echo ${ BASTION_IP } 1 .2.3.4 2 - Use the AWS cli to get the IP of an auth server: $ export AUTH_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=auth\" --query \"Reservations[0].Instances[*].PrivateIpAddress\" --output text ) $ echo ${ AUTH_IP } 172 .31.0.196 3 - Use both these values to SSH into the auth server. Make sure that the AWS keypair that you specified in the key_name variable is available in the current directory, or update the -i parameter to point to it: $ ssh -i ${ TF_VAR_key_name } .pem -o ProxyCommand = \"ssh -i ${ TF_VAR_key_name } .pem -W '[%h]:%p' ec2-user@ ${ BASTION_IP } \" ec2-user@ ${ AUTH_IP } The authenticity of host '1.2.3.4 (1.2.3.4)' can 't be established. ECDSA key fingerprint is SHA256:vFPnCFliRsRQ1Dk+muIv2B1Owm96hXiihlOUsj5H3bg. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added ' 1 .2.3.4 ' (ECDSA) to the list of known hosts. The authenticity of host ' 172 .31.0.196 ( <no hostip for proxy command> ) ' can' t be established. ECDSA key fingerprint is SHA256:vFPnCFliRsRQ1Dk+muIv2B1Owm96hXiihlOUsj5H3bg. Are you sure you want to continue connecting ( yes/no/ [ fingerprint ]) ? yes Warning: Permanently added '172.31.0.196' ( ECDSA ) to the list of known hosts. Last login: Tue Mar 3 18 :57:12 2020 from 1 .2.3.5 __ | __ | _ ) _ | ( / Amazon Linux 2 AMI ___ | \\_ __ | ___ | https://aws.amazon.com/amazon-linux-2/ 1 package ( s ) needed for security, out of 6 available Run \"sudo yum update\" to apply all updates. [ ec2-user@ip-172-31-0-196 ~ ] $ 4 - Use the tctl command to create an admin user for Teleport: [ ec2-user@ip-172-31-0-196 ~ ] $ sudo tctl users add teleport-admin --roles = admin Signup token has been created and is valid for 1 hours. Share this URL with the user: https://teleport.example.com:443/web/newuser/6489ae886babf4232826076279bcb2fb NOTE: Make sure teleport.example.com:443 points at a Teleport proxy which users can access. When the user 'teleport-admin' activates their account, they will be assigned roles [ admin ] 5 - Click the link to launch the Teleport web UI and finish setting up your user. You will need to scan the QR code with an TOTP-compatible app like Google Authenticator or Authy. You will also set a password for the teleport-admin user on this page. Once this user is successfully configured, you should be logged into the Teleport web UI. Logging into the cluster with tsh You can use the Teleport command line tool ( tsh ) to log into your Teleport cluster after provisioning a user. You can download the Teleport package containing the tsh client from here - the client is the same for both OSS and Enterprise versions of Teleport. $ tsh login --proxy = ${ TF_VAR_route53_domain } :443 --user = teleport-admin Enter password for Teleport user teleport-admin: Enter your OTP token: 567989 > Profile URL: https://teleport.example.com:443 Logged in as: teleport-admin Cluster: example-cluster Roles: admin* Logins: root Valid until : 2020 -03-06 22 :07:11 -0400 AST [ valid for 12h0m0s ] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty * RBAC is only available in Teleport Enterprise https://gravitational.com/teleport/docs/enterprise $ tsh ls Node Name Address Labels ---------------------------- ----------------- ------ ip-172-31-11-69-ec2-internal 172 .31.11.69:3022 $ tsh ssh root@ip-172-31-11-69-ec2-internal [ root@ip-172-31-11-69 ~ ] # Restarting/checking Teleport services LetsEncrypt Note You are using LetsEncrypt if your use_acm variable is set to \"false\" . Auth service $ systemctl status teleport-auth.service \u25cf teleport-auth.service - Teleport Auth Service Loaded: loaded ( /etc/systemd/system/teleport-auth.service ; enabled ; vendor preset: disabled ) Active: active ( running ) since Thu 2020 -03-05 16 :45:18 UTC ; 4h 14min ago Main PID: 3766 ( teleport ) CGroup: /system.slice/teleport-auth.service \u2514\u25003766 /usr/bin/teleport start --config = /etc/teleport.yaml --diag-addr = 127 .0.0.1:3434 --pid-file = /run/teleport/teleport.pid Mar 05 17 :54:58 ip-172-31-0-196.ec2.internal /usr/bin/teleport [ 3766 ] : INFO [ CA ] Generating TLS certificate { 0x3767920 0xc0012802f0 CN = teleport-admin,O = admin,POSTALCODE ={ \\\" kubernetes_groups \\\" :null \\,\\\" logins \\\" :null } ,STREET = ,L = root 2020 -03-06 05 :54:58.846233666 +0000 UTC []} . common_name:teleport-admin dns_name... Mar 05 18 :04:39 ip-172-31-0-196.ec2.internal /usr/bin/teleport [ 3766 ] : INFO [ CA ] Generating TLS certificate { 0x3767920 0xc00155d200 CN = teleport-admin,O = admin,POSTALCODE ={ \\\" kubernetes_groups \\\" :null \\,\\\" logins \\\" :null } ,STREET = ,L = root 2020 -03-06 06 :04:39.844777551 +0000 UTC []} . common_name:teleport-admin dns_name... You can get detailed logs for the Teleport auth servers using the journalctl command: $ journalctl -u teleport-auth.service Remember that there is more than one auth server in an HA deployment. You should use this command to get the IP addresses of each auth server that you'll need to connect to: $ aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=auth\" --query \"Reservations[*].Instances[*].PrivateIpAddress\" --output text 172 .31.0.196 172 .31.1.78 You can run tctl commands on any of the auth instances connected to your cluster, however. Proxy service $ systemctl status teleport-proxy.service \u25cf teleport-proxy.service - Teleport Proxy Service Loaded: loaded ( /etc/systemd/system/teleport-proxy.service ; enabled ; vendor preset: disabled ) Active: active ( running ) since Thu 2020 -03-05 17 :14:37 UTC ; 3h 47min ago Process: 4502 ExecStartPre = /usr/bin/teleport-ssm-get-token ( code = exited, status = 0 /SUCCESS ) Main PID: 4514 ( teleport ) CGroup: /system.slice/teleport-proxy.service \u2514\u25004514 /usr/bin/teleport start --config = /etc/teleport.yaml --diag-addr = 127 .0.0.1:3434 --pid-file = /run/teleport/teleport.pid Mar 05 20 :58:25 ip-172-31-2-109.ec2.internal /usr/bin/teleport [ 4514 ] : ERRO read tcp 172 .31.2.109:3024->172.31.2.143:1577: read: connection reset by peer Mar 05 20 :58:50 ip-172-31-2-109.ec2.internal /usr/bin/teleport [ 4514 ] : ERRO read tcp 172 .31.2.109:3023->172.31.2.143:38011: read: connection reset by peer You can get detailed logs for the Teleport proxy service using the journalctl command: $ journalctl -u teleport-proxy.service Remember that there is more than one proxy instance in an HA deployment. You should use this command to get the IP addresses of each auth instance that you'll need to connect to: $ aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=proxy\" --query \"Reservations[*].Instances[*].PrivateIpAddress\" --output text 172 .31.2.109 172 .31.3.215 Node service $ systemctl status teleport-node.service \u25cf teleport-node.service - Teleport SSH Node Service Loaded: loaded ( /etc/systemd/system/teleport-node.service ; enabled ; vendor preset: disabled ) Active: active ( running ) since Thu 2020 -03-05 17 :18:25 UTC ; 3h 44min ago Process: 4444 ExecStartPre = /usr/bin/teleport-ssm-get-token ( code = exited, status = 0 /SUCCESS ) Main PID: 4456 ( teleport ) CGroup: /system.slice/teleport-node.service \u2514\u25004456 /usr/bin/teleport start --config = /etc/teleport.yaml --diag-addr = 127 .0.0.1:3434 --pid-file = /run/teleport/teleport.pid Mar 05 17 :18:25 ip-172-31-11-69.ec2.internal /usr/bin/teleport [ 4456 ] : INFO [ AUDIT:1 ] Creating directory /var/lib/teleport/log/upload/sessions. ser...o:1630 Mar 05 17 :18:25 ip-172-31-11-69.ec2.internal /usr/bin/teleport [ 4456 ] : INFO [ AUDIT:1 ] Setting directory /var/lib/teleport/log/upload/sessions owner...o:1639 You can get detailed logs for the Teleport node service using the journalctl command: $ journalctl -u teleport-node.service ACM Note You are using ACM if your use_acm variable is set to \"true\" . When using ACM, the service name for the proxy is different ( teleport-proxy-acm.service vs teleport-proxy.service ). Auth service $ systemctl status teleport-auth.service \u25cf teleport-auth.service - Teleport Auth Service Loaded: loaded ( /etc/systemd/system/teleport-auth.service ; enabled ; vendor preset: disabled ) Active: active ( running ) since Thu 2020 -03-05 16 :45:18 UTC ; 4h 14min ago Main PID: 3766 ( teleport ) CGroup: /system.slice/teleport-auth.service \u2514\u25003766 /usr/bin/teleport start --config = /etc/teleport.yaml --diag-addr = 127 .0.0.1:3434 --pid-file = /run/teleport/teleport.pid Mar 05 17 :54:58 ip-172-31-0-196.ec2.internal /usr/bin/teleport [ 3766 ] : INFO [ CA ] Generating TLS certificate { 0x3767920 0xc0012802f0 CN = teleport-admin,O = admin,POSTALCODE ={ \\\" kubernetes_groups \\\" :null \\,\\\" logins \\\" :null } ,STREET = ,L = root 2020 -03-06 05 :54:58.846233666 +0000 UTC []} . common_name:teleport-admin dns_name... Mar 05 18 :04:39 ip-172-31-0-196.ec2.internal /usr/bin/teleport [ 3766 ] : INFO [ CA ] Generating TLS certificate { 0x3767920 0xc00155d200 CN = teleport-admin,O = admin,POSTALCODE ={ \\\" kubernetes_groups \\\" :null \\,\\\" logins \\\" :null } ,STREET = ,L = root 2020 -03-06 06 :04:39.844777551 +0000 UTC []} . common_name:teleport-admin dns_name... You can get detailed logs for the Teleport auth server using the journalctl command: $ journalctl -u teleport-auth.service Remember that there is more than one auth instance in an HA deployment. You should use this command to get the IP addresses of each auth instance that you'd need to connect to for checking logs: $ aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=auth\" --query \"Reservations[*].Instances[*].PrivateIpAddress\" --output text 172 .31.0.196 172 .31.1.78 You can run tctl commands on any of the auth instances connected to your cluster, however. Proxy service (ACM) $ systemctl status teleport-proxy-acm.service \u25cf teleport-proxy-acm.service - Teleport Proxy Service ( ACM ) Loaded: loaded ( /etc/systemd/system/teleport-proxy-acm.service ; enabled ; vendor preset: disabled ) Active: active ( running ) since Thu 2020 -03-05 17 :14:37 UTC ; 3h 47min ago Process: 4502 ExecStartPre = /usr/bin/teleport-ssm-get-token ( code = exited, status = 0 /SUCCESS ) Main PID: 4514 ( teleport ) CGroup: /system.slice/teleport-proxy-acm.service \u2514\u25004514 /usr/bin/teleport start --config = /etc/teleport.yaml --diag-addr = 127 .0.0.1:3434 --pid-file = /run/teleport/teleport.pid Mar 05 20 :58:25 ip-172-31-2-109.ec2.internal /usr/bin/teleport [ 4514 ] : ERRO read tcp 172 .31.2.109:3024->172.31.2.143:1577: read: connection reset by peer Mar 05 20 :58:50 ip-172-31-2-109.ec2.internal /usr/bin/teleport [ 4514 ] : ERRO read tcp 172 .31.2.109:3023->172.31.2.143:38011: read: connection reset by peer You can get detailed logs for the Teleport proxy service using the journalctl command: $ journalctl -u teleport-proxy-acm.service Remember that there is more than one proxy instance in an HA deployment. You can use this command to get the IP addresses of each proxy instance that you'd need to connect to for checking logs: $ aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=proxy\" --query \"Reservations[*].Instances[*].PrivateIpAddress\" --output text 172 .31.2.109 172 .31.3.215 Node service $ systemctl status teleport-node.service \u25cf teleport-node.service - Teleport SSH Node Service Loaded: loaded ( /etc/systemd/system/teleport-node.service ; enabled ; vendor preset: disabled ) Active: active ( running ) since Thu 2020 -03-05 17 :18:25 UTC ; 3h 44min ago Process: 4444 ExecStartPre = /usr/bin/teleport-ssm-get-token ( code = exited, status = 0 /SUCCESS ) Main PID: 4456 ( teleport ) CGroup: /system.slice/teleport-node.service \u2514\u25004456 /usr/bin/teleport start --config = /etc/teleport.yaml --diag-addr = 127 .0.0.1:3434 --pid-file = /run/teleport/teleport.pid Mar 05 17 :18:25 ip-172-31-11-69.ec2.internal /usr/bin/teleport [ 4456 ] : INFO [ AUDIT:1 ] Creating directory /var/lib/teleport/log/upload/sessions. ser...o:1630 Mar 05 17 :18:25 ip-172-31-11-69.ec2.internal /usr/bin/teleport [ 4456 ] : INFO [ AUDIT:1 ] Setting directory /var/lib/teleport/log/upload/sessions owner...o:1639 You can get detailed logs for the Teleport node service using the journalctl command: $ journalctl -u teleport-node.service Adding EC2 instances to your Teleport cluster Customers run many workloads within EC2 and depending on how you work, there are many ways to integrate Teleport onto your servers. We recommend looking at our Admin manual . To add new nodes/EC2 servers that you can \"SSH into\" you'll need to: Install the Teleport binary on the Server Run Teleport - we recommend using systemd Set the correct settings in /etc/teleport.yaml Add nodes to the Teleport cluster Getting the CA pin hash You can use this command to get the CA pin hash for your Teleport cluster: $ aws ssm get-parameter --name \"/teleport/ ${ TF_VAR_cluster_name } /ca-pin-hash\" --query \"Parameter.Value\" --output text sha256:d021ef54aaf8633c4e15c5cc59479fb2f19b1bbc5432bb95213ee047000689dd You should use this so that nodes can validate the auth server's identity when joining your cluster. Getting the node join token You can use this command to get a join token for your Teleport cluster: $ aws ssm get-parameter --name \"/teleport/ ${ TF_VAR_cluster_name } /tokens/node\" --query \"Parameter.Value\" --output text AQICAHgLq8feq4riNouuw8Wxs5EEPlS2qKIVE5Z/qEo1i6mqfwGX3dW56SdoS6PinTWbZL1RAAAAgzCBgAYJKoZIhvcNAQcGoHMwcQIBADBsBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDNdu5TxaT8gyJx63eAIBEIA/JEpX2Vte90UmufIzZzvBQcQaKgWr95aN9xZYMEjWbAiNitxkvZgb98FgFn8d9GNwKQgDGfUYDbzsX8EqTtx9 You should use this so that nodes can validate the auth server's identity when joining your cluster. You can also generate a node join token using tctl tokens add --type=node as detailed here in our admin guide . Joining nodes via the Teleport auth server To join Teleport nodes in the same VPC via the auth server, you can find the hostname for the auth load balancer with this command: $ aws elbv2 describe-load-balancers --names \" ${ TF_VAR_cluster_name } -auth\" --query \"LoadBalancers[*].DNSName\" --output text example-cluster-auth-c5b0fc2764ee015b.elb.us-east-1.amazonaws.com With this method, the nodes should be configured like so: auth_servers : - example-cluster-auth-c5b0fc2764ee015b.elb.us-east-1.amazonaws.com:3025 Joining nodes via Teleport IoT/node tunnelling To join Teleport nodes from outside the same VPC, you will either need to investigate VPC peering/gateways (out of scope for this document) or join your nodes using Teleport's node tunnelling functionality. With this method, you can join the nodes using the public facing proxy address - teleport.example.com:443 for our example. auth_servers : - teleport.example.com:443 Trusted clusters To add a trusted cluster, you'll need the hostname of the proxy load balancer. You can get it using this command: $ aws elbv2 describe-load-balancers --names \" ${ TF_VAR_cluster_name } -proxy\" --query \"LoadBalancers[*].DNSName\" --output text example-cluster-proxy-7c97b76593d6bf21.elb.us-east-1.amazonaws.com In this example, the tunnel_addr and web_proxy_addr in the trusted cluster configuration should be set up like this: spec : tunnel_addr : example-cluster-proxy-7c97b76593d6bf21.elb.us-east-1.amazonaws.com:3024 web_proxy_addr : teleport.example.com:443 You can generate a token for adding the trusted cluster using tctl tokens add --type=trusted_cluster after connecting to an auth server. Follow the instructions in our trusted cluster guide . Script to quickly connect to instances Here's a bash script that you can use to quickly connect to instances: #!/bin/bash if [[ \" $1 \" ! = \"\" ]] ; then INSTANCE_TYPE = $1 else INSTANCE_TYPE = \"auth\" fi if [[ \" $2 \" ! = \"\" ]] ; then INSTANCE_ID = $2 else INSTANCE_ID = \"0\" fi export BASTION_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=bastion\" --query \"Reservations[*].Instances[*].PublicIpAddress\" --output text ) echo \"Bastion IP: ${ BASTION_IP } \" if [[ \" ${ INSTANCE_TYPE } \" == \"auth\" ]] ; then export SERVER_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=auth\" --query \"Reservations[ ${ INSTANCE_ID } ].Instances[*].PrivateIpAddress\" --output text ) echo \"Auth ${ INSTANCE_ID } IP: ${ SERVER_IP } \" elif [[ \" ${ INSTANCE_TYPE } \" == \"proxy\" ]] ; then export SERVER_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=proxy\" --query \"Reservations[ ${ INSTANCE_ID } ].Instances[*].PrivateIpAddress\" --output text ) echo \"Proxy ${ INSTANCE_ID } IP: ${ SERVER_IP } \" elif [[ \" ${ INSTANCE_TYPE } \" == \"node\" ]] ; then export SERVER_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=node\" --query \"Reservations[*].Instances[*].PrivateIpAddress\" --output text ) echo \"Node IP: ${ SERVER_IP } \" fi echo \"Keypair name: ${ TF_VAR_key_name } \" ssh -i ${ TF_VAR_key_name } .pem -o ProxyCommand = \"ssh -i ${ TF_VAR_key_name } .pem -W '[%h]:%p' ec2-user@ ${ BASTION_IP } \" ec2-user@ ${ SERVER_IP } Save this as connect.sh , run chmod +x connect.sh to make it executable, then use it like so: # connect to the first auth server $ ./connect.sh auth 0 # connect to the second auth server $ ./connect.sh auth 1 # connect to the first proxy server $ ./connect.sh proxy 0 # connect to the second proxy server $ ./connect.sh proxy 1 # connect to the node $ ./connect.sh node","title":"AWS Terraform"},{"location":"aws-terraform-guide/#running-teleport-enterprise-in-ha-mode-on-aws","text":"This guide is designed to accompany our reference Terraform code and describe how to use and administrate the resulting Teleport deployment. Running Teleport Enterprise in HA mode on AWS Prerequisites Get the Terraform code Set up variables region cluster_name ami_name key_name license_path route53_zone route53_domain s3_bucket_name email grafana_pass use_acm Reference deployment defaults Instances Cluster state database storage Audit event storage Recorded session storage Cluster domain Deploying with Terraform Accessing the cluster after Terraform setup Adding an admin user to the Teleport cluster Logging into the cluster with tsh Restarting/checking Teleport services LetsEncrypt ACM Adding EC2 instances to your Teleport cluster Getting the CA pin hash Getting the node join token Joining nodes via the Teleport auth server Joining nodes via Teleport IoT/node tunnelling Trusted clusters Script to quickly connect to instances","title":"Running Teleport Enterprise in HA mode on AWS"},{"location":"aws-terraform-guide/#prerequisites","text":"Our code requires Terraform 0.12+. You can download Terraform here . We will assume that you have terraform installed and available on your path. $ which terraform /usr/local/bin/terraform $ terraform version Terraform v0.12.20 You will also require the aws command line tool. This is available in Ubuntu/Debian/Fedora/CentOS and MacOS Homebrew as the awscli package. Fedora/CentOS: yum -y install awscli Ubuntu/Debian: apt-get -y install awscli MacOS (with Homebrew ): brew install awscli When possible, installing via a package is always preferable. If you can't find a package available for your distribution, you can also download the tool from https://aws.amazon.com/cli/ We will assume that you have configured your AWS cli access with credentials available at ~/.aws/credentials : $ cat ~/.aws/credentials [ default ] aws_access_key_id = AKIA.... aws_secret_access_key = 8ZRxy.... You should also have a default region set under ~/.aws/config : $ cat ~/.aws/config [ default ] region = us-east-1 As a result, you should be able to run a command like aws ec2 describe-instances to list running EC2 instances. If you get an \"access denied\", \"403 Forbidden\" or similar message, you will need to grant additional permissions to the AWS IAM user that your aws_access_key_id and aws_secret_access_key refers to. As a general rule, we assume that any user running Terraform has administrator-level permissions for the following AWS services: EC2 S3 Route 53 DynamoDB Elastic Load Balancing IAM SSM Parameter Store The Terraform deployment itself will create new IAM roles to be used by Teleport instances which have appropriately limited permission scopes for AWS services. However, the initial cluster setup must be done by a user with a high level of AWS permissions.","title":"Prerequisites"},{"location":"aws-terraform-guide/#get-the-terraform-code","text":"Firstly, you'll need to clone the Teleport repo to get the Terraform code available on your system: $ git clone https://github.com/gravitational/teleport Cloning into 'teleport' ... remote: Enumerating objects: 106 , done . remote: Counting objects: 100 % ( 106 /106 ) , done . remote: Compressing objects: 100 % ( 95 /95 ) , done . remote: Total 61144 ( delta 33 ) , reused 35 ( delta 11 ) , pack-reused 61038 Receiving objects: 100 % ( 61144 /61144 ) , 85 .17 MiB | 4 .66 MiB/s, done . Resolving deltas: 100 % ( 39141 /39141 ) , done . Once this is done, you can change into the directory where the Terraform code is checked out and run terraform init : $ cd teleport/examples/aws/terraform/ha-autoscale-cluster $ terraform init Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"template\" ( hashicorp/template ) 2 .1.2... - Downloading plugin for provider \"aws\" ( hashicorp/aws ) 2 .51.0... - Downloading plugin for provider \"random\" ( hashicorp/random ) 2 .2.1... Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. This will download the appropriate Terraform plugins needed to spin up Teleport using our reference code.","title":"Get the Terraform code"},{"location":"aws-terraform-guide/#set-up-variables","text":"Terraform modules use variables to pass in input. You can do this in a few ways: on the command line to terraform apply by editing the vars.tf file by setting environment variables For this guide, we are going to make extensive use of environment variables. This is because it makes it easier for us to reference values from our configuration when running Teleport commands after the cluster has been created. Any set environment variable starting with TF_VAR_ is automatically processed and stripped down by Terraform, so TF_VAR_test_variable becomes test_variable . We maintain an up-to-date list of the variables and what they do in the README.md file under the examples/aws/terraform/ha-autoscale-cluster section of the Teleport repo but we'll run through an example list here. Things you will need to decide on:","title":"Set up variables"},{"location":"aws-terraform-guide/#region","text":"Setting export TF_VAR_region=\"us-west-2\" The AWS region to run in. You should pick from the supported list as detailed in the README . These are regions which support DynamoDB encryption at rest .","title":"region"},{"location":"aws-terraform-guide/#cluster_name","text":"Setting export TF_VAR_cluster_name=\"example-cluster\" This is the internal Teleport cluster name to use. This should be unique, and not contain spaces, dots (.) or other special characters. Some AWS services will not allow you to use dots in a name, so this should not be set to a domain name. This will appear in the web UI for your cluster and cannot be changed after creation without rebuilding your cluster from scratch, so choose carefully.","title":"cluster_name"},{"location":"aws-terraform-guide/#ami_name","text":"Setting export TF_VAR_ami_name=\"gravitational-teleport-ami-ent-{{ teleport.version }}\" Gravitational automatically builds and publishes OSS, Enterprise and Enterprise FIPS 140-2 AMIs when we release a new version of Teleport. The AMI names follow the format: gravitational-teleport-ami-<type>-<version> where <type> is either oss or ent (Enterprise) and version is the version of Teleport e.g. {{ teleport.version }} . FIPS 140-2 compatible AMIs (which deploy Teleport in FIPS 140-2 mode by default) have the -fips suffix. The AWS account ID which publishes these AMIs is 126027368216 . You can list the available AMIs with the example awscli commands below. The output is in JSON format by default. List Gravitational AMIs OSS AMIs aws ec2 describe-images --owners 126027368216 --filters 'Name=name,Values=gravitational-teleport-ami-oss*' Enterprise AMIs aws ec2 describe-images --owners 126027368216 --filters 'Name=name,Values=gravitational-teleport-ami-ent*' List Enterprise FIPS 140-2 AMIs aws ec2 describe-images --owners 126027368216 --filters 'Name=name,Values=gravitational-teleport-ami-ent*-fips'","title":"ami_name"},{"location":"aws-terraform-guide/#key_name","text":"Setting export TF_VAR_key_name=\"exampleuser\" The AWS keypair name to use when deploying EC2 instances. This must exist in the same region as you specify in the region variable, and you will need a copy of this keypair available to connect to the deployed EC2 instances. Do not use a keypair that you do not have access to.","title":"key_name"},{"location":"aws-terraform-guide/#license_path","text":"Setting export TF_VAR_license_path=\"/home/user/teleport-license.pem\" The full local path to your Teleport license file, which customers can download from the Gravitational dashboard . This license will be uploaded to AWS SSM and automatically downloaded to Teleport auth nodes in order to enable Teleport Enterprise/Pro functionality. (OSS users can provide any valid local file path here - it isn't used by the auth server in a Teleport OSS install)","title":"license_path"},{"location":"aws-terraform-guide/#route53_zone","text":"Setting export TF_VAR_route53_zone=\"example.com\" Our Terraform setup requires you to have your domain provisioned in AWS Route 53 - it will automatically add DNS records for route53_domain as set up below. You can list these with this command: $ aws route53 list-hosted-zones --query \"HostedZones[*].Name\" --output text [ \"example.com.\" , \"testing.net.\" , \"subdomain.wow.org.\" ] You should use the appropriate domain without the trailing dot.","title":"route53_zone"},{"location":"aws-terraform-guide/#route53_domain","text":"Setting export TF_VAR_route53_domain=\"teleport.example.com\" A subdomain to set up as a CNAME to the Teleport load balancer for web access. This will be the public-facing domain that people use to connect to your Teleport cluster, so choose wisely. This must be a subdomain of the domain you chose for route53_zone above.","title":"route53_domain"},{"location":"aws-terraform-guide/#s3_bucket_name","text":"Setting export TF_VAR_s3_bucket_name=\"example-cluster\" The Terraform example also provisions an S3 bucket to hold certificates provisioned by LetsEncrypt and distribute these to EC2 instances. This can be any S3-compatible name, and will be generated in the same region as set above. This bucket is still provisioned when using ACM, as it is also used to store Teleport session logs.","title":"s3_bucket_name"},{"location":"aws-terraform-guide/#email","text":"Setting export TF_VAR_email=\"support@example.com\" LetsEncrypt requires an email address for every certificate registered which can be used to send notifications and useful information. We recommend a generic ops/support email address which the team deploying Teleport has access to.","title":"email"},{"location":"aws-terraform-guide/#grafana_pass","text":"Setting export TF_VAR_grafana_pass=\"CHANGE_THIS_VALUE\" We deploy Grafana along with every Terraform deployment and automatically make stats on cluster usage available in a custom dashboard. This variable sets up the password for the Grafana admin user. The Grafana web UI is served on the same subdomain as specified above in route53_domain on port 8443. With the variables set in this example, it would be available on https://teleport.example.com:8443 If you do not change this from the default ( CHANGE_THIS_VALUE ), then it will be set to a random value for security and you will need to log into the monitoring instance to discover this manually. As such, we recommend setting this to a known value at the outset.","title":"grafana_pass"},{"location":"aws-terraform-guide/#use_acm","text":"Setting export TF_VAR_use_acm=\"false\" If set to the string \"false\" , Terraform will use LetsEncrypt to provision the public-facing web UI certificate for the Teleport cluster ( route53_domain - so https://teleport.example.com in this example). This uses an AWS network load balancer to load-balance connections to the Teleport cluster's web UI, and its SSL termination is handled by Teleport itself. If set to the string \"true\" , Terraform will use AWS ACM to provision the public-facing web UI certificate for the cluster. This uses an AWS application load balancer to load-balance connections to the Teleport cluster's web UI, and its SSL termination is handled by the load balancer. If you wish to use a pre-existing ACM certificate rather than having Terraform generate one for you, you can make Terraform use it by running this command before terraform apply : terraform import aws_acm_certificate.cert <certificate_arn>","title":"use_acm"},{"location":"aws-terraform-guide/#reference-deployment-defaults","text":"","title":"Reference deployment defaults"},{"location":"aws-terraform-guide/#instances","text":"Our reference deployment will provision the following instances for your cluster by default: 2 x m4.large Teleport auth instances in an ASG, behind an internal network load balancer, configured using DynamoDB for shared storage. The desired size of the ASG is configured here 2 x m4.large Teleport proxy instances in an ASG, behind a public-facing load balancer - NLB for LetsEncrypt, ALB for ACM. The desired size of the ASG is configured here 1 x m4.large Teleport node instance in an ASG. The desired size of the ASG is configured here 1 x m4.large monitoring server in an ASG which hosts the Grafana instance and receives monitoring data from each service in the cluster. The desired size of the ASG is configured here 1 x t2.medium bastion server which is the only permitted source for inbound SSH traffic to the instances. This is done to avoid exposing each instance to the internet directly. The instance types used for each ASG can be configured here If you don't wish to set up a node or the monitoring services, you can set the desired_size and min_size for an ASG to 0 and Terraform will not provision it.","title":"Instances"},{"location":"aws-terraform-guide/#cluster-state-database-storage","text":"The reference Terraform deployment sets Teleport up to store its cluster state database in DynamoDB. The name of the table for cluster state will be the same as the cluster name configured in the cluster_name variable above. In our example, the DynamoDB table would be called example-cluster . More information about how Teleport works with DynamoDB can be found in our DynamoDB Admin Guide .","title":"Cluster state database storage"},{"location":"aws-terraform-guide/#audit-event-storage","text":"The reference Terraform deployment sets Teleport up to store cluster audit logs in DynamoDB. The name of the table for audit event storage will be the same as the cluster name configured in the cluster_name variable above with -events appended to the end. In our example, the DynamoDB table would be called example-cluster-events . More information about how Teleport works with DynamoDB can be found in our DynamoDB Admin Guide .","title":"Audit event storage"},{"location":"aws-terraform-guide/#recorded-session-storage","text":"The reference Terraform deployment sets Teleport up to store recorded session logs in the same S3 bucket configured in the s3_bucket_name variable, under the records directory. In our example this would be s3://example-cluster/records Tip S3 provides Amazon S3 Object Lock , which is useful for customers deploying Teleport in regulated environments. Configuration of object lock is out of the scope of this guide.","title":"Recorded session storage"},{"location":"aws-terraform-guide/#cluster-domain","text":"The reference Terraform deployment sets the Teleport cluster up to be available on a domain defined in Route53, referenced by the route53_domain variable. In our example this would be teleport.example.com Teleport's web interface will be available on port 443 - https://teleport.example.com - this is via a configured CNAME to the AWS load balancer. Teleport's proxy SSH interface will be available via a network load balancer with an AWS-controlled hostname on port 3023. This is the default port used when connecting with the tsh client and will not require any additional configuration. Teleport's tunnel interface will be available via the same network load balancer with an AWS-controlled hostname on port 3024. This allows trusted clusters and nodes connected via node tunnelling to access the cluster. After deploying, you can get the hostname of the proxy/tunnel network load balancer if needed with this command: $ aws elbv2 describe-load-balancers --names \" ${ TF_VAR_cluster_name } -proxy\" --query \"LoadBalancers[*].DNSName\" --output text example-cluster-proxy-7c97b76593d6bf21.elb.us-east-1.amazonaws.com Teleport's auth server interface will be available via an internal load balancer with an AWS-controlled hostname on port 3025. After deploying, you can get the hostname of the internal auth load balancer if needed with this command: $ aws elbv2 describe-load-balancers --names \" ${ TF_VAR_cluster_name } -auth\" --query \"LoadBalancers[*].DNSName\" --output text example-cluster-auth-c5b0fc2764ee015b.elb.us-east-1.amazonaws.com","title":"Cluster domain"},{"location":"aws-terraform-guide/#deploying-with-terraform","text":"Once you have set values for and exported all the variables detailed above, you should run terraform plan to validate the configuration. $ terraform plan Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.template_file.monitor_user_data: Refreshing state... data.aws_kms_alias.ssm: Refreshing state... data.aws_caller_identity.current: Refreshing state... data.aws_ami.base: Refreshing state... data.aws_availability_zones.available: Refreshing state... data.aws_route53_zone.proxy: Refreshing state... data.aws_region.current: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create < = read ( data resources ) Terraform will perform the following actions: <output trimmed> Plan: 121 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn 't specify an \"-out\" parameter to save this plan, so Terraform can' t guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. This looks good (no errors produced by Terraform) so we can run terraform apply : $ terraform apply <output trimmed> Plan: 121 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Entering yes here will start the Terraform deployment. It takes around 8-10 minutes to deploy in full.","title":"Deploying with Terraform"},{"location":"aws-terraform-guide/#accessing-the-cluster-after-terraform-setup","text":"Once the Terraform setup is finished, your Teleport cluster's web UI should be available on https:// route53_domain - this is https://teleport.example.com in our example.","title":"Accessing the cluster after Terraform setup"},{"location":"aws-terraform-guide/#adding-an-admin-user-to-the-teleport-cluster","text":"To add users to the Teleport cluster, you will need to connect to a Teleport auth server via SSH and run the tctl command. 1 - Use the AWS cli to get the IP of the bastion server: $ export BASTION_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=bastion\" --query \"Reservations[*].Instances[*].PublicIpAddress\" --output text ) $ echo ${ BASTION_IP } 1 .2.3.4 2 - Use the AWS cli to get the IP of an auth server: $ export AUTH_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=auth\" --query \"Reservations[0].Instances[*].PrivateIpAddress\" --output text ) $ echo ${ AUTH_IP } 172 .31.0.196 3 - Use both these values to SSH into the auth server. Make sure that the AWS keypair that you specified in the key_name variable is available in the current directory, or update the -i parameter to point to it: $ ssh -i ${ TF_VAR_key_name } .pem -o ProxyCommand = \"ssh -i ${ TF_VAR_key_name } .pem -W '[%h]:%p' ec2-user@ ${ BASTION_IP } \" ec2-user@ ${ AUTH_IP } The authenticity of host '1.2.3.4 (1.2.3.4)' can 't be established. ECDSA key fingerprint is SHA256:vFPnCFliRsRQ1Dk+muIv2B1Owm96hXiihlOUsj5H3bg. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added ' 1 .2.3.4 ' (ECDSA) to the list of known hosts. The authenticity of host ' 172 .31.0.196 ( <no hostip for proxy command> ) ' can' t be established. ECDSA key fingerprint is SHA256:vFPnCFliRsRQ1Dk+muIv2B1Owm96hXiihlOUsj5H3bg. Are you sure you want to continue connecting ( yes/no/ [ fingerprint ]) ? yes Warning: Permanently added '172.31.0.196' ( ECDSA ) to the list of known hosts. Last login: Tue Mar 3 18 :57:12 2020 from 1 .2.3.5 __ | __ | _ ) _ | ( / Amazon Linux 2 AMI ___ | \\_ __ | ___ | https://aws.amazon.com/amazon-linux-2/ 1 package ( s ) needed for security, out of 6 available Run \"sudo yum update\" to apply all updates. [ ec2-user@ip-172-31-0-196 ~ ] $ 4 - Use the tctl command to create an admin user for Teleport: [ ec2-user@ip-172-31-0-196 ~ ] $ sudo tctl users add teleport-admin --roles = admin Signup token has been created and is valid for 1 hours. Share this URL with the user: https://teleport.example.com:443/web/newuser/6489ae886babf4232826076279bcb2fb NOTE: Make sure teleport.example.com:443 points at a Teleport proxy which users can access. When the user 'teleport-admin' activates their account, they will be assigned roles [ admin ] 5 - Click the link to launch the Teleport web UI and finish setting up your user. You will need to scan the QR code with an TOTP-compatible app like Google Authenticator or Authy. You will also set a password for the teleport-admin user on this page. Once this user is successfully configured, you should be logged into the Teleport web UI.","title":"Adding an admin user to the Teleport cluster"},{"location":"aws-terraform-guide/#logging-into-the-cluster-with-tsh","text":"You can use the Teleport command line tool ( tsh ) to log into your Teleport cluster after provisioning a user. You can download the Teleport package containing the tsh client from here - the client is the same for both OSS and Enterprise versions of Teleport. $ tsh login --proxy = ${ TF_VAR_route53_domain } :443 --user = teleport-admin Enter password for Teleport user teleport-admin: Enter your OTP token: 567989 > Profile URL: https://teleport.example.com:443 Logged in as: teleport-admin Cluster: example-cluster Roles: admin* Logins: root Valid until : 2020 -03-06 22 :07:11 -0400 AST [ valid for 12h0m0s ] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty * RBAC is only available in Teleport Enterprise https://gravitational.com/teleport/docs/enterprise $ tsh ls Node Name Address Labels ---------------------------- ----------------- ------ ip-172-31-11-69-ec2-internal 172 .31.11.69:3022 $ tsh ssh root@ip-172-31-11-69-ec2-internal [ root@ip-172-31-11-69 ~ ] #","title":"Logging into the cluster with tsh"},{"location":"aws-terraform-guide/#restartingchecking-teleport-services","text":"","title":"Restarting/checking Teleport services"},{"location":"aws-terraform-guide/#letsencrypt","text":"Note You are using LetsEncrypt if your use_acm variable is set to \"false\" .","title":"LetsEncrypt"},{"location":"aws-terraform-guide/#acm","text":"Note You are using ACM if your use_acm variable is set to \"true\" . When using ACM, the service name for the proxy is different ( teleport-proxy-acm.service vs teleport-proxy.service ).","title":"ACM"},{"location":"aws-terraform-guide/#adding-ec2-instances-to-your-teleport-cluster","text":"Customers run many workloads within EC2 and depending on how you work, there are many ways to integrate Teleport onto your servers. We recommend looking at our Admin manual . To add new nodes/EC2 servers that you can \"SSH into\" you'll need to: Install the Teleport binary on the Server Run Teleport - we recommend using systemd Set the correct settings in /etc/teleport.yaml Add nodes to the Teleport cluster","title":"Adding EC2 instances to your Teleport cluster"},{"location":"aws-terraform-guide/#getting-the-ca-pin-hash","text":"You can use this command to get the CA pin hash for your Teleport cluster: $ aws ssm get-parameter --name \"/teleport/ ${ TF_VAR_cluster_name } /ca-pin-hash\" --query \"Parameter.Value\" --output text sha256:d021ef54aaf8633c4e15c5cc59479fb2f19b1bbc5432bb95213ee047000689dd You should use this so that nodes can validate the auth server's identity when joining your cluster.","title":"Getting the CA pin hash"},{"location":"aws-terraform-guide/#getting-the-node-join-token","text":"You can use this command to get a join token for your Teleport cluster: $ aws ssm get-parameter --name \"/teleport/ ${ TF_VAR_cluster_name } /tokens/node\" --query \"Parameter.Value\" --output text AQICAHgLq8feq4riNouuw8Wxs5EEPlS2qKIVE5Z/qEo1i6mqfwGX3dW56SdoS6PinTWbZL1RAAAAgzCBgAYJKoZIhvcNAQcGoHMwcQIBADBsBgkqhkiG9w0BBwEwHgYJYIZIAWUDBAEuMBEEDNdu5TxaT8gyJx63eAIBEIA/JEpX2Vte90UmufIzZzvBQcQaKgWr95aN9xZYMEjWbAiNitxkvZgb98FgFn8d9GNwKQgDGfUYDbzsX8EqTtx9 You should use this so that nodes can validate the auth server's identity when joining your cluster. You can also generate a node join token using tctl tokens add --type=node as detailed here in our admin guide .","title":"Getting the node join token"},{"location":"aws-terraform-guide/#joining-nodes-via-the-teleport-auth-server","text":"To join Teleport nodes in the same VPC via the auth server, you can find the hostname for the auth load balancer with this command: $ aws elbv2 describe-load-balancers --names \" ${ TF_VAR_cluster_name } -auth\" --query \"LoadBalancers[*].DNSName\" --output text example-cluster-auth-c5b0fc2764ee015b.elb.us-east-1.amazonaws.com With this method, the nodes should be configured like so: auth_servers : - example-cluster-auth-c5b0fc2764ee015b.elb.us-east-1.amazonaws.com:3025","title":"Joining nodes via the Teleport auth server"},{"location":"aws-terraform-guide/#joining-nodes-via-teleport-iotnode-tunnelling","text":"To join Teleport nodes from outside the same VPC, you will either need to investigate VPC peering/gateways (out of scope for this document) or join your nodes using Teleport's node tunnelling functionality. With this method, you can join the nodes using the public facing proxy address - teleport.example.com:443 for our example. auth_servers : - teleport.example.com:443","title":"Joining nodes via Teleport IoT/node tunnelling"},{"location":"aws-terraform-guide/#trusted-clusters","text":"To add a trusted cluster, you'll need the hostname of the proxy load balancer. You can get it using this command: $ aws elbv2 describe-load-balancers --names \" ${ TF_VAR_cluster_name } -proxy\" --query \"LoadBalancers[*].DNSName\" --output text example-cluster-proxy-7c97b76593d6bf21.elb.us-east-1.amazonaws.com In this example, the tunnel_addr and web_proxy_addr in the trusted cluster configuration should be set up like this: spec : tunnel_addr : example-cluster-proxy-7c97b76593d6bf21.elb.us-east-1.amazonaws.com:3024 web_proxy_addr : teleport.example.com:443 You can generate a token for adding the trusted cluster using tctl tokens add --type=trusted_cluster after connecting to an auth server. Follow the instructions in our trusted cluster guide .","title":"Trusted clusters"},{"location":"aws-terraform-guide/#script-to-quickly-connect-to-instances","text":"Here's a bash script that you can use to quickly connect to instances: #!/bin/bash if [[ \" $1 \" ! = \"\" ]] ; then INSTANCE_TYPE = $1 else INSTANCE_TYPE = \"auth\" fi if [[ \" $2 \" ! = \"\" ]] ; then INSTANCE_ID = $2 else INSTANCE_ID = \"0\" fi export BASTION_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=bastion\" --query \"Reservations[*].Instances[*].PublicIpAddress\" --output text ) echo \"Bastion IP: ${ BASTION_IP } \" if [[ \" ${ INSTANCE_TYPE } \" == \"auth\" ]] ; then export SERVER_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=auth\" --query \"Reservations[ ${ INSTANCE_ID } ].Instances[*].PrivateIpAddress\" --output text ) echo \"Auth ${ INSTANCE_ID } IP: ${ SERVER_IP } \" elif [[ \" ${ INSTANCE_TYPE } \" == \"proxy\" ]] ; then export SERVER_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=proxy\" --query \"Reservations[ ${ INSTANCE_ID } ].Instances[*].PrivateIpAddress\" --output text ) echo \"Proxy ${ INSTANCE_ID } IP: ${ SERVER_IP } \" elif [[ \" ${ INSTANCE_TYPE } \" == \"node\" ]] ; then export SERVER_IP = $( aws ec2 describe-instances --filters \"Name=tag:TeleportCluster,Values= ${ TF_VAR_cluster_name } ,Name=tag:TeleportRole,Values=node\" --query \"Reservations[*].Instances[*].PrivateIpAddress\" --output text ) echo \"Node IP: ${ SERVER_IP } \" fi echo \"Keypair name: ${ TF_VAR_key_name } \" ssh -i ${ TF_VAR_key_name } .pem -o ProxyCommand = \"ssh -i ${ TF_VAR_key_name } .pem -W '[%h]:%p' ec2-user@ ${ BASTION_IP } \" ec2-user@ ${ SERVER_IP } Save this as connect.sh , run chmod +x connect.sh to make it executable, then use it like so: # connect to the first auth server $ ./connect.sh auth 0 # connect to the second auth server $ ./connect.sh auth 1 # connect to the first proxy server $ ./connect.sh proxy 0 # connect to the second proxy server $ ./connect.sh proxy 1 # connect to the node $ ./connect.sh node","title":"Script to quickly connect to instances"},{"location":"cli-docs/","text":"Command Line (CLI) Reference Teleport is made up of three CLI tools. teleport : The Teleport daemon that runs the Teleport Service, and acts as a daemon on a node allowing SSH connections. tsh : A tool that let's end users interact with Teleport nodes. This replaces ssh . tctl : An administrative tool that can configure Teleport Auth Service. teleport The Teleport daemon is called teleport . It can be configured to run one or more \"roles\" with the --roles flags. The arguments to --roles correspond to the following services. Service Role Name Description Node node Runs a daemon on a node which allows SSH connections from authenticated clients. Auth auth Authenticates nodes and users who want access to Teleport Nodes or information about the cluster Proxy proxy The gateway that clients use to connect to the Auth or Node Services teleport start Flags Name Default Value(s) Allowed Value(s) Description -d, --debug none none enable verbose logging to stderr --insecure-no-tls false true or false Tells proxy to not generate default self-signed TLS certificates. This is useful when running Teleport on kubernetes (behind reverse proxy) or behind things like AWS ELBs, GCP LBs or Azure Load Balancers where SSL termination is provided externally. -r, --roles proxy,node,auth string comma-separated list of proxy, node or auth start listed services/roles. These roles are explained in the Teleport Architecture document. --pid-file none string filepath create a PID file at the path --advertise-ip none string IP advertise IP to clients, often used behind NAT -l, --listen-ip 0.0.0.0 net. IP binds services to IP --auth-server none string IP proxy attempts to connect to a specified auth server instead of local auth, disables --roles=auth if set --token none string set invitation token to register with an auth server on start, used once and ignored afterwards. Obtain it by running tctl nodes add on the auth server. We recommend to use tools like pwgen to generate sufficiently random tokens of 32+ byte length. --ca-pin none string sha256:<hash> set CA pin to validate the Auth Server. Generated by tctl status --nodename hostname command on the machine string assigns an alternative name for the node which can be used by clients to login. By default it's equal to the value returned by -c, --config /etc/teleport.yaml string .yaml filepath starts services with config specified in the YAML file, overrides CLI flags if set --bootstrap none string .yaml filepath bootstrap configured YAML resources --labels none string comma-separated list assigns a set of labels to a node. See the explanation of labeling mechanism in the Labeling Nodes section. --insecure none none disable certificate validation on Proxy Service, validation still occurs on Auth Service. --fips none none start Teleport in FedRAMP/FIPS 140-2 mode. --diag-addr none none Enable diagnostic endpoints --permit-user-env none none flag reads in environment variables from ~/.tsh/environment when creating a session. Token Generation We recommend the use of tools like pwgen to generate sufficiently random tokens of 32+ byte length. teleport status teleport status shows the status of a Teleport connection. This command is only available from inside of a recorded SSH session. teleport configure teleport configure dumps a sample configuration file in YAML format into standard output. Caution : This sample config is not the default config and should be used for reference only. teleport version teleport version show the release version teleport help teleport help shows help teleport and its subcommands like this teleport help <subcommand> tsh tsh is a CLI client used by Teleport Users. It allows users to interact with current and past sessions on the cluster, copy files to and from nodes, and list information about the cluster. tsh Global Flags Name Default Value(s) Allowed Value(s) Description -l, --login none an identity name the login identity that the Teleport User should use --proxy none host:https_port[,ssh_proxy_port] set SSH proxy address --user $USER none the Teleport User name --ttl none relative duration like 5s, 2m, or 3h set time to live for a SSH session, session ttl unrestricted if unset -i, --identity none string filepath Identity file --cert-format file file or openssh SSH certificate format --insecure none none Do not verify server's certificate and host name. Use only in test environments --auth local any defined authentication connector Specify the type of authentication connector to use. --skip-version-check none none Skip version checking between server and client. -d, --debug none none Verbose logging to stdout -J, --jumphost none A jump host SSH jumphost tsh help Prints help Usage tsh help tsh version Prints client version Usage tsh version tsh ssh Run shell or execute a command on a remote SSH node Usage : tsh ssh [<flags>] <[user@]host> [<command>...] Arguments <[user@]host> [<command>...] user The login identity to use on the remote host. If [user] is not specified the user defaults to $USER or can be set with --user . If the flag --user and positional argument [user] are specified the arg [user] takes precedence. host A nodename of a cluster node or a command The command to execute on a remote host. Flags Name Default Value(s) Allowed Value(s) Description -p, --port none port SSH port on a remote host -A, --forward-agent none none Forward agent to target node like ssh -A -L, --forward none none Forward localhost connections to remote server -D, --dynamic-forward none none Forward localhost connections to remote server using SOCKS5 -N, -no-remote-exec none none Don't execute remote command, useful for port forwarding --local none Execute command on localhost after connecting to SSH node -t, --tty file Allocate TTY --cluster none Specify the cluster to connect -o, --option local OpenSSH options in the format used in the configuration file --enable-escape-sequences Enable support for SSH escape sequences. Type '~?' during an SSH session to list supported sequences. --no-use-local-ssh-agent Do not load generated SSH certificates into the local ssh-agent (specified via $SSH_AUTH_SOCK ). Useful when using gpg-agent or Yubikeys. You can also set the TELEPORT_USE_LOCAL_SSH_AGENT environment variable to false (default true ) Global Flags These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version-check, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section Examples # Log in to node `grav-00` as OS User `root` with Teleport User `teleport` $ tsh ssh --proxy proxy.example.com --user teleport -d root@grav-00 # `tsh ssh` takes the same arguments as OpenSSH client: $ tsh ssh -o ForwardAgent=yes root@grav-00 $ tsh ssh -o AddKeysToAgent=yes root@grav-00 tsh join Joins an active session Usage : tsh join [<flags>] <session-id> Arguments <session-id> session-id The UUID of the an active Teleport Session obtained by teleport status within the session. Flags Name Default Value(s) Allowed Value(s) Description --cluster none a cluster_name Specify the cluster to connect Global Flags These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version-check, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section Examples tsh --proxy proxy.example.com join <session-id> tsh play Plays back a prior session Usage : tsh play [<flags>] <session-id> Arguments <session-id> session-id The UUID of the a past Teleport Session obtained by teleport status within the session or from the Web UI. Flags Name Default Value(s) Allowed Value(s) Description --cluster none a cluster_name Specify the cluster to connect Global Flags These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version-check, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section Examples tsh --proxy proxy.example.com play <session-id> tsh scp Copies files from source to dest Usage usage: tsh scp [<flags>] <source>... <dest> Arguments <source> - filepath to copy <dest> - target destination Flags Name Default Value(s) Allowed Value(s) Description --cluster none a cluster_name Specify the cluster to connect -r, --recursive none none Recursive copy of subdirectories -P, --port none port number Port to connect to on the remote host -q, --quiet none none Quiet mode Global Flags These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section Examples $ tsh --proxy=proxy.example.com scp -P example.txt user@host/destination/dir tsh ls List cluster nodes Usage usage: tsh ls [<flags>] [<label>] Arguments <label> - key=value label to filer nodes by Flags Name Default Value(s) Allowed Value(s) Description -v, --verbose none none also print Node ID Global Flags These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section Examples $ tsh ls Node Name Address Labels --------- ------------------ ------ grav-00 10.164.0.0:3022 os:linux grav-01 10.156.0.2:3022 os:linux grav-02 10.156.0.7:3022 os:osx $ tsh ls -v Node Name Node ID Address Labels --------- ------------------------------------ ------------------ ------ grav-00 52e3e46a-372f-494b-bdd9-a1d25b9d6dec 10.164.0.0:3022 os:linux grav-01 73d86fc7-7c4b-42e3-9a5f-c46e177a29e8 10.156.0.2:3022 os:linux grav-02 24503590-e8ae-4a0a-ad7a-dd1865c04e30 10.156.0.7:3022 os:osx # only show nodes with os label set to 'osx': $ tsh ls os=osx Node Name Address Labels --------- ------------------ ------ grav-02 10.156.0.7:3022 os:osx tsh clusters Usage : tsh clusters [<flags>] Flags Name Default Value(s) Allowed Value(s) Description -q, --quiet none none no headers in output Global Flags These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section Examples $ tsh clusters Cluster Name Status ------------ ------ staging online production offline $ tsh clusters --quiet staging online production offline tsh login Logs in to the cluster. When tsh logs in, the auto-expiring key is stored in ~/.tsh and is valid for 12 hours by default, unless you specify another interval via --ttl flag (capped by the server-side configuration). Usage : tsh login [<flags>] [<cluster>] Arguments <cluster> - the name of the cluster, see Trusted Cluster for more information. Flags Name Default Value(s) Allowed Value(s) Description --bind-addr none host:port Address in the form of host:port to bind to for login command webhook -o, --out none filepath Identity output filepath --format file file , openssh or kubernetes Identity format: file, openssh (for OpenSSH compatibility) or kubernetes (for kubeconfig) --browser none none Set to 'none' to suppress opening system default browser for tsh login commands --request-roles none Request one or more extra roles Global Flags These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version-check, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section Examples The proxy endpoint can take a https and ssh port in this format host:https_port[,ssh_proxy_port] # Use ports 8080 and 8023 for https and SSH proxy: $ tsh --proxy=proxy.example.com:8080,8023 login # Use port 8080 and 3023 (default) for SSH proxy: $ tsh --proxy=proxy.example.com:8080 login # Use port 23 as custom SSH port, keep HTTPS proxy port as default $ tsh --proxy=work.example.com:,23 login # Login and select cluster \"two\": $ tsh --proxy=proxy.example.com login two # Select cluster \"two\" using existing credentials and proxy: $ tsh login two # Login to the cluster with a very short-lived certificate $ tsh --ttl=1 login # Login using the local Teleport 'admin' user: $ tsh --proxy=proxy.example.com --auth=local --user=admin login # Login using Github as an SSO provider, assuming the Github connector is called \"github\" $ tsh --proxy=proxy.example.com --auth=github --user=admin login # Suppress the opening of the system default browser for external provider logins $ tsh --proxy=proxy.example.com --browser=none # Login to cluster and output a local kubeconfig $ tsh login --proxy=proxy.example.com --format=kubernetes -o kubeconfig tsh logout Deletes the client's cluster certificate Usage : tsh logout tsh status Display the list of proxy servers and retrieved certificates Usage : tsh status Examples $ tsh status > Profile URL: https://proxy.example.com:3080 Logged in as: johndoe Roles: admin* Logins: root, admin, guest Valid until : 2017 -04-25 15 :02:30 -0700 PDT [ valid for 1h0m0s ] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty tctl tctl is an admin CLI tool used to administer a Teleport cluster. It connects to an Auth Server, meaning that it must be run on the same host with the role auth . If you run tctl on a non- auth node it will show an error. tctl allows a cluster administrator to manage all resources in a cluster including nodes, users, tokens, and certificates. tctl can also be used to modify the dynamic configuration of the cluster, like creating new user roles or connecting trusted clusters. tctl Global Flags Name Default Value(s) Allowed Value(s) Description -d, --debug none none Enable verbose logging to stderr -c, --config /etc/teleport.yaml string filepath Path to a configuration file tctl help Shows help. Usage tctl help tctl users add Generates a user invitation token. Usage: tctl users add [<flags>] <account> [<local-logins>] Arguments <account> - The Teleport user account name. <local-logins> - A comma-separated list of local UNIX users this account can log in as. If unspecified the account will be mapped to an OS user of the same name. See examples below. Flags Name Default Value(s) Allowed Value(s) Description --k8s-groups none a kubernetes group Kubernetes groups to assign to a user, e.g. system:masters --k8s-users none a kubernetes user Kubernetes user to assign to a user, e.g. jenkins --ttl 1h relative duration like 5s, 2m, or 3h, maximum 48h Set expiration time for token Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section Examples # Adds teleport user \"joe\" with mappings to # OS users \"joe\" and \"root\" tctl users add joe joe,root # Adds teleport user \"joe\" with mappings to # OS users \"joe\" only tctl users add joe tctl users ls Lists all user accounts Usage: tctl users ls [<flags>] tctl users rm Deletes user accounts Usage : tctl users rm <logins> Arguments <logins> - comma-separated list of Teleport users Examples tctl users rm sally,tim # Removes users sally and tim tctl request ls List of open requests Usage : tctl request ls Examples tctl request ls Token Requestor Metadata Created At (UTC) Status ------------------------------------ --------- -------------- ------------------- ------- request-id-1 alice roles=dictator 07 Nov 19 19:38 UTC PENDING tctl request approve Approve a user's request. Usage : tctl request approve [token] Arguments <tokens> - comma-separated list of Teleport tokens. Examples tctl request approve request-id-1, request-id-2 tctl request deny Denies a user's request. Usage : tctl request deny [token] Arguments <tokens> - comma-separated list of Teleport tokens. Examples tctl request deny request-id-1, request-id-2 tctl request rm Delete a users role request. Usage : tctl request rm [token] Arguments <tokens> - comma-separated list of Teleport tokens. Examples tctl request rm request-id-1 tctl nodes add Generate a node invitation token Usage : tctl nodes add [<flags>] Flags Name Default Value(s) Allowed Value(s) Description --roles node node,auth or proxy Comma-separated list of roles for the new node to assume --ttl 30m relative duration like 5s, 2m, or 3h Time to live for a generated token --token none string token value A custom token to use, auto-generated if not provided. Should match token set with teleport start --token Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section Examples # Generates a token which can be used by a node to join the cluster, default ttl is 30 minutes $ tctl nodes add # Generates a token which can be used to add an SSH node to the cluster # The node will run the proxy service in addition is the node (ssh) service. # This token can be used within an hour. $ tctl nodes add --roles=node,proxy --ttl=1h tctl nodes ls List all active SSH nodes within the cluster Usage : tctl nodes ls [<flags>] Flags Name Default Value(s) Allowed Value(s) Description --namespace none string namespace Namespace of the nodes Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section tctl tokens add Create an invitation token Usage : tctl tokens add --type=TYPE [<flags>] Flags Name Default Value(s) Allowed Value(s) Description --type none trusted_cluster , node , proxy Type of token to add --value none string token value Value of token to add --ttl 1h relative duration like 5s, 2m, or 3h, maximum 48h Set expiration time for token Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section Examples # Generate an invite token for a trusted_cluster $ tctl tokens add --type = trusted_cluster --ttl = 5m # Generate an invite token for a node # This is equivalent to `tctl nodes add` $ tctl tokens add --type node tctl tokens rm Delete/revoke an invitation token Usage : tctl tokens rm [<token>] Arguments <token> The full-length token string to delete tctl tokens ls List node and user invitation tokens Usage : tctl tokens ls [<flags>] Example $ tctl tokens ls Token Type Expiry Time (UTC) -------------------------------- --------------- ------------------- ecce46d19bb4144716e5984269db1ac0 Node 11 Oct 19 22:17 UTC fcbf269ca26440f35865ec29994f0fb4 trusted_cluster 11 Oct 19 22:19 UTC 6fd001d4200348deec9b50c4479ba07d User signup 11 Oct 19 22:20 UTC tctl auth export Export public cluster (CA) keys to stdout Usage : tctl auth export [<flags>] Flags Name Default Value(s) Allowed Value(s) Description --keys none none if set, will print private keys --fingerprint none string e.g. SHA265:<fingerprint> filter authority by fingerprint --compat none version number export certificates compatible with specific version of Teleport --type none user, host or tls certificate type Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section Examples # Export all keys $ tctl auth export # Filter by fingerprint $ tctl auth export --fingerprint=SHA256:8xu5kh1CbHCZRrGuitbQd4hM+d9V+I7YA1mUwA/2tAo # Export tls certs only $ tctl auth export --type tls tctl auth sign Create an identity file(s) for a given user Usage : tctl auth sign -o <filepath> [--user <user> | --host <host>][--format] [<flags>] Flags Name Default Value(s) Allowed Value(s) Description --user none existing user Teleport user name --host none auth host Teleport host name -o, --out none filepath identity output --format file file , openssh , tls or kubernetes identity format --identity file file identity format --auth-server none auth host & port Remote Teleport host name --ttl none relative duration like 5s, 2m, or 3h TTL (time to live) for the generated certificate --compat \"\" standard or oldssh OpenSSH compatibility flag --proxy \"\" Address of the teleport proxy. When --format is set to \"kubernetes\", this address will be set as cluster address in the generated kubeconfig file Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section Examples # Export identity file to teleport_id.pem # for user `teleport` with a ttl set to 5m $ tctl auth sign --format file --ttl = 5m --user teleport -o teleport_id.pem # Export identity formatted for openssh to teleport_id.pem $ tctl auth sign --format openssh --user teleport -o teleport_id.pem # Export host identity, `--format openssh` must be set with `--host` # Generates grav-01 (private key) and grav-01-cert.pub in the current directory $ tctl auth sign --format openssh --host grav-00 # Invalid command, only one of --user or --host should be set $ tctl auth sign --format openssh --host grav-00 --user teleport -o grav_host error: --user or --host must be specified # create a certificate with a TTL of 10 years for the jenkins user # the jenkins.pem file can later be used with `tsh` $ tctl auth sign --ttl = 87600h --user = jenkins --out = jenkins.pem # create a certificate with a TTL of 1 day for the jenkins user # the jenkins.pem file can later be used with `tsh` $ tctl auth sign --ttl = 24h --user = jenkins --out = jenkins.pem # create a certificate with a TTL of 1 day for the jenkins user # The kubeconfig file can later be used with `kubectl` or compatible tooling. $ tctl auth sign --ttl = 24h --user = jenkins --out = kubeconfig --format = kubernetes # Exports an identity from the Auth Server in preparation for remote # tctl execution. $ tctl auth sign --user = admin --out = identity.pem tctl auth rotate Rotate certificate authorities in the cluster Usage : tctl auth rotate [<flags>] Flags Name Default Value(s) Allowed Value(s) Description --grace-period none relative duration like 5s, 2m, or 3h Grace period keeps previous certificate authorities signatures valid, if set to 0 will force users to login again and nodes to re-register. --manual none none Activate manual rotation, set rotation phases manually --type user,host user or host Certificate authority to rotate --phase init, standby, update_clients, update_servers, rollback Target rotation phase to set, used in manual rotation Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section Examples # rotate only user certificates with a grace period of 200 hours: $ tctl auth rotate --type=user --grace-period=200h # rotate only host certificates with a grace period of 8 hours: $ tctl auth rotate --type=host --grace-period=8h tctl create Create or update a Teleport resource from a YAML file. The supported resource types are: user, node, cluster, role, connector. See the Resource Guide for complete docs on how to build these yaml files. Usage : tctl create [<flags>] <filename> Arguments <filename> resource definition file Flags Name Default Value(s) Allowed Value(s) Description -f, --force none none Overwrite the resource if already exists Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section Examples # Update a user record $ tctl create -f joe.yaml # Add a trusted cluster $ tctl create cluster.yaml $ Update a trusted cluster $ tctl create -f cluster.yaml tctl rm Delete a resource Usage : tctl rm [<resource-type/resource-name>] Arguments [<resource-type/resource-name>] Resource to delete <resource type> Type of a resource [for example: saml,oidc,github,user,cluster,token ] <resource name> Resource name to delete Examples # delete a SAML connector called \"okta\": $ tctl rm saml/okta # delete a local user called \"admin\": $ tctl rm users/admin tctl get Print a YAML declaration of various Teleport resources Usage : tctl get [<flags>] [<resource-type/resource-name>],... Arguments [<resource-type/resource-name>] Resource to delete <resource type> Type of a resource [for example: user,cluster,token ] <resource name> Resource name to delete Flags Name Default Value(s) Allowed Value(s) Description --format yaml, json or text Output format --with-secrets none none Include secrets in resources like certificate authorities or OIDC connectors Global Flags These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section Examples $ tctl get users # dump the user definition into a file: $ tctl get user/joe > joe.yaml # prints the trusted cluster 'east' $ tctl get cluster/east # prints all trusted clusters and all users $ tctl get clusters,users # Dump all resources for backup into state.yaml $ tctl get all > state.yaml tctl status Report cluster status Usage tctl status Examples # Checks status of cluster. $ tctl status Cluster grav-00 User CA never updated Host CA never updated CA pin sha256:1146cdd2b887772dcc2e879232c8f60012a839f7958724ce5744005474b15b9d # Checks remote auth status using exported identity. $ tctl status \\ --auth-server = 192 .168.99.102:3025 \\ --identity = identity.pem tctl top Reports diagnostic information. The diagnostic metrics endpoint must be enabled with teleport start --diag-addr=<bind-addr> for tctl top to work. Usage tctl top [<diag-addr>] [<refresh>] Argument [<diag-addr>] Diagnostic HTTP URL (HTTPS not supported) [<refresh>] Refresh period e.g.5s, 2m, or 3h Example $ teleport start --diag-addr=127.0.0.1:3434 # View stats with refresh period of 5 seconds $ tctl top http://127.0.0.1:3434 5s tctl version Print cluster version Usage: tctl version","title":"CLI"},{"location":"cli-docs/#command-line-cli-reference","text":"Teleport is made up of three CLI tools. teleport : The Teleport daemon that runs the Teleport Service, and acts as a daemon on a node allowing SSH connections. tsh : A tool that let's end users interact with Teleport nodes. This replaces ssh . tctl : An administrative tool that can configure Teleport Auth Service.","title":"Command Line (CLI) Reference"},{"location":"cli-docs/#teleport","text":"The Teleport daemon is called teleport . It can be configured to run one or more \"roles\" with the --roles flags. The arguments to --roles correspond to the following services. Service Role Name Description Node node Runs a daemon on a node which allows SSH connections from authenticated clients. Auth auth Authenticates nodes and users who want access to Teleport Nodes or information about the cluster Proxy proxy The gateway that clients use to connect to the Auth or Node Services","title":"teleport"},{"location":"cli-docs/#teleport-start","text":"","title":"teleport start"},{"location":"cli-docs/#flags","text":"Name Default Value(s) Allowed Value(s) Description -d, --debug none none enable verbose logging to stderr --insecure-no-tls false true or false Tells proxy to not generate default self-signed TLS certificates. This is useful when running Teleport on kubernetes (behind reverse proxy) or behind things like AWS ELBs, GCP LBs or Azure Load Balancers where SSL termination is provided externally. -r, --roles proxy,node,auth string comma-separated list of proxy, node or auth start listed services/roles. These roles are explained in the Teleport Architecture document. --pid-file none string filepath create a PID file at the path --advertise-ip none string IP advertise IP to clients, often used behind NAT -l, --listen-ip 0.0.0.0 net. IP binds services to IP --auth-server none string IP proxy attempts to connect to a specified auth server instead of local auth, disables --roles=auth if set --token none string set invitation token to register with an auth server on start, used once and ignored afterwards. Obtain it by running tctl nodes add on the auth server. We recommend to use tools like pwgen to generate sufficiently random tokens of 32+ byte length. --ca-pin none string sha256:<hash> set CA pin to validate the Auth Server. Generated by tctl status --nodename hostname command on the machine string assigns an alternative name for the node which can be used by clients to login. By default it's equal to the value returned by -c, --config /etc/teleport.yaml string .yaml filepath starts services with config specified in the YAML file, overrides CLI flags if set --bootstrap none string .yaml filepath bootstrap configured YAML resources --labels none string comma-separated list assigns a set of labels to a node. See the explanation of labeling mechanism in the Labeling Nodes section. --insecure none none disable certificate validation on Proxy Service, validation still occurs on Auth Service. --fips none none start Teleport in FedRAMP/FIPS 140-2 mode. --diag-addr none none Enable diagnostic endpoints --permit-user-env none none flag reads in environment variables from ~/.tsh/environment when creating a session. Token Generation We recommend the use of tools like pwgen to generate sufficiently random tokens of 32+ byte length.","title":"Flags"},{"location":"cli-docs/#teleport-status","text":"teleport status shows the status of a Teleport connection. This command is only available from inside of a recorded SSH session.","title":"teleport status"},{"location":"cli-docs/#teleport-configure","text":"teleport configure dumps a sample configuration file in YAML format into standard output. Caution : This sample config is not the default config and should be used for reference only.","title":"teleport configure"},{"location":"cli-docs/#teleport-version","text":"teleport version show the release version","title":"teleport version"},{"location":"cli-docs/#teleport-help","text":"teleport help shows help teleport and its subcommands like this teleport help <subcommand>","title":"teleport help"},{"location":"cli-docs/#tsh","text":"tsh is a CLI client used by Teleport Users. It allows users to interact with current and past sessions on the cluster, copy files to and from nodes, and list information about the cluster.","title":"tsh"},{"location":"cli-docs/#tsh-global-flags","text":"Name Default Value(s) Allowed Value(s) Description -l, --login none an identity name the login identity that the Teleport User should use --proxy none host:https_port[,ssh_proxy_port] set SSH proxy address --user $USER none the Teleport User name --ttl none relative duration like 5s, 2m, or 3h set time to live for a SSH session, session ttl unrestricted if unset -i, --identity none string filepath Identity file --cert-format file file or openssh SSH certificate format --insecure none none Do not verify server's certificate and host name. Use only in test environments --auth local any defined authentication connector Specify the type of authentication connector to use. --skip-version-check none none Skip version checking between server and client. -d, --debug none none Verbose logging to stdout -J, --jumphost none A jump host SSH jumphost","title":"tsh Global Flags"},{"location":"cli-docs/#tsh-help","text":"Prints help Usage tsh help","title":"tsh help"},{"location":"cli-docs/#tsh-version","text":"Prints client version Usage tsh version","title":"tsh version"},{"location":"cli-docs/#tsh-ssh","text":"Run shell or execute a command on a remote SSH node Usage : tsh ssh [<flags>] <[user@]host> [<command>...]","title":"tsh ssh"},{"location":"cli-docs/#arguments","text":"<[user@]host> [<command>...] user The login identity to use on the remote host. If [user] is not specified the user defaults to $USER or can be set with --user . If the flag --user and positional argument [user] are specified the arg [user] takes precedence. host A nodename of a cluster node or a command The command to execute on a remote host.","title":"Arguments"},{"location":"cli-docs/#flags_1","text":"Name Default Value(s) Allowed Value(s) Description -p, --port none port SSH port on a remote host -A, --forward-agent none none Forward agent to target node like ssh -A -L, --forward none none Forward localhost connections to remote server -D, --dynamic-forward none none Forward localhost connections to remote server using SOCKS5 -N, -no-remote-exec none none Don't execute remote command, useful for port forwarding --local none Execute command on localhost after connecting to SSH node -t, --tty file Allocate TTY --cluster none Specify the cluster to connect -o, --option local OpenSSH options in the format used in the configuration file --enable-escape-sequences Enable support for SSH escape sequences. Type '~?' during an SSH session to list supported sequences. --no-use-local-ssh-agent Do not load generated SSH certificates into the local ssh-agent (specified via $SSH_AUTH_SOCK ). Useful when using gpg-agent or Yubikeys. You can also set the TELEPORT_USE_LOCAL_SSH_AGENT environment variable to false (default true )","title":"Flags"},{"location":"cli-docs/#global-flags","text":"These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version-check, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples","text":"# Log in to node `grav-00` as OS User `root` with Teleport User `teleport` $ tsh ssh --proxy proxy.example.com --user teleport -d root@grav-00 # `tsh ssh` takes the same arguments as OpenSSH client: $ tsh ssh -o ForwardAgent=yes root@grav-00 $ tsh ssh -o AddKeysToAgent=yes root@grav-00","title":"Examples"},{"location":"cli-docs/#tsh-join","text":"Joins an active session Usage : tsh join [<flags>] <session-id>","title":"tsh join"},{"location":"cli-docs/#arguments_1","text":"<session-id> session-id The UUID of the an active Teleport Session obtained by teleport status within the session.","title":"Arguments"},{"location":"cli-docs/#flags_2","text":"Name Default Value(s) Allowed Value(s) Description --cluster none a cluster_name Specify the cluster to connect","title":"Flags"},{"location":"cli-docs/#global-flags_1","text":"These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version-check, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_1","text":"tsh --proxy proxy.example.com join <session-id>","title":"Examples"},{"location":"cli-docs/#tsh-play","text":"Plays back a prior session Usage : tsh play [<flags>] <session-id>","title":"tsh play"},{"location":"cli-docs/#arguments_2","text":"<session-id> session-id The UUID of the a past Teleport Session obtained by teleport status within the session or from the Web UI.","title":"Arguments"},{"location":"cli-docs/#flags_3","text":"Name Default Value(s) Allowed Value(s) Description --cluster none a cluster_name Specify the cluster to connect","title":"Flags"},{"location":"cli-docs/#global-flags_2","text":"These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version-check, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_2","text":"tsh --proxy proxy.example.com play <session-id>","title":"Examples"},{"location":"cli-docs/#tsh-scp","text":"Copies files from source to dest Usage usage: tsh scp [<flags>] <source>... <dest>","title":"tsh scp"},{"location":"cli-docs/#arguments_3","text":"<source> - filepath to copy <dest> - target destination","title":"Arguments"},{"location":"cli-docs/#flags_4","text":"Name Default Value(s) Allowed Value(s) Description --cluster none a cluster_name Specify the cluster to connect -r, --recursive none none Recursive copy of subdirectories -P, --port none port number Port to connect to on the remote host -q, --quiet none none Quiet mode","title":"Flags"},{"location":"cli-docs/#global-flags_3","text":"These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_3","text":"$ tsh --proxy=proxy.example.com scp -P example.txt user@host/destination/dir","title":"Examples"},{"location":"cli-docs/#tsh-ls","text":"List cluster nodes Usage usage: tsh ls [<flags>] [<label>]","title":"tsh ls"},{"location":"cli-docs/#arguments_4","text":"<label> - key=value label to filer nodes by","title":"Arguments"},{"location":"cli-docs/#flags_5","text":"Name Default Value(s) Allowed Value(s) Description -v, --verbose none none also print Node ID","title":"Flags"},{"location":"cli-docs/#global-flags_4","text":"These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_4","text":"$ tsh ls Node Name Address Labels --------- ------------------ ------ grav-00 10.164.0.0:3022 os:linux grav-01 10.156.0.2:3022 os:linux grav-02 10.156.0.7:3022 os:osx $ tsh ls -v Node Name Node ID Address Labels --------- ------------------------------------ ------------------ ------ grav-00 52e3e46a-372f-494b-bdd9-a1d25b9d6dec 10.164.0.0:3022 os:linux grav-01 73d86fc7-7c4b-42e3-9a5f-c46e177a29e8 10.156.0.2:3022 os:linux grav-02 24503590-e8ae-4a0a-ad7a-dd1865c04e30 10.156.0.7:3022 os:osx # only show nodes with os label set to 'osx': $ tsh ls os=osx Node Name Address Labels --------- ------------------ ------ grav-02 10.156.0.7:3022 os:osx","title":"Examples"},{"location":"cli-docs/#tsh-clusters","text":"Usage : tsh clusters [<flags>]","title":"tsh clusters"},{"location":"cli-docs/#flags_6","text":"Name Default Value(s) Allowed Value(s) Description -q, --quiet none none no headers in output","title":"Flags"},{"location":"cli-docs/#global-flags_5","text":"These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_5","text":"$ tsh clusters Cluster Name Status ------------ ------ staging online production offline $ tsh clusters --quiet staging online production offline","title":"Examples"},{"location":"cli-docs/#tsh-login","text":"Logs in to the cluster. When tsh logs in, the auto-expiring key is stored in ~/.tsh and is valid for 12 hours by default, unless you specify another interval via --ttl flag (capped by the server-side configuration). Usage : tsh login [<flags>] [<cluster>]","title":"tsh login"},{"location":"cli-docs/#arguments_5","text":"<cluster> - the name of the cluster, see Trusted Cluster for more information.","title":"Arguments"},{"location":"cli-docs/#flags_7","text":"Name Default Value(s) Allowed Value(s) Description --bind-addr none host:port Address in the form of host:port to bind to for login command webhook -o, --out none filepath Identity output filepath --format file file , openssh or kubernetes Identity format: file, openssh (for OpenSSH compatibility) or kubernetes (for kubeconfig) --browser none none Set to 'none' to suppress opening system default browser for tsh login commands --request-roles none Request one or more extra roles","title":"Flags"},{"location":"cli-docs/#global-flags_6","text":"These flags are available for all commands --login, --proxy, --user, --ttl, --identity, --cert-format, --insecure, --auth, --skip-version-check, --debug, --jumphost . Run tsh help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_6","text":"The proxy endpoint can take a https and ssh port in this format host:https_port[,ssh_proxy_port] # Use ports 8080 and 8023 for https and SSH proxy: $ tsh --proxy=proxy.example.com:8080,8023 login # Use port 8080 and 3023 (default) for SSH proxy: $ tsh --proxy=proxy.example.com:8080 login # Use port 23 as custom SSH port, keep HTTPS proxy port as default $ tsh --proxy=work.example.com:,23 login # Login and select cluster \"two\": $ tsh --proxy=proxy.example.com login two # Select cluster \"two\" using existing credentials and proxy: $ tsh login two # Login to the cluster with a very short-lived certificate $ tsh --ttl=1 login # Login using the local Teleport 'admin' user: $ tsh --proxy=proxy.example.com --auth=local --user=admin login # Login using Github as an SSO provider, assuming the Github connector is called \"github\" $ tsh --proxy=proxy.example.com --auth=github --user=admin login # Suppress the opening of the system default browser for external provider logins $ tsh --proxy=proxy.example.com --browser=none # Login to cluster and output a local kubeconfig $ tsh login --proxy=proxy.example.com --format=kubernetes -o kubeconfig","title":"Examples"},{"location":"cli-docs/#tsh-logout","text":"Deletes the client's cluster certificate Usage : tsh logout","title":"tsh logout"},{"location":"cli-docs/#tsh-status","text":"Display the list of proxy servers and retrieved certificates Usage : tsh status","title":"tsh status"},{"location":"cli-docs/#examples_7","text":"$ tsh status > Profile URL: https://proxy.example.com:3080 Logged in as: johndoe Roles: admin* Logins: root, admin, guest Valid until : 2017 -04-25 15 :02:30 -0700 PDT [ valid for 1h0m0s ] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty","title":"Examples"},{"location":"cli-docs/#tctl","text":"tctl is an admin CLI tool used to administer a Teleport cluster. It connects to an Auth Server, meaning that it must be run on the same host with the role auth . If you run tctl on a non- auth node it will show an error. tctl allows a cluster administrator to manage all resources in a cluster including nodes, users, tokens, and certificates. tctl can also be used to modify the dynamic configuration of the cluster, like creating new user roles or connecting trusted clusters.","title":"tctl"},{"location":"cli-docs/#tctl-global-flags","text":"Name Default Value(s) Allowed Value(s) Description -d, --debug none none Enable verbose logging to stderr -c, --config /etc/teleport.yaml string filepath Path to a configuration file","title":"tctl Global Flags"},{"location":"cli-docs/#tctl-help","text":"Shows help. Usage tctl help","title":"tctl  help"},{"location":"cli-docs/#tctl-users-add","text":"Generates a user invitation token. Usage: tctl users add [<flags>] <account> [<local-logins>]","title":"tctl users add"},{"location":"cli-docs/#arguments_6","text":"<account> - The Teleport user account name. <local-logins> - A comma-separated list of local UNIX users this account can log in as. If unspecified the account will be mapped to an OS user of the same name. See examples below.","title":"Arguments"},{"location":"cli-docs/#flags_8","text":"Name Default Value(s) Allowed Value(s) Description --k8s-groups none a kubernetes group Kubernetes groups to assign to a user, e.g. system:masters --k8s-users none a kubernetes user Kubernetes user to assign to a user, e.g. jenkins --ttl 1h relative duration like 5s, 2m, or 3h, maximum 48h Set expiration time for token","title":"Flags"},{"location":"cli-docs/#global-flags_7","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_8","text":"# Adds teleport user \"joe\" with mappings to # OS users \"joe\" and \"root\" tctl users add joe joe,root # Adds teleport user \"joe\" with mappings to # OS users \"joe\" only tctl users add joe","title":"Examples"},{"location":"cli-docs/#tctl-users-ls","text":"Lists all user accounts Usage: tctl users ls [<flags>]","title":"tctl users ls"},{"location":"cli-docs/#tctl-users-rm","text":"Deletes user accounts Usage : tctl users rm <logins>","title":"tctl users rm"},{"location":"cli-docs/#arguments_7","text":"<logins> - comma-separated list of Teleport users","title":"Arguments"},{"location":"cli-docs/#examples_9","text":"tctl users rm sally,tim # Removes users sally and tim","title":"Examples"},{"location":"cli-docs/#tctl-request-ls","text":"List of open requests Usage : tctl request ls","title":"tctl request ls"},{"location":"cli-docs/#examples_10","text":"tctl request ls Token Requestor Metadata Created At (UTC) Status ------------------------------------ --------- -------------- ------------------- ------- request-id-1 alice roles=dictator 07 Nov 19 19:38 UTC PENDING","title":"Examples"},{"location":"cli-docs/#tctl-request-approve","text":"Approve a user's request. Usage : tctl request approve [token]","title":"tctl request approve"},{"location":"cli-docs/#arguments_8","text":"<tokens> - comma-separated list of Teleport tokens.","title":"Arguments"},{"location":"cli-docs/#examples_11","text":"tctl request approve request-id-1, request-id-2","title":"Examples"},{"location":"cli-docs/#tctl-request-deny","text":"Denies a user's request. Usage : tctl request deny [token]","title":"tctl request deny"},{"location":"cli-docs/#arguments_9","text":"<tokens> - comma-separated list of Teleport tokens.","title":"Arguments"},{"location":"cli-docs/#examples_12","text":"tctl request deny request-id-1, request-id-2","title":"Examples"},{"location":"cli-docs/#tctl-request-rm","text":"Delete a users role request. Usage : tctl request rm [token]","title":"tctl request rm"},{"location":"cli-docs/#arguments_10","text":"<tokens> - comma-separated list of Teleport tokens.","title":"Arguments"},{"location":"cli-docs/#examples_13","text":"tctl request rm request-id-1","title":"Examples"},{"location":"cli-docs/#tctl-nodes-add","text":"Generate a node invitation token Usage : tctl nodes add [<flags>]","title":"tctl nodes add"},{"location":"cli-docs/#flags_9","text":"Name Default Value(s) Allowed Value(s) Description --roles node node,auth or proxy Comma-separated list of roles for the new node to assume --ttl 30m relative duration like 5s, 2m, or 3h Time to live for a generated token --token none string token value A custom token to use, auto-generated if not provided. Should match token set with teleport start --token","title":"Flags"},{"location":"cli-docs/#global-flags_8","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_14","text":"# Generates a token which can be used by a node to join the cluster, default ttl is 30 minutes $ tctl nodes add # Generates a token which can be used to add an SSH node to the cluster # The node will run the proxy service in addition is the node (ssh) service. # This token can be used within an hour. $ tctl nodes add --roles=node,proxy --ttl=1h","title":"Examples"},{"location":"cli-docs/#tctl-nodes-ls","text":"List all active SSH nodes within the cluster Usage : tctl nodes ls [<flags>]","title":"tctl nodes ls"},{"location":"cli-docs/#flags_10","text":"Name Default Value(s) Allowed Value(s) Description --namespace none string namespace Namespace of the nodes","title":"Flags"},{"location":"cli-docs/#global-flags_9","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#tctl-tokens-add","text":"Create an invitation token Usage : tctl tokens add --type=TYPE [<flags>]","title":"tctl tokens add"},{"location":"cli-docs/#flags_11","text":"Name Default Value(s) Allowed Value(s) Description --type none trusted_cluster , node , proxy Type of token to add --value none string token value Value of token to add --ttl 1h relative duration like 5s, 2m, or 3h, maximum 48h Set expiration time for token","title":"Flags"},{"location":"cli-docs/#global-flags_10","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_15","text":"# Generate an invite token for a trusted_cluster $ tctl tokens add --type = trusted_cluster --ttl = 5m # Generate an invite token for a node # This is equivalent to `tctl nodes add` $ tctl tokens add --type node","title":"Examples"},{"location":"cli-docs/#tctl-tokens-rm","text":"Delete/revoke an invitation token Usage : tctl tokens rm [<token>]","title":"tctl tokens rm"},{"location":"cli-docs/#arguments_11","text":"<token> The full-length token string to delete","title":"Arguments"},{"location":"cli-docs/#tctl-tokens-ls","text":"List node and user invitation tokens Usage : tctl tokens ls [<flags>]","title":"tctl tokens ls"},{"location":"cli-docs/#example","text":"$ tctl tokens ls Token Type Expiry Time (UTC) -------------------------------- --------------- ------------------- ecce46d19bb4144716e5984269db1ac0 Node 11 Oct 19 22:17 UTC fcbf269ca26440f35865ec29994f0fb4 trusted_cluster 11 Oct 19 22:19 UTC 6fd001d4200348deec9b50c4479ba07d User signup 11 Oct 19 22:20 UTC","title":"Example"},{"location":"cli-docs/#tctl-auth-export","text":"Export public cluster (CA) keys to stdout Usage : tctl auth export [<flags>]","title":"tctl auth export"},{"location":"cli-docs/#flags_12","text":"Name Default Value(s) Allowed Value(s) Description --keys none none if set, will print private keys --fingerprint none string e.g. SHA265:<fingerprint> filter authority by fingerprint --compat none version number export certificates compatible with specific version of Teleport --type none user, host or tls certificate type","title":"Flags"},{"location":"cli-docs/#global-flags_11","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_16","text":"# Export all keys $ tctl auth export # Filter by fingerprint $ tctl auth export --fingerprint=SHA256:8xu5kh1CbHCZRrGuitbQd4hM+d9V+I7YA1mUwA/2tAo # Export tls certs only $ tctl auth export --type tls","title":"Examples"},{"location":"cli-docs/#tctl-auth-sign","text":"Create an identity file(s) for a given user Usage : tctl auth sign -o <filepath> [--user <user> | --host <host>][--format] [<flags>]","title":"tctl auth sign"},{"location":"cli-docs/#flags_13","text":"Name Default Value(s) Allowed Value(s) Description --user none existing user Teleport user name --host none auth host Teleport host name -o, --out none filepath identity output --format file file , openssh , tls or kubernetes identity format --identity file file identity format --auth-server none auth host & port Remote Teleport host name --ttl none relative duration like 5s, 2m, or 3h TTL (time to live) for the generated certificate --compat \"\" standard or oldssh OpenSSH compatibility flag --proxy \"\" Address of the teleport proxy. When --format is set to \"kubernetes\", this address will be set as cluster address in the generated kubeconfig file","title":"Flags"},{"location":"cli-docs/#global-flags_12","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_17","text":"# Export identity file to teleport_id.pem # for user `teleport` with a ttl set to 5m $ tctl auth sign --format file --ttl = 5m --user teleport -o teleport_id.pem # Export identity formatted for openssh to teleport_id.pem $ tctl auth sign --format openssh --user teleport -o teleport_id.pem # Export host identity, `--format openssh` must be set with `--host` # Generates grav-01 (private key) and grav-01-cert.pub in the current directory $ tctl auth sign --format openssh --host grav-00 # Invalid command, only one of --user or --host should be set $ tctl auth sign --format openssh --host grav-00 --user teleport -o grav_host error: --user or --host must be specified # create a certificate with a TTL of 10 years for the jenkins user # the jenkins.pem file can later be used with `tsh` $ tctl auth sign --ttl = 87600h --user = jenkins --out = jenkins.pem # create a certificate with a TTL of 1 day for the jenkins user # the jenkins.pem file can later be used with `tsh` $ tctl auth sign --ttl = 24h --user = jenkins --out = jenkins.pem # create a certificate with a TTL of 1 day for the jenkins user # The kubeconfig file can later be used with `kubectl` or compatible tooling. $ tctl auth sign --ttl = 24h --user = jenkins --out = kubeconfig --format = kubernetes # Exports an identity from the Auth Server in preparation for remote # tctl execution. $ tctl auth sign --user = admin --out = identity.pem","title":"Examples"},{"location":"cli-docs/#tctl-auth-rotate","text":"Rotate certificate authorities in the cluster Usage : tctl auth rotate [<flags>]","title":"tctl auth rotate"},{"location":"cli-docs/#flags_14","text":"Name Default Value(s) Allowed Value(s) Description --grace-period none relative duration like 5s, 2m, or 3h Grace period keeps previous certificate authorities signatures valid, if set to 0 will force users to login again and nodes to re-register. --manual none none Activate manual rotation, set rotation phases manually --type user,host user or host Certificate authority to rotate --phase init, standby, update_clients, update_servers, rollback Target rotation phase to set, used in manual rotation","title":"Flags"},{"location":"cli-docs/#global-flags_13","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_18","text":"# rotate only user certificates with a grace period of 200 hours: $ tctl auth rotate --type=user --grace-period=200h # rotate only host certificates with a grace period of 8 hours: $ tctl auth rotate --type=host --grace-period=8h","title":"Examples"},{"location":"cli-docs/#tctl-create","text":"Create or update a Teleport resource from a YAML file. The supported resource types are: user, node, cluster, role, connector. See the Resource Guide for complete docs on how to build these yaml files. Usage : tctl create [<flags>] <filename>","title":"tctl create"},{"location":"cli-docs/#arguments_12","text":"<filename> resource definition file","title":"Arguments"},{"location":"cli-docs/#flags_15","text":"Name Default Value(s) Allowed Value(s) Description -f, --force none none Overwrite the resource if already exists","title":"Flags"},{"location":"cli-docs/#global-flags_14","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_19","text":"# Update a user record $ tctl create -f joe.yaml # Add a trusted cluster $ tctl create cluster.yaml $ Update a trusted cluster $ tctl create -f cluster.yaml","title":"Examples"},{"location":"cli-docs/#tctl-rm","text":"Delete a resource Usage : tctl rm [<resource-type/resource-name>]","title":"tctl rm"},{"location":"cli-docs/#arguments_13","text":"[<resource-type/resource-name>] Resource to delete <resource type> Type of a resource [for example: saml,oidc,github,user,cluster,token ] <resource name> Resource name to delete","title":"Arguments"},{"location":"cli-docs/#examples_20","text":"# delete a SAML connector called \"okta\": $ tctl rm saml/okta # delete a local user called \"admin\": $ tctl rm users/admin","title":"Examples"},{"location":"cli-docs/#tctl-get","text":"Print a YAML declaration of various Teleport resources Usage : tctl get [<flags>] [<resource-type/resource-name>],...","title":"tctl get"},{"location":"cli-docs/#arguments_14","text":"[<resource-type/resource-name>] Resource to delete <resource type> Type of a resource [for example: user,cluster,token ] <resource name> Resource name to delete","title":"Arguments"},{"location":"cli-docs/#flags_16","text":"Name Default Value(s) Allowed Value(s) Description --format yaml, json or text Output format --with-secrets none none Include secrets in resources like certificate authorities or OIDC connectors","title":"Flags"},{"location":"cli-docs/#global-flags_15","text":"These flags are available for all commands --debug, --config . Run tctl help <subcommand> or see the Global Flags Section","title":"Global Flags"},{"location":"cli-docs/#examples_21","text":"$ tctl get users # dump the user definition into a file: $ tctl get user/joe > joe.yaml # prints the trusted cluster 'east' $ tctl get cluster/east # prints all trusted clusters and all users $ tctl get clusters,users # Dump all resources for backup into state.yaml $ tctl get all > state.yaml","title":"Examples"},{"location":"cli-docs/#tctl-status","text":"Report cluster status Usage tctl status","title":"tctl status"},{"location":"cli-docs/#examples_22","text":"# Checks status of cluster. $ tctl status Cluster grav-00 User CA never updated Host CA never updated CA pin sha256:1146cdd2b887772dcc2e879232c8f60012a839f7958724ce5744005474b15b9d # Checks remote auth status using exported identity. $ tctl status \\ --auth-server = 192 .168.99.102:3025 \\ --identity = identity.pem","title":"Examples"},{"location":"cli-docs/#tctl-top","text":"Reports diagnostic information. The diagnostic metrics endpoint must be enabled with teleport start --diag-addr=<bind-addr> for tctl top to work. Usage tctl top [<diag-addr>] [<refresh>]","title":"tctl top"},{"location":"cli-docs/#argument","text":"[<diag-addr>] Diagnostic HTTP URL (HTTPS not supported) [<refresh>] Refresh period e.g.5s, 2m, or 3h","title":"Argument"},{"location":"cli-docs/#example_1","text":"$ teleport start --diag-addr=127.0.0.1:3434 # View stats with refresh period of 5 seconds $ tctl top http://127.0.0.1:3434 5s","title":"Example"},{"location":"cli-docs/#tctl-version","text":"Print cluster version Usage: tctl version","title":"tctl version"},{"location":"config-reference/","text":"Teleport Configuration Reference teleport.yaml Teleport uses the YAML file format for configuration. A full configuration reference file is shown below, this provides comments and all available options for teleport.yaml By default, it is stored in /etc/teleport.yaml . # By default, this file should be stored in /etc/teleport.yaml # This section of the configuration file applies to all teleport # services. teleport : # nodename allows to assign an alternative name this node can be reached by. # by default it's equal to hostname nodename : graviton # Data directory where Teleport daemon keeps its data. # See \"Filesystem Layout\" section above for more details. data_dir : /var/lib/teleport # Invitation token used to join a cluster. it is not used on # subsequent starts auth_token : xxxx-token-xxxx # Optional CA pin of the auth server. This enables more secure way of adding new # nodes to a cluster. See \"Adding Nodes\" section above. ca_pin : \"sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1\" # When running in multi-homed or NATed environments Teleport nodes need # to know which IP it will be reachable at by other nodes # # This value can be specified as FQDN e.g. host.example.com advertise_ip : 10.1.0.5 # list of auth servers in a cluster. you will have more than one auth server # if you configure teleport auth to run in HA configuration. # If adding a node located behind NAT, use the Proxy URL. e.g. # auth_servers: # - teleport-proxy.example.com:3080 auth_servers : - 10.1.0.5:3025 - 10.1.0.6:3025 # Teleport throttles all connections to avoid abuse. These settings allow # you to adjust the default limits connection_limits : max_connections : 1000 max_users : 250 # Logging configuration. Possible output values to disk via '/var/lib/teleport/teleport.log', # 'stdout', 'stderr' and 'syslog'. Possible severity values are INFO, WARN # and ERROR (default). log : output : /var/lib/teleport/teleport.log severity : ERROR # Configuration for the storage back-end used for the cluster state and the # audit log. Several back-end types are supported. See \"High Availability\" # section of this Admin Manual below to learn how to configure DynamoDB, # S3, etcd and other highly available back-ends. storage : # By default teleport uses the `data_dir` directory on a local filesystem type : dir # List of locations where the audit log events will be stored. By default, # they are stored in `/var/lib/teleport/log` # When specifying multiple destinations like this, make sure that any highly-available # storage methods (like DynamoDB or Firestore) are specified first, as this is what the # Teleport web UI uses as its source of events to display. audit_events_uri : [ 'dynamodb://events_table_name' , 'firestore://events_table_name' , 'file:///var/lib/teleport/log' , 'stdout://' ] # Use this setting to configure teleport to store the recorded sessions in # an AWS S3 bucket or use GCP Storage with 'gs://'. See \"Using Amazon S3\" # chapter for more information. audit_sessions_uri : 's3://example.com/path/to/bucket?region=us-east-1' # CA Signing algorithm used for OpenSSH Certificates # Defaults to rsa-sha2-512 in 4.3 and above. # valid values are: ssh-rsa, rsa-sha2-256, rsa-sha2-512; ssh-rsa is SHA1 ca_signature_algo : \"rsa-sha2-512\" # Cipher algorithms that the server supports. This section only needs to be # set if you want to override the defaults. ciphers : - aes128-ctr - aes192-ctr - aes256-ctr - aes128-gcm@openssh.com - chacha20-poly1305@openssh.com # Key exchange algorithms that the server supports. This section only needs # to be set if you want to override the defaults. kex_algos : - curve25519-sha256@libssh.org - ecdh-sha2-nistp256 - ecdh-sha2-nistp384 - ecdh-sha2-nistp521 # Message authentication code (MAC) algorithms that the server supports. # This section only needs to be set if you want to override the defaults. mac_algos : - hmac-sha2-256-etm@openssh.com - hmac-sha2-256 # List of the supported ciphersuites. If this section is not specified, # only the default ciphersuites are enabled. ciphersuites : - tls-ecdhe-rsa-with-aes-128-gcm-sha256 - tls-ecdhe-ecdsa-with-aes-128-gcm-sha256 - tls-ecdhe-rsa-with-aes-256-gcm-sha384 - tls-ecdhe-ecdsa-with-aes-256-gcm-sha384 - tls-ecdhe-rsa-with-chacha20-poly1305 - tls-ecdhe-ecdsa-with-chacha20-poly1305 # This section configures the 'auth service': auth_service : # Turns 'auth' role on. Default is 'yes' enabled : yes # A cluster name is used as part of a signature in certificates # generated by this CA. # # We strongly recommend to explicitly set it to something meaningful as it # becomes important when configuring trust between multiple clusters. # # By default an automatically generated name is used (not recommended) # # IMPORTANT: if you change cluster_name, it will invalidate all generated # certificates and keys (may need to wipe out /var/lib/teleport directory) cluster_name : \"main\" authentication : # default authentication type. possible values are 'local' and 'github' for OSS # and 'oidc', 'saml' and 'false' for Enterprise. # 'false' is required for FedRAMP / FIPS, see # https://gravitational.com/teleport/docs/enterprise/ssh-kubernetes-fedramp/ # only local authentication (Teleport's own user DB) & Github is supported in the open # source version type : local # second_factor can be off, otp, or u2f second_factor : otp # this section is used if second_factor is set to 'u2f' u2f : # app_id must point to the URL of the Teleport Web UI (proxy) accessible # by the end users app_id : https://localhost:3080 # facets must list all proxy servers if there are more than one deployed facets : - https://localhost:3080 # IP and the port to bind to. Other Teleport nodes will be connecting to # this port (AKA \"Auth API\" or \"Cluster API\") to validate client # certificates listen_addr : 0.0.0.0:3025 # The optional DNS name the auth server if located behind a load balancer. # (see public_addr section below) public_addr : auth.example.com:3025 # Pre-defined tokens for adding new nodes to a cluster. Each token specifies # the role a new node will be allowed to assume. The more secure way to # add nodes is to use `ttl node add --ttl` command to generate auto-expiring # tokens. # # We recommend to use tools like `pwgen` to generate sufficiently random # tokens of 32+ byte length. tokens : - \"proxy,node:xxxxx\" - \"auth:yyyy\" # Optional setting for configuring session recording. Possible values are: # \"node\" : sessions will be recorded on the node level (the default) # \"proxy\" : recording on the proxy level, see \"recording proxy mode\" section. # \"off\" : session recording is turned off # # EXPERIMENTAL *-sync modes # Proxy and node send logs directly to S3 or other # storage without storing the records on disk at all. *-sync requires all # nodes to be upgraded to 4.4 # # \"node-sync\" : sessions recording will be streamed from node -> auth -> storage service # \"proxy-sync : sessions recording will be streamed from proxy -> auth -> storage service # session_recording : \"node\" # This setting determines if a Teleport proxy performs strict host key checks. # Only applicable if session_recording=proxy, see \"recording proxy mode\" for details. proxy_checks_host_keys : yes # Determines if SSH sessions to cluster nodes are forcefully terminated # after no activity from a client (idle client). # Examples: \"30m\", \"1h\" or \"1h30m\" client_idle_timeout : never # Determines if the clients will be forcefully disconnected when their # certificates expire in the middle of an active SSH session. (default is 'no') disconnect_expired_cert : no # Determines the interval at which Teleport will send keep-alive messages. The default # is set to 5 minutes (300 seconds) to stay lower than the common load balancer timeout # of 350 seconds. # keep_alive_count_max is the number of missed keep-alive messages before the server # tears down the connection to the client. keep_alive_interval : 5m keep_alive_count_max : 3 # Determines the internal session control timeout cluster wide. This value will # be used with enterprise max_connections and max_sessions. It's unlikely that # you'll need to change this. # session_control_timeout: 2m # License file to start auth server with. Note that this setting is ignored # in open-source Teleport and is required only for Teleport Pro, Business # and Enterprise subscription plans. # # The path can be either absolute or relative to the configured `data_dir` # and should point to the license file obtained from Teleport Download Portal. # # If not set, by default Teleport will look for the `license.pem` file in # the configured `data_dir` . license_file : /var/lib/teleport/license.pem # This section configures the 'node service': ssh_service : # Turns 'ssh' role on. Default is 'yes' enabled : yes # IP and the port for SSH service to bind to. listen_addr : 0.0.0.0:3022 # The optional public address the SSH service. This is useful if administrators # want to allow users to connect to nodes directly, bypassing a Teleport proxy # (see public_addr section below) public_addr : node.example.com:3022 # See explanation of labels in \"Labeling Nodes\" section below labels : role : leader type : postgres # List of the commands to periodically execute. Their output will be used as node labels. # See \"Labeling Nodes\" section below for more information and more examples. commands : # this command will add a label 'arch=x86_64' to a node - name : arch command : [ '/bin/uname' , '-p' ] period : 1h0m0s # enables reading ~/.tsh/environment before creating a session. by default # set to false, can be set true here or as a command line flag. permit_user_env : false # Enhanced Session Recording # see https://gravitational.com/teleport/docs/features/enhanced-session-recording enhanced_recording : # Enable or disable enhanced auditing for this node. Default value: # false. enabled : false # command_buffer_size is optional with a default value of 8 pages. command_buffer_size : 8 # disk_buffer_size is optional with default value of 128 pages. disk_buffer_size : 128 # network_buffer_size is optional with default value of 8 pages. network_buffer_size : 8 # Controls where cgroupv2 hierarchy is mounted. Default value: # /cgroup2. cgroup_path : /cgroup2 # configures PAM integration. see below for more details. pam : enabled : no service_name : teleport # This section configures the 'proxy service' proxy_service : # Turns 'proxy' role on. Default is 'yes' enabled : yes # SSH forwarding/proxy address. Command line (CLI) clients always begin their # SSH sessions by connecting to this port listen_addr : 0.0.0.0:3023 # Reverse tunnel listening address. An auth server (CA) can establish an # outbound (from behind the firewall) connection to this address. # This will allow users of the outside CA to connect to behind-the-firewall # nodes. tunnel_listen_addr : 0.0.0.0:3024 # The HTTPS listen address to serve the Web UI and also to authenticate the # command line (CLI) users via password+HOTP web_listen_addr : 0.0.0.0:3080 # The DNS name of the proxy HTTPS endpoint as accessible by cluster users. # Defaults to the proxy's hostname if not specified. If running multiple # proxies behind a load balancer, this name must point to the load balancer # (see public_addr section below) public_addr : proxy.example.com:3080 # The DNS name of the proxy SSH endpoint as accessible by cluster clients. # Defaults to the proxy's hostname if not specified. If running multiple proxies # behind a load balancer, this name must point to the load balancer. # Use a TCP load balancer because this port uses SSH protocol. ssh_public_addr : proxy.example.com:3023 # The DNS name of the tunnel SSH endpoint as accessible by trusted clusters and # nodes joining the cluster via Teleport IoT/node tunneling. # Defaults to the proxy's hostname if not specified. If running multiple proxies # behind a load balancer, this name must point to the load balancer. # Use a TCP load balancer because this port uses SSH protocol. tunnel_public_addr : proxy.example.com:3024 # TLS certificate for the HTTPS connection. Configuring these properly is # critical for Teleport security. https_key_file : /var/lib/teleport/webproxy_key.pem https_cert_file : /var/lib/teleport/webproxy_cert.pem # This section configures the Kubernetes proxy service kubernetes : # Turns 'kubernetes' proxy on. Default is 'no' enabled : yes # Kubernetes proxy listen address. listen_addr : 0.0.0.0:3026 # The DNS name of the Kubernetes proxy server that is accessible by cluster clients. # If running multiple proxies behind a load balancer, this name must point to the # load balancer. public_addr : [ 'kube.example.com:3026' ] # This setting is not required if the Teleport proxy service is # deployed inside a Kubernetes cluster. Otherwise, Teleport proxy # will use the credentials from this file: kubeconfig_file : /path/to/kube/config","title":"YAML"},{"location":"config-reference/#teleport-configuration-reference","text":"","title":"Teleport Configuration Reference"},{"location":"config-reference/#teleportyaml","text":"Teleport uses the YAML file format for configuration. A full configuration reference file is shown below, this provides comments and all available options for teleport.yaml By default, it is stored in /etc/teleport.yaml . # By default, this file should be stored in /etc/teleport.yaml # This section of the configuration file applies to all teleport # services. teleport : # nodename allows to assign an alternative name this node can be reached by. # by default it's equal to hostname nodename : graviton # Data directory where Teleport daemon keeps its data. # See \"Filesystem Layout\" section above for more details. data_dir : /var/lib/teleport # Invitation token used to join a cluster. it is not used on # subsequent starts auth_token : xxxx-token-xxxx # Optional CA pin of the auth server. This enables more secure way of adding new # nodes to a cluster. See \"Adding Nodes\" section above. ca_pin : \"sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1\" # When running in multi-homed or NATed environments Teleport nodes need # to know which IP it will be reachable at by other nodes # # This value can be specified as FQDN e.g. host.example.com advertise_ip : 10.1.0.5 # list of auth servers in a cluster. you will have more than one auth server # if you configure teleport auth to run in HA configuration. # If adding a node located behind NAT, use the Proxy URL. e.g. # auth_servers: # - teleport-proxy.example.com:3080 auth_servers : - 10.1.0.5:3025 - 10.1.0.6:3025 # Teleport throttles all connections to avoid abuse. These settings allow # you to adjust the default limits connection_limits : max_connections : 1000 max_users : 250 # Logging configuration. Possible output values to disk via '/var/lib/teleport/teleport.log', # 'stdout', 'stderr' and 'syslog'. Possible severity values are INFO, WARN # and ERROR (default). log : output : /var/lib/teleport/teleport.log severity : ERROR # Configuration for the storage back-end used for the cluster state and the # audit log. Several back-end types are supported. See \"High Availability\" # section of this Admin Manual below to learn how to configure DynamoDB, # S3, etcd and other highly available back-ends. storage : # By default teleport uses the `data_dir` directory on a local filesystem type : dir # List of locations where the audit log events will be stored. By default, # they are stored in `/var/lib/teleport/log` # When specifying multiple destinations like this, make sure that any highly-available # storage methods (like DynamoDB or Firestore) are specified first, as this is what the # Teleport web UI uses as its source of events to display. audit_events_uri : [ 'dynamodb://events_table_name' , 'firestore://events_table_name' , 'file:///var/lib/teleport/log' , 'stdout://' ] # Use this setting to configure teleport to store the recorded sessions in # an AWS S3 bucket or use GCP Storage with 'gs://'. See \"Using Amazon S3\" # chapter for more information. audit_sessions_uri : 's3://example.com/path/to/bucket?region=us-east-1' # CA Signing algorithm used for OpenSSH Certificates # Defaults to rsa-sha2-512 in 4.3 and above. # valid values are: ssh-rsa, rsa-sha2-256, rsa-sha2-512; ssh-rsa is SHA1 ca_signature_algo : \"rsa-sha2-512\" # Cipher algorithms that the server supports. This section only needs to be # set if you want to override the defaults. ciphers : - aes128-ctr - aes192-ctr - aes256-ctr - aes128-gcm@openssh.com - chacha20-poly1305@openssh.com # Key exchange algorithms that the server supports. This section only needs # to be set if you want to override the defaults. kex_algos : - curve25519-sha256@libssh.org - ecdh-sha2-nistp256 - ecdh-sha2-nistp384 - ecdh-sha2-nistp521 # Message authentication code (MAC) algorithms that the server supports. # This section only needs to be set if you want to override the defaults. mac_algos : - hmac-sha2-256-etm@openssh.com - hmac-sha2-256 # List of the supported ciphersuites. If this section is not specified, # only the default ciphersuites are enabled. ciphersuites : - tls-ecdhe-rsa-with-aes-128-gcm-sha256 - tls-ecdhe-ecdsa-with-aes-128-gcm-sha256 - tls-ecdhe-rsa-with-aes-256-gcm-sha384 - tls-ecdhe-ecdsa-with-aes-256-gcm-sha384 - tls-ecdhe-rsa-with-chacha20-poly1305 - tls-ecdhe-ecdsa-with-chacha20-poly1305 # This section configures the 'auth service': auth_service : # Turns 'auth' role on. Default is 'yes' enabled : yes # A cluster name is used as part of a signature in certificates # generated by this CA. # # We strongly recommend to explicitly set it to something meaningful as it # becomes important when configuring trust between multiple clusters. # # By default an automatically generated name is used (not recommended) # # IMPORTANT: if you change cluster_name, it will invalidate all generated # certificates and keys (may need to wipe out /var/lib/teleport directory) cluster_name : \"main\" authentication : # default authentication type. possible values are 'local' and 'github' for OSS # and 'oidc', 'saml' and 'false' for Enterprise. # 'false' is required for FedRAMP / FIPS, see # https://gravitational.com/teleport/docs/enterprise/ssh-kubernetes-fedramp/ # only local authentication (Teleport's own user DB) & Github is supported in the open # source version type : local # second_factor can be off, otp, or u2f second_factor : otp # this section is used if second_factor is set to 'u2f' u2f : # app_id must point to the URL of the Teleport Web UI (proxy) accessible # by the end users app_id : https://localhost:3080 # facets must list all proxy servers if there are more than one deployed facets : - https://localhost:3080 # IP and the port to bind to. Other Teleport nodes will be connecting to # this port (AKA \"Auth API\" or \"Cluster API\") to validate client # certificates listen_addr : 0.0.0.0:3025 # The optional DNS name the auth server if located behind a load balancer. # (see public_addr section below) public_addr : auth.example.com:3025 # Pre-defined tokens for adding new nodes to a cluster. Each token specifies # the role a new node will be allowed to assume. The more secure way to # add nodes is to use `ttl node add --ttl` command to generate auto-expiring # tokens. # # We recommend to use tools like `pwgen` to generate sufficiently random # tokens of 32+ byte length. tokens : - \"proxy,node:xxxxx\" - \"auth:yyyy\" # Optional setting for configuring session recording. Possible values are: # \"node\" : sessions will be recorded on the node level (the default) # \"proxy\" : recording on the proxy level, see \"recording proxy mode\" section. # \"off\" : session recording is turned off # # EXPERIMENTAL *-sync modes # Proxy and node send logs directly to S3 or other # storage without storing the records on disk at all. *-sync requires all # nodes to be upgraded to 4.4 # # \"node-sync\" : sessions recording will be streamed from node -> auth -> storage service # \"proxy-sync : sessions recording will be streamed from proxy -> auth -> storage service # session_recording : \"node\" # This setting determines if a Teleport proxy performs strict host key checks. # Only applicable if session_recording=proxy, see \"recording proxy mode\" for details. proxy_checks_host_keys : yes # Determines if SSH sessions to cluster nodes are forcefully terminated # after no activity from a client (idle client). # Examples: \"30m\", \"1h\" or \"1h30m\" client_idle_timeout : never # Determines if the clients will be forcefully disconnected when their # certificates expire in the middle of an active SSH session. (default is 'no') disconnect_expired_cert : no # Determines the interval at which Teleport will send keep-alive messages. The default # is set to 5 minutes (300 seconds) to stay lower than the common load balancer timeout # of 350 seconds. # keep_alive_count_max is the number of missed keep-alive messages before the server # tears down the connection to the client. keep_alive_interval : 5m keep_alive_count_max : 3 # Determines the internal session control timeout cluster wide. This value will # be used with enterprise max_connections and max_sessions. It's unlikely that # you'll need to change this. # session_control_timeout: 2m # License file to start auth server with. Note that this setting is ignored # in open-source Teleport and is required only for Teleport Pro, Business # and Enterprise subscription plans. # # The path can be either absolute or relative to the configured `data_dir` # and should point to the license file obtained from Teleport Download Portal. # # If not set, by default Teleport will look for the `license.pem` file in # the configured `data_dir` . license_file : /var/lib/teleport/license.pem # This section configures the 'node service': ssh_service : # Turns 'ssh' role on. Default is 'yes' enabled : yes # IP and the port for SSH service to bind to. listen_addr : 0.0.0.0:3022 # The optional public address the SSH service. This is useful if administrators # want to allow users to connect to nodes directly, bypassing a Teleport proxy # (see public_addr section below) public_addr : node.example.com:3022 # See explanation of labels in \"Labeling Nodes\" section below labels : role : leader type : postgres # List of the commands to periodically execute. Their output will be used as node labels. # See \"Labeling Nodes\" section below for more information and more examples. commands : # this command will add a label 'arch=x86_64' to a node - name : arch command : [ '/bin/uname' , '-p' ] period : 1h0m0s # enables reading ~/.tsh/environment before creating a session. by default # set to false, can be set true here or as a command line flag. permit_user_env : false # Enhanced Session Recording # see https://gravitational.com/teleport/docs/features/enhanced-session-recording enhanced_recording : # Enable or disable enhanced auditing for this node. Default value: # false. enabled : false # command_buffer_size is optional with a default value of 8 pages. command_buffer_size : 8 # disk_buffer_size is optional with default value of 128 pages. disk_buffer_size : 128 # network_buffer_size is optional with default value of 8 pages. network_buffer_size : 8 # Controls where cgroupv2 hierarchy is mounted. Default value: # /cgroup2. cgroup_path : /cgroup2 # configures PAM integration. see below for more details. pam : enabled : no service_name : teleport # This section configures the 'proxy service' proxy_service : # Turns 'proxy' role on. Default is 'yes' enabled : yes # SSH forwarding/proxy address. Command line (CLI) clients always begin their # SSH sessions by connecting to this port listen_addr : 0.0.0.0:3023 # Reverse tunnel listening address. An auth server (CA) can establish an # outbound (from behind the firewall) connection to this address. # This will allow users of the outside CA to connect to behind-the-firewall # nodes. tunnel_listen_addr : 0.0.0.0:3024 # The HTTPS listen address to serve the Web UI and also to authenticate the # command line (CLI) users via password+HOTP web_listen_addr : 0.0.0.0:3080 # The DNS name of the proxy HTTPS endpoint as accessible by cluster users. # Defaults to the proxy's hostname if not specified. If running multiple # proxies behind a load balancer, this name must point to the load balancer # (see public_addr section below) public_addr : proxy.example.com:3080 # The DNS name of the proxy SSH endpoint as accessible by cluster clients. # Defaults to the proxy's hostname if not specified. If running multiple proxies # behind a load balancer, this name must point to the load balancer. # Use a TCP load balancer because this port uses SSH protocol. ssh_public_addr : proxy.example.com:3023 # The DNS name of the tunnel SSH endpoint as accessible by trusted clusters and # nodes joining the cluster via Teleport IoT/node tunneling. # Defaults to the proxy's hostname if not specified. If running multiple proxies # behind a load balancer, this name must point to the load balancer. # Use a TCP load balancer because this port uses SSH protocol. tunnel_public_addr : proxy.example.com:3024 # TLS certificate for the HTTPS connection. Configuring these properly is # critical for Teleport security. https_key_file : /var/lib/teleport/webproxy_key.pem https_cert_file : /var/lib/teleport/webproxy_cert.pem # This section configures the Kubernetes proxy service kubernetes : # Turns 'kubernetes' proxy on. Default is 'no' enabled : yes # Kubernetes proxy listen address. listen_addr : 0.0.0.0:3026 # The DNS name of the Kubernetes proxy server that is accessible by cluster clients. # If running multiple proxies behind a load balancer, this name must point to the # load balancer. public_addr : [ 'kube.example.com:3026' ] # This setting is not required if the Teleport proxy service is # deployed inside a Kubernetes cluster. Otherwise, Teleport proxy # will use the credentials from this file: kubeconfig_file : /path/to/kube/config","title":"teleport.yaml"},{"location":"faq/","text":"FAQ Community FAQ Can I use Teleport in production today? Teleport has been deployed on server clusters with thousands of nodes at Fortune 500 companies. It has been through several security audits from nationally recognized technology security companies, so we are comfortable with the stability of Teleport from a security perspective. Can Teleport be deployed in agentless mode? Yes. Teleport can be deployed with a tiny footprint as an authentication gateway/proxy and you can keep your existing SSH servers on the nodes. But some innovating Teleport features, such as cluster introspection, will not be available unless the Teleport SSH daemon is present on all cluster nodes. Can I use OpenSSH with a Teleport cluster? Yes, this question comes up often and is related to the previous one. Take a look at Using OpenSSH client section in the User Manual and Using OpenSSH servers in the Admin Manual. Can I connect to nodes behind a firewall? Yes, Teleport supports reverse SSH tunnels out of the box. To configure behind-firewall clusters refer to Trusted Clusters section of the Admin Manual. Can individual nodes create reverse tunnels to a proxy server without creating a new cluster? This has been a long standing request of Teleport and it has been fixed with Teleport 4.0. Once you've upgraded your Teleport Cluster, change the node config option --auth-server to point to web proxy address (this would be public_addr and web_listen_addr in file configuration). As defined in Adding a node located behind NAT - Teleport Node Tunneling Can nodes use a single port for reverse tunnels? Yes, Teleport supports tunnel multiplexing on a single port. Set the tunnel_listen_addr to use the same port as the web_listen_addr address setting in the proxy_service configuration. Teleport will automatically use multiplexing with that configuration. What are Teleport's scalability and hardware recommendations? We recommend setting up Teleport with a High Availability configuration . Below is our recommended hardware for the Proxy and Auth server. If you plan to connect more than 10,000 nodes, please get in touch and we can help architect the best solution for you. Scenario Max Recommended Count Proxy Auth server Teleport nodes connected to auth server 10,000 2x 2-4 vCPUs/8GB RAM 2x 4-8 vCPUs/16GB RAM Teleport nodes connected to proxy server (IoT) 2,000* 2x 2-4 vCPUs/8GB RAM 2x 4-8 vCPUs/16+GB RAM Which version of Teleport is supported? Release Long Term Support Release Date Supported Until Min tsh version 4.4 Yes October 20th, 2020 October 20th, 2021 3.x 4.3 Yes July 8th, 2020 July 8th, 2021 3.x 4.2 Yes December 19th, 2019 December 19th, 2020 3.x 4.1 (EOL) Yes October 1st, 2019 October 1st, 2020 3.x 4.0 (EOL) Yes June 18th, 2019 June 18th, 2020 3.x Teleport uses semantic versioning to release updates of software, with releases taking around 3-4 months, for MINOR releases. The team has these informal rules about releasing software: Teleport Team will do its best to allow for a non-breaking upgrade even with MAJOR releases. We aim to support versions for 3 releases (current and two back) or around 9 months. This means critical security fixes will be backported to earlier versions. We provide Enterprise customers edge case support for older versions of Operating Systems. This might be a specific patch for an OSS in maintenance. This is provided on a somewhat ad-hoc basis. We won\u2019t support Teleport on an OS Distro that has stopped receiving maintenance updates and is EOL. How should I upgrade my cluster? Please follow our standard guidelines for upgrading . We recommend that the Auth Server should be upgraded first, and proxy is bumped after. Does Web UI support copy and paste? Yes. You can copy&paste using the mouse. For working with a keyboard, Teleport employs tmux -like \"prefix\" mode. To enter prefix mode, press Ctrl+A . While in prefix mode, you can press Ctrl+V to paste, or enter text selection mode by pressing [ . When in text selection mode, move around using hjkl , select text by toggling space and copy it via Ctrl+C . What TCP ports does Teleport use? Please refer to the Ports section of the Admin Manual. Does Teleport support authentication via OAuth, SAML or Active Directory? Gravitational offers this feature for the Enterprise versions of Teleport . Commercial Teleport Editions What is included in the commercial version, Teleport Enterprise? The Teleport Enterprise offering gives users the following additional features: Role-based access control, also known as RBAC . Authentication via SAML and OpenID with providers like Okta, Active Directory, Auth0, etc. SSO . Premium support. We also offer implementation services, to help you integrate Teleport with your existing systems and processes. You can read more in the Teleport Enterprise section of the docs Does Teleport send any data to Gravitational? The open source edition of Teleport does not send any information to Gravitational and can be used on servers without internet access. The commercial versions of Teleport may or may not be configured to send anonymized information to Gravitational, depending on the license purchased. This information contains the following: Anonymized user ID: SHA256 hash of a username with a randomly generated prefix. Anonymized server ID: SHA256 hash of a server IP with a randomly generated prefix. This allows Teleport Pro to print a warning if users are exceeding the usage limits of their license. The reporting library code is on Github . Reach out to sales@gravitational.com if you have questions about the commercial edition of Teleport.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#community-faq","text":"","title":"Community FAQ"},{"location":"faq/#can-i-use-teleport-in-production-today","text":"Teleport has been deployed on server clusters with thousands of nodes at Fortune 500 companies. It has been through several security audits from nationally recognized technology security companies, so we are comfortable with the stability of Teleport from a security perspective.","title":"Can I use Teleport in production today?"},{"location":"faq/#can-teleport-be-deployed-in-agentless-mode","text":"Yes. Teleport can be deployed with a tiny footprint as an authentication gateway/proxy and you can keep your existing SSH servers on the nodes. But some innovating Teleport features, such as cluster introspection, will not be available unless the Teleport SSH daemon is present on all cluster nodes.","title":"Can Teleport be deployed in agentless mode?"},{"location":"faq/#can-i-use-openssh-with-a-teleport-cluster","text":"Yes, this question comes up often and is related to the previous one. Take a look at Using OpenSSH client section in the User Manual and Using OpenSSH servers in the Admin Manual.","title":"Can I use OpenSSH with a Teleport cluster?"},{"location":"faq/#can-i-connect-to-nodes-behind-a-firewall","text":"Yes, Teleport supports reverse SSH tunnels out of the box. To configure behind-firewall clusters refer to Trusted Clusters section of the Admin Manual.","title":"Can I connect to nodes behind a firewall?"},{"location":"faq/#can-individual-nodes-create-reverse-tunnels-to-a-proxy-server-without-creating-a-new-cluster","text":"This has been a long standing request of Teleport and it has been fixed with Teleport 4.0. Once you've upgraded your Teleport Cluster, change the node config option --auth-server to point to web proxy address (this would be public_addr and web_listen_addr in file configuration). As defined in Adding a node located behind NAT - Teleport Node Tunneling","title":"Can individual nodes create reverse tunnels to a proxy server without creating a new cluster?"},{"location":"faq/#can-nodes-use-a-single-port-for-reverse-tunnels","text":"Yes, Teleport supports tunnel multiplexing on a single port. Set the tunnel_listen_addr to use the same port as the web_listen_addr address setting in the proxy_service configuration. Teleport will automatically use multiplexing with that configuration.","title":"Can nodes use a single port for reverse tunnels?"},{"location":"faq/#what-are-teleports-scalability-and-hardware-recommendations","text":"We recommend setting up Teleport with a High Availability configuration . Below is our recommended hardware for the Proxy and Auth server. If you plan to connect more than 10,000 nodes, please get in touch and we can help architect the best solution for you. Scenario Max Recommended Count Proxy Auth server Teleport nodes connected to auth server 10,000 2x 2-4 vCPUs/8GB RAM 2x 4-8 vCPUs/16GB RAM Teleport nodes connected to proxy server (IoT) 2,000* 2x 2-4 vCPUs/8GB RAM 2x 4-8 vCPUs/16+GB RAM","title":"What are Teleport's scalability and hardware recommendations?"},{"location":"faq/#which-version-of-teleport-is-supported","text":"Release Long Term Support Release Date Supported Until Min tsh version 4.4 Yes October 20th, 2020 October 20th, 2021 3.x 4.3 Yes July 8th, 2020 July 8th, 2021 3.x 4.2 Yes December 19th, 2019 December 19th, 2020 3.x 4.1 (EOL) Yes October 1st, 2019 October 1st, 2020 3.x 4.0 (EOL) Yes June 18th, 2019 June 18th, 2020 3.x Teleport uses semantic versioning to release updates of software, with releases taking around 3-4 months, for MINOR releases. The team has these informal rules about releasing software: Teleport Team will do its best to allow for a non-breaking upgrade even with MAJOR releases. We aim to support versions for 3 releases (current and two back) or around 9 months. This means critical security fixes will be backported to earlier versions. We provide Enterprise customers edge case support for older versions of Operating Systems. This might be a specific patch for an OSS in maintenance. This is provided on a somewhat ad-hoc basis. We won\u2019t support Teleport on an OS Distro that has stopped receiving maintenance updates and is EOL. How should I upgrade my cluster? Please follow our standard guidelines for upgrading . We recommend that the Auth Server should be upgraded first, and proxy is bumped after.","title":"Which version of Teleport is supported?"},{"location":"faq/#does-web-ui-support-copy-and-paste","text":"Yes. You can copy&paste using the mouse. For working with a keyboard, Teleport employs tmux -like \"prefix\" mode. To enter prefix mode, press Ctrl+A . While in prefix mode, you can press Ctrl+V to paste, or enter text selection mode by pressing [ . When in text selection mode, move around using hjkl , select text by toggling space and copy it via Ctrl+C .","title":"Does Web UI support copy and paste?"},{"location":"faq/#what-tcp-ports-does-teleport-use","text":"Please refer to the Ports section of the Admin Manual.","title":"What TCP ports does Teleport use?"},{"location":"faq/#does-teleport-support-authentication-via-oauth-saml-or-active-directory","text":"Gravitational offers this feature for the Enterprise versions of Teleport .","title":"Does Teleport support authentication via OAuth, SAML or Active Directory?"},{"location":"faq/#commercial-teleport-editions","text":"","title":"Commercial Teleport Editions"},{"location":"faq/#what-is-included-in-the-commercial-version-teleport-enterprise","text":"The Teleport Enterprise offering gives users the following additional features: Role-based access control, also known as RBAC . Authentication via SAML and OpenID with providers like Okta, Active Directory, Auth0, etc. SSO . Premium support. We also offer implementation services, to help you integrate Teleport with your existing systems and processes. You can read more in the Teleport Enterprise section of the docs","title":"What is included in the commercial version, Teleport Enterprise?"},{"location":"faq/#does-teleport-send-any-data-to-gravitational","text":"The open source edition of Teleport does not send any information to Gravitational and can be used on servers without internet access. The commercial versions of Teleport may or may not be configured to send anonymized information to Gravitational, depending on the license purchased. This information contains the following: Anonymized user ID: SHA256 hash of a username with a randomly generated prefix. Anonymized server ID: SHA256 hash of a server IP with a randomly generated prefix. This allows Teleport Pro to print a warning if users are exceeding the usage limits of their license. The reporting library code is on Github . Reach out to sales@gravitational.com if you have questions about the commercial edition of Teleport.","title":"Does Teleport send any data to Gravitational?"},{"location":"gcp-guide/","text":"Running Teleport on GCP We've created this guide to give customers a high level overview of how to use Teleport on Google Cloud (GCP). This guide provides a high level introduction leading to a deep dive into how to setup and run Teleport in production. We have split this guide into: Teleport on GCP FAQ GCP Teleport Introduction GCP Quickstart Teleport on GCP FAQ Why would you want to use Teleport with GCP? As leader in BeyondCorp , GCP already provides some great tools out of the box such as Cloud Identity-Aware Proxy . This is an excellent tool to quickly get setup securely with GCP but it can become complicated to integrate into existing workflows and complicated if you want to share tool across clouds. Which Services can I use Teleport with? You can use Teleport for all the services that you would SSH into. This guide is focused on Google Compute Engine. In the future we'll plan to update on how to use Teleport with Google Kubernetes Engine (GKE). GCP Teleport Introduction This guide will cover how to setup, configure and run Teleport on GCP. GCP Services required to run Teleport in HA: Compute Engine: VM Instances with Instance Groups Computer Engine: Health Checks Storage: Cloud Firestore Storage: Google Cloud Storage Network Services: Load Balancing Network Services: Cloud DNS Other things needed: SSL Certificate Optional: Management Tools: Cloud Deployment Manager Stackdriver Logging We recommend setting up Teleport in high availability mode (HA). In HA mode Firestore stores the state of the system and Google Cloud Storage stores the audit logs. Compute Engine: VM Instances with Instance Groups To run Teleport in a HA configuration we recommend using n1-standard-2 instances in Production. It's best practice to separate the proxy and authentication server, using Instance groups for the proxy and auth server. Computer Engine: Health Checks GCP relies heavily on Health Checks , this is helpful when adding new instances to an instance group. To enable health checks in Teleport start with teleport start --diag-addr=0.0.0.0:3000 see Admin Guide: Troubleshooting for more information. Storage: Cloud Firestore Cloud Firestore This storage backend uses real-time updates to keep individual auth instances in sync and requires Firestore configured in native mode. Add this storage configuration in teleport section of the config file (by default it's /etc/teleport.yaml ): teleport : storage : type : firestore collection_name : cluster-data project_id : EXAMPLE_gcp-proj-with-firestore-enabled credentials_path : /var/lib/teleport/firestore_creds.json audit_events_uri : [ 'firestore://events_table_name' ] Storage: Google Cloud Storage When creating the Bucket we would recommend setting it up as Dual-region and with Standard storage class. Provide access using a Uniform access control with a Google-managed key. When setting up audit_session_uri use gs:// session prefix. storage : ... audit_sessions_uri : 'gs://teleport-session-storage-2?credentialsPath=/var/lib/teleport/gcs_creds.json&projectID=EXAMPLE_gcp-proj-with-firestore-enabled' ... Network Services: Load Balancing Load Balancing is required for Proxy and SSH traffic. Use TCP Load Balancing as Teleport requires custom ports for SSH and Web Traffic. Network Services: Cloud DNS Cloud DNS is used to setup the public URL of the Teleport Proxy. Once setup an A record is sufficient. Access: Service accounts The Authentication server will need to read and write to Firestore. For this it'll need the correct permission via Server Accounts. Learn how to enable and create service accounts for instances . Download JSON Service Key GCP Quickstart 1. Create Resources We recommend starting by creating the resources. We highly recommend creating these an infrastructure automation tool such as Cloud Deployment Manager or Terraform. 2. Install & Configure Teleport Follow install instructions from our installation page . We recommend configuring Teleport as per the below steps: 1. Configure Teleport Auth Server using the below example teleport.yaml , and start it using systemd or use DEB/.RPM packages available from our Downloads Page . # # Sample Teleport configuration teleport.yaml file for Auth Server # teleport : nodename : teleport-auth-server data_dir : /var/lib/teleport pid_file : /run/teleport.pid auth_token : EXAMPLE-CLUSTER-JOIN-TOKEN auth_servers : - 0.0.0.0:3025 connection_limits : max_connections : 15000 max_users : 250 log : output : stderr severity : DEBUG storage : type : firestore collection_name : cluster-data # Credentials: Path to google service account file, used for Firestore and Google Storage. credentials_path : /var/lib/teleport/gcs_creds.json project_id : example_Teleport-Project-Name audit_events_uri : 'firestore://events?projectID=example_Teleport-Project-Name&credentialsPath=/var/lib/teleport/gcs_creds' audit_sessions_uri : 'gs://teleport-session-storage-2?credentialsPath=/var/lib/teleport/gcs_creds.json&projectID=example_Teleport-Project-Name' auth_service : enabled : yes auth_service : tokens : - \"proxy,node:EXAMPLE-CLUSTER-JOIN-TOKEN\" 2. Setup Proxy Save the following configuration file as /etc/teleport.yaml on the Proxy Server: teleport : auth_token : EXAMPLE-CLUSTER-JOIN-TOKEN # We recommend using a TCP load balancer pointed to the auth servers when # setting up in HA mode. auth_servers : [ \"auth.example.com:3025\" ] # enable ssh service and disable auth and proxy: ssh_service : enabled : true auth_service : enabled : false proxy_service : enabled : false 3. Setup Teleport Nodes Save the following configuration file as /etc/teleport.yaml on the node: teleport : auth_token : EXAMPLE-CLUSTER-JOIN-TOKEN # We recommend using a TCP load balancer pointed to the auth servers when # setting up in HA mode. auth_servers : [ \"auth.example.com:3025\" ] # enable ssh service and disable auth and proxy: ssh_service : enabled : true auth_service : enabled : false proxy_service : enabled : false 4. Add Users Follow adding users or integrate with G Suite to provide SSO access.","title":"GCP"},{"location":"gcp-guide/#running-teleport-on-gcp","text":"We've created this guide to give customers a high level overview of how to use Teleport on Google Cloud (GCP). This guide provides a high level introduction leading to a deep dive into how to setup and run Teleport in production. We have split this guide into: Teleport on GCP FAQ GCP Teleport Introduction GCP Quickstart","title":"Running Teleport on GCP"},{"location":"gcp-guide/#teleport-on-gcp-faq","text":"","title":"Teleport on GCP FAQ"},{"location":"gcp-guide/#gcp-teleport-introduction","text":"This guide will cover how to setup, configure and run Teleport on GCP. GCP Services required to run Teleport in HA: Compute Engine: VM Instances with Instance Groups Computer Engine: Health Checks Storage: Cloud Firestore Storage: Google Cloud Storage Network Services: Load Balancing Network Services: Cloud DNS Other things needed: SSL Certificate Optional: Management Tools: Cloud Deployment Manager Stackdriver Logging We recommend setting up Teleport in high availability mode (HA). In HA mode Firestore stores the state of the system and Google Cloud Storage stores the audit logs.","title":"GCP Teleport Introduction"},{"location":"gcp-guide/#compute-engine-vm-instances-with-instance-groups","text":"To run Teleport in a HA configuration we recommend using n1-standard-2 instances in Production. It's best practice to separate the proxy and authentication server, using Instance groups for the proxy and auth server.","title":"Compute Engine: VM Instances with Instance Groups"},{"location":"gcp-guide/#computer-engine-health-checks","text":"GCP relies heavily on Health Checks , this is helpful when adding new instances to an instance group. To enable health checks in Teleport start with teleport start --diag-addr=0.0.0.0:3000 see Admin Guide: Troubleshooting for more information.","title":"Computer Engine: Health Checks"},{"location":"gcp-guide/#storage-cloud-firestore","text":"Cloud Firestore This storage backend uses real-time updates to keep individual auth instances in sync and requires Firestore configured in native mode. Add this storage configuration in teleport section of the config file (by default it's /etc/teleport.yaml ): teleport : storage : type : firestore collection_name : cluster-data project_id : EXAMPLE_gcp-proj-with-firestore-enabled credentials_path : /var/lib/teleport/firestore_creds.json audit_events_uri : [ 'firestore://events_table_name' ]","title":"Storage: Cloud Firestore"},{"location":"gcp-guide/#storage-google-cloud-storage","text":"When creating the Bucket we would recommend setting it up as Dual-region and with Standard storage class. Provide access using a Uniform access control with a Google-managed key. When setting up audit_session_uri use gs:// session prefix. storage : ... audit_sessions_uri : 'gs://teleport-session-storage-2?credentialsPath=/var/lib/teleport/gcs_creds.json&projectID=EXAMPLE_gcp-proj-with-firestore-enabled' ...","title":"Storage: Google Cloud Storage"},{"location":"gcp-guide/#network-services-load-balancing","text":"Load Balancing is required for Proxy and SSH traffic. Use TCP Load Balancing as Teleport requires custom ports for SSH and Web Traffic.","title":"Network Services: Load Balancing"},{"location":"gcp-guide/#network-services-cloud-dns","text":"Cloud DNS is used to setup the public URL of the Teleport Proxy. Once setup an A record is sufficient.","title":"Network Services: Cloud DNS"},{"location":"gcp-guide/#access-service-accounts","text":"The Authentication server will need to read and write to Firestore. For this it'll need the correct permission via Server Accounts. Learn how to enable and create service accounts for instances . Download JSON Service Key","title":"Access: Service accounts"},{"location":"gcp-guide/#gcp-quickstart","text":"","title":"GCP Quickstart"},{"location":"gcp-guide/#1-create-resources","text":"We recommend starting by creating the resources. We highly recommend creating these an infrastructure automation tool such as Cloud Deployment Manager or Terraform.","title":"1. Create Resources"},{"location":"gcp-guide/#2-install-configure-teleport","text":"Follow install instructions from our installation page . We recommend configuring Teleport as per the below steps: 1. Configure Teleport Auth Server using the below example teleport.yaml , and start it using systemd or use DEB/.RPM packages available from our Downloads Page . # # Sample Teleport configuration teleport.yaml file for Auth Server # teleport : nodename : teleport-auth-server data_dir : /var/lib/teleport pid_file : /run/teleport.pid auth_token : EXAMPLE-CLUSTER-JOIN-TOKEN auth_servers : - 0.0.0.0:3025 connection_limits : max_connections : 15000 max_users : 250 log : output : stderr severity : DEBUG storage : type : firestore collection_name : cluster-data # Credentials: Path to google service account file, used for Firestore and Google Storage. credentials_path : /var/lib/teleport/gcs_creds.json project_id : example_Teleport-Project-Name audit_events_uri : 'firestore://events?projectID=example_Teleport-Project-Name&credentialsPath=/var/lib/teleport/gcs_creds' audit_sessions_uri : 'gs://teleport-session-storage-2?credentialsPath=/var/lib/teleport/gcs_creds.json&projectID=example_Teleport-Project-Name' auth_service : enabled : yes auth_service : tokens : - \"proxy,node:EXAMPLE-CLUSTER-JOIN-TOKEN\" 2. Setup Proxy Save the following configuration file as /etc/teleport.yaml on the Proxy Server: teleport : auth_token : EXAMPLE-CLUSTER-JOIN-TOKEN # We recommend using a TCP load balancer pointed to the auth servers when # setting up in HA mode. auth_servers : [ \"auth.example.com:3025\" ] # enable ssh service and disable auth and proxy: ssh_service : enabled : true auth_service : enabled : false proxy_service : enabled : false 3. Setup Teleport Nodes Save the following configuration file as /etc/teleport.yaml on the node: teleport : auth_token : EXAMPLE-CLUSTER-JOIN-TOKEN # We recommend using a TCP load balancer pointed to the auth servers when # setting up in HA mode. auth_servers : [ \"auth.example.com:3025\" ] # enable ssh service and disable auth and proxy: ssh_service : enabled : true auth_service : enabled : false proxy_service : enabled : false 4. Add Users Follow adding users or integrate with G Suite to provide SSO access.","title":"2. Install &amp; Configure Teleport"},{"location":"ibm-cloud-guide/","text":"Running Teleport on IBM Cloud We've created this guide to give customers a high level overview of how to use Teleport on the IBM Cloud . This guide provides a high level introduction leading to a deep dive into how to setup and run Teleport in production. We have split this guide into: Teleport on IBM FAQ IBM Teleport Introduction Teleport on IBM Cloud FAQ Why would you want to use Teleport with IBM Cloud? Teleport provides privileged access management for cloud-native infrastructure that doesn\u2019t get in the way. Infosec and systems engineers can secure access to their infrastructure, meet compliance requirements, reduce operational overhead, and have complete visibility into access and behavior. By using Teleport with IBM you can easily unify all access for both IBM Cloud and Softlayer infrastructure. Which Services can I use Teleport with? You can use Teleport for all the services that you would SSH into. This guide is focused on IBM Cloud. In the future we'll plan to update on how to use Teleport with IBM Cloud Kubernetes Service. IBM Teleport Introduction This guide will cover how to setup, configure and run Teleport on IBM Cloud. IBM Services required to run Teleport in HA: IBM Cloud: Virtual Servers with Instance Groups Storage: Database for etcd Storage: IBM Cloud File Storage Network Services: Cloud DNS Other things needed: SSL Certificate We recommend setting up Teleport in high availability mode (HA). In HA mode etcd stores the state of the system and IBM Cloud Storage stores the audit logs. IBM Cloud: Virtual Servers with Instance Groups We recommend Gen 2 Cloud IBM Virtual Servers and Auto Scaling For Staging and POCs we recommend using bx2-2x8 machines with 2 vCPUs, 4GB RAM, 4 Gbps. For Production we would recommend cx2-4x8 with 4 vCPUs, 8 GB RAM, 8 Gbps. Storage: Database for etcd IBM offers managed etcd instances. Teleport uses etcd as a scalable database to maintain high availability and provide graceful restarts. The service has to be turned on from within the IBM Cloud Dashboard . We recommend picking an etcd instance in the same region as your planned Teleport cluster. Deployment region: Same as rest of Teleport Cluster Initial Memory allocation: 2GB/member (6GB total) Initial disk allocation: 20GB/member (60GB total) CPU allocation: Shared etcd version: 3.3 Saving Credentials teleport : storage : type : etcd # list of etcd peers to connect to: # Showing IBM example host and port. peers : [ \"https://a9e977c0-224a-40bb-af51-21893b8fde79.b2b5a92ee2df47d58bad0fa448c15585.databases.appdomain.cloud:30359\" ] # optional password based authentication # See https://etcd.io/docs/v3.4.0/op-guide/authentication/ for setting # up a new user. IBM Defaults to `root` username : 'root' # The password file should just contain the password. password_file : '/var/lib/etcd-pass' # TLS certificate Name, with file contents from Overview tls_ca_file : '/var/lib/teleport/797cfsdf23e-4027-11e9-a020-42025ffb08c8.pem' # etcd key (location) where teleport will be storing its state under. # make sure it ends with a '/'! prefix : '/teleport/' Storage: IBM Cloud File Storage We recommend using IBM Cloud File Storage to store Teleport recorded sessions. Create New File Storage Resource. IBM Catalog - File Storage Quick Link 1a. We recommend using Standard Create a new bucket. Setup HMAC Credentials Update audit session URI audit_sessions_uri: 's3://BUCKET-NAME/readonly/records?endpoint=s3.us-east.cloud-object-storage.appdomain.cloud&region=ibm' When setting up audit_session_uri use s3:// session prefix. The credentials are used from ~/.aws/credentials and should be created with HMAC option: { \"apikey\" : \"LU9VCDf4dDzj1wjt0Q-BHaa2VEM7I53_3lPff50d_uv3\" , \"cos_hmac_keys\" : { \"access_key_id\" : \"e668d66374e141668ef0089f43bc879e\" , \"secret_access_key\" : \"d8762b57f61d5dd524ccd49c7d44861ceab098d217d05836\" }, \"endpoints\" : \"https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\" , \"iam_apikey_description\" : \"Auto-generated for key e668d663-74e1-4166-8ef0-089f43bc879e\" , \"iam_apikey_name\" : \"Service credentials-1\" , \"iam_role_crn\" : \"crn:v1:bluemix:public:iam::::serviceRole:Writer\" , \"iam_serviceid_crn\" : \"crn:v1:bluemix:public:iam-identity::a/0328d127d04047548c9d4bedcd24b85e::serviceid:ServiceId-c7ee0ee9-ea74-467f-a49e-ef60f6b27a71\" , \"resource_instance_id\" : \"crn:v1:bluemix:public:cloud-object-storage:global:a/0328d127d04047548c9d4bedcd24b85e:32049c3c-207e-4731-8b8a-53bf3b4844e7::\" } Save these settings to ~/.aws/credentials # Example keys from example service account to be saved into ~/.aws/credentials [ default ] access_key_id=\"e668d66374e141668e3432443bc879e\" secret_access_key=\"d8762b57f61d5dd524ccd49c7d44861ceafdsfds37d05836\" Example /etc/teleport.yaml ... storage : ... # Note # # endpoint=s3.us-east.cloud-object-storage.appdomain.cloud | This URL will # differ depending on which region the bucket is created. Use the public # endpoints. # # region=ibm | Should always be set as IBM. audit_sessions_uri : 's3://BUCKETNAME/readonly/records?endpoint=s3.us-east.cloud-object-storage.appdomain.cloud&region=ibm' ... Tip When starting with teleport start --config=/etc/teleport.yaml -d you can confirm that the bucket has been created. root@teleport:~# teleport start --config = /etc/teleport.yaml -d DEBU [ SQLITE ] Connected to: file:/var/lib/teleport/proc/sqlite.db?_busy_timeout = 10000 & _sync = OFF, poll stream period: 1s lite/lite.go:173 DEBU [ SQLITE ] Synchronous: 0 , busy timeout: 10000 lite/lite.go:220 DEBU [ KEYGEN ] SSH cert authority is going to pre-compute 25 keys. native/native.go:104 DEBU [ PROC:1 ] Using etcd backend. service/service.go:2309 INFO [ S3 ] Setting up bucket \"ben-teleport-test-cos-standard-b98\" , sessions path \"/readonly/records\" in region \"ibm\" . s3sessions/s3handler.go:143 INFO [ S3 ] Setup bucket \"ben-teleport-test-cos-standard-b98\" completed. duration:356.958618ms s3sessions/s3handler.go:147 Network: IBM Cloud DNS Services We recommend using IBM Cloud DNS for the Teleport Proxy public address. See the Admin Guide for more information. # The DNS name the proxy HTTPS endpoint as accessible by cluster users. # Defaults to the proxy's hostname if not specified. If running multiple # proxies behind a load balancer, this name must point to the load balancer # (see public_addr section below) public_addr : proxy.example.com:3080","title":"IBM"},{"location":"ibm-cloud-guide/#running-teleport-on-ibm-cloud","text":"We've created this guide to give customers a high level overview of how to use Teleport on the IBM Cloud . This guide provides a high level introduction leading to a deep dive into how to setup and run Teleport in production. We have split this guide into: Teleport on IBM FAQ IBM Teleport Introduction","title":"Running Teleport on IBM Cloud"},{"location":"ibm-cloud-guide/#teleport-on-ibm-cloud-faq","text":"","title":"Teleport on IBM Cloud FAQ"},{"location":"ibm-cloud-guide/#ibm-teleport-introduction","text":"This guide will cover how to setup, configure and run Teleport on IBM Cloud. IBM Services required to run Teleport in HA: IBM Cloud: Virtual Servers with Instance Groups Storage: Database for etcd Storage: IBM Cloud File Storage Network Services: Cloud DNS Other things needed: SSL Certificate We recommend setting up Teleport in high availability mode (HA). In HA mode etcd stores the state of the system and IBM Cloud Storage stores the audit logs.","title":"IBM Teleport Introduction"},{"location":"ibm-cloud-guide/#ibm-cloud-virtual-servers-with-instance-groups","text":"We recommend Gen 2 Cloud IBM Virtual Servers and Auto Scaling For Staging and POCs we recommend using bx2-2x8 machines with 2 vCPUs, 4GB RAM, 4 Gbps. For Production we would recommend cx2-4x8 with 4 vCPUs, 8 GB RAM, 8 Gbps.","title":"IBM Cloud: Virtual Servers with Instance Groups"},{"location":"ibm-cloud-guide/#storage-database-for-etcd","text":"IBM offers managed etcd instances. Teleport uses etcd as a scalable database to maintain high availability and provide graceful restarts. The service has to be turned on from within the IBM Cloud Dashboard . We recommend picking an etcd instance in the same region as your planned Teleport cluster. Deployment region: Same as rest of Teleport Cluster Initial Memory allocation: 2GB/member (6GB total) Initial disk allocation: 20GB/member (60GB total) CPU allocation: Shared etcd version: 3.3","title":"Storage: Database for etcd"},{"location":"ibm-cloud-guide/#storage-ibm-cloud-file-storage","text":"We recommend using IBM Cloud File Storage to store Teleport recorded sessions. Create New File Storage Resource. IBM Catalog - File Storage Quick Link 1a. We recommend using Standard Create a new bucket. Setup HMAC Credentials Update audit session URI audit_sessions_uri: 's3://BUCKET-NAME/readonly/records?endpoint=s3.us-east.cloud-object-storage.appdomain.cloud&region=ibm' When setting up audit_session_uri use s3:// session prefix. The credentials are used from ~/.aws/credentials and should be created with HMAC option: { \"apikey\" : \"LU9VCDf4dDzj1wjt0Q-BHaa2VEM7I53_3lPff50d_uv3\" , \"cos_hmac_keys\" : { \"access_key_id\" : \"e668d66374e141668ef0089f43bc879e\" , \"secret_access_key\" : \"d8762b57f61d5dd524ccd49c7d44861ceab098d217d05836\" }, \"endpoints\" : \"https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\" , \"iam_apikey_description\" : \"Auto-generated for key e668d663-74e1-4166-8ef0-089f43bc879e\" , \"iam_apikey_name\" : \"Service credentials-1\" , \"iam_role_crn\" : \"crn:v1:bluemix:public:iam::::serviceRole:Writer\" , \"iam_serviceid_crn\" : \"crn:v1:bluemix:public:iam-identity::a/0328d127d04047548c9d4bedcd24b85e::serviceid:ServiceId-c7ee0ee9-ea74-467f-a49e-ef60f6b27a71\" , \"resource_instance_id\" : \"crn:v1:bluemix:public:cloud-object-storage:global:a/0328d127d04047548c9d4bedcd24b85e:32049c3c-207e-4731-8b8a-53bf3b4844e7::\" } Save these settings to ~/.aws/credentials # Example keys from example service account to be saved into ~/.aws/credentials [ default ] access_key_id=\"e668d66374e141668e3432443bc879e\" secret_access_key=\"d8762b57f61d5dd524ccd49c7d44861ceafdsfds37d05836\" Example /etc/teleport.yaml ... storage : ... # Note # # endpoint=s3.us-east.cloud-object-storage.appdomain.cloud | This URL will # differ depending on which region the bucket is created. Use the public # endpoints. # # region=ibm | Should always be set as IBM. audit_sessions_uri : 's3://BUCKETNAME/readonly/records?endpoint=s3.us-east.cloud-object-storage.appdomain.cloud&region=ibm' ... Tip When starting with teleport start --config=/etc/teleport.yaml -d you can confirm that the bucket has been created. root@teleport:~# teleport start --config = /etc/teleport.yaml -d DEBU [ SQLITE ] Connected to: file:/var/lib/teleport/proc/sqlite.db?_busy_timeout = 10000 & _sync = OFF, poll stream period: 1s lite/lite.go:173 DEBU [ SQLITE ] Synchronous: 0 , busy timeout: 10000 lite/lite.go:220 DEBU [ KEYGEN ] SSH cert authority is going to pre-compute 25 keys. native/native.go:104 DEBU [ PROC:1 ] Using etcd backend. service/service.go:2309 INFO [ S3 ] Setting up bucket \"ben-teleport-test-cos-standard-b98\" , sessions path \"/readonly/records\" in region \"ibm\" . s3sessions/s3handler.go:143 INFO [ S3 ] Setup bucket \"ben-teleport-test-cos-standard-b98\" completed. duration:356.958618ms s3sessions/s3handler.go:147","title":"Storage: IBM Cloud File Storage"},{"location":"ibm-cloud-guide/#network-ibm-cloud-dns-services","text":"We recommend using IBM Cloud DNS for the Teleport Proxy public address. See the Admin Guide for more information. # The DNS name the proxy HTTPS endpoint as accessible by cluster users. # Defaults to the proxy's hostname if not specified. If running multiple # proxies behind a load balancer, this name must point to the load balancer # (see public_addr section below) public_addr : proxy.example.com:3080","title":"Network: IBM Cloud DNS Services"},{"location":"installation/","text":"Installation Teleport core service teleport and admin tool tctl have been designed to run on Linux and Mac operating systems. The Teleport user client tsh and UI are available for Linux, Mac and Windows operating systems. Linux The following examples install the 64-bit version of Teleport binaries, but 32-bit (i386) and ARM binaries are also available. Check the Latest Release page for the most up-to-date information. Tarball $ curl https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-amd64-bin.tar.gz.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-amd64-bin.tar.gz $ shasum -a 256 teleport-v {{ teleport.version }} -linux-amd64-bin.tar.gz # Verify that the checksums match $ tar -xzf teleport-v {{ teleport.version }} -linux-amd64-bin.tar.gz $ cd teleport $ ./install $ which teleport /usr/local/bin/teleport DEB $ curl https://get.gravitational.com/teleport_ {{ teleport.version }} _amd64.deb.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport_ {{ teleport.version }} _amd64.deb $ sha256sum teleport_ {{ teleport.version }} _amd64.deb # Verify that the checksums match $ dpkg -i teleport_ {{ teleport.version }} _amd64.deb $ which teleport /usr/local/bin/teleport RPM $ curl https://get.gravitational.com/teleport- {{ teleport.version }} -1.x86_64.rpm.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport- {{ teleport.version }} -1.x86_64.rpm $ sha256sum teleport- {{ teleport.version }} -1.x86_64.rpm # Verify that the checksums match $ rpm -i teleport- {{ teleport.version }} -1.x86_64.rpm $ which teleport /usr/local/bin/teleport Amazon Linux 2 $ yum install https://get.gravitational.com/teleport- {{ teleport.version }} -1.x86_64.rpm.sha256 $ which teleport /usr/local/bin/teleport $ cd teleport $ sudo ./install $ which teleport /usr/local/bin/teleport ARMv7 (32-bit) $ curl https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-arm-bin.tar.gz.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-arm-bin.tar.gz $ shasum -a 256 teleport-v {{ teleport.version }} -linux-arm-bin.tar.gz # Verify that the checksums match $ tar -xzf teleport-v {{ teleport.version }} -linux-arm-bin.tar.gz $ cd teleport $ ./install $ which teleport /usr/local/bin/teleport ARM64/ARMv8 (64-bit) $ curl https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-arm64-bin.tar.gz.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-arm64-bin.tar.gz $ shasum -a 256 teleport-v {{ teleport.version }} -linux-arm64-bin.tar.gz # Verify that the checksums match $ tar -xzf teleport-v {{ teleport.version }} -linux-arm64-bin.tar.gz $ cd teleport $ ./install $ which teleport /usr/local/bin/teleport Docker Please follow our OSS Docker Quickstart or Enterprise Docker Quickstart for install and setup instructions. $ docker pull quay.io/gravitational/teleport: {{ teleport.version }} Helm Please follow our Helm Chart Readme for install and setup instructions. $ helm repo add gravitational https://charts.gravitational.io $ helm install teleport gravitational/teleport MacOS Homebrew $ brew install teleport Download Download MacOS .pkg installer (tsh client only, signed) file, double-click to run the Installer. Note This method only installs the tsh client for interacting with Teleport clusters. If you need the teleport server or tctl admin tool, use the \"Terminal\" method instead. Terminal $ curl -O https://get.gravitational.com/teleport- {{ teleport.version }} .pkg $ sudo installer -pkg teleport- {{ teleport.version }} .pkg -target / # Installs on Macintosh HD Password: installer: Package name is teleport- {{ teleport.version }} installer: Upgrading at base path / installer: The upgrade was successful. $ which teleport /usr/local/bin/teleport Windows (tsh client only) As of version v3.0.1 we have tsh client binary available for Windows 64-bit architecture - teleport and tctl are not supported. Powershell > curl https://get.gravitational.com/teleport-v {{ teleport.version }} -windows-amd64-bin.zip.sha256 # <checksum> <filename> > curl -O teleport-v {{ teleport.version }} -windows-amd64-bin.zip https://get.gravitational.com/teleport-v {{ teleport.version }} -windows-amd64-bin.zip > echo %PATH% # Edit %PATH% if necessary > certUtil -hashfile teleport-v {{ teleport.version }} -windows-amd64-bin.zip SHA256 SHA256 hash of teleport-v {{ teleport.version }} -windows-amd64-bin.zip: # <checksum> <filename> CertUtil: -hashfile command completed successfully. # Verify that the checksums match # Move `tsh` to your %PATH% Installing from Source Gravitational Teleport is written in Go language. It requires Golang v{{ teleport.golang }} or newer. Check the repo README for the latest requirements. Install Go If you don't already have Golang installed you can see installation instructions here . If you are new to Go there are a few quick set up things to note: Go installs all dependencies for all projects in a single directory determined by the $GOPATH variable. The default directory is GOPATH=$HOME/go but you can set it to any directory you wish. If you plan to use Golang for more than just this installation you may want to echo \"export GOPATH=$HOME/go\" >> ~/.bashrc (or your shell config). Build Teleport # get the source & build: $ mkdir -p $GOPATH /src/github.com/gravitational $ cd $GOPATH /src/github.com/gravitational $ git clone https://github.com/gravitational/teleport.git $ cd teleport # Make sure you have `zip` installed - the Makefile uses it $ make full # create the default data directory before running `teleport` $ sudo mkdir -p /var/lib/teleport $ sudo chown $USER /var/lib/teleport If the build succeeds, the binaries teleport, tsh , and tctl are now in the directory $GOPATH/src/github.com/gravitational/teleport/build Checksums Gravitational Teleport provides a checksum from the Downloads . This should be used to verify the integrity of our binary. If you download Teleport via an automated system, you can programmatically obtain the checksum by adding .sha256 to the binary. This is the method shown in the installation examples. $ export version = v {{ teleport.version }} $ export os = linux # 'darwin' 'linux' or 'windows' $ export arch = amd64 # '386' 'arm' on linux or 'amd64' for all distros $ curl https://get.gravitational.com/teleport- $version - $os - $arch -bin.tar.gz.sha256 # <checksum> <filename>","title":"Installation"},{"location":"installation/#installation","text":"Teleport core service teleport and admin tool tctl have been designed to run on Linux and Mac operating systems. The Teleport user client tsh and UI are available for Linux, Mac and Windows operating systems.","title":"Installation"},{"location":"installation/#linux","text":"The following examples install the 64-bit version of Teleport binaries, but 32-bit (i386) and ARM binaries are also available. Check the Latest Release page for the most up-to-date information. Tarball $ curl https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-amd64-bin.tar.gz.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-amd64-bin.tar.gz $ shasum -a 256 teleport-v {{ teleport.version }} -linux-amd64-bin.tar.gz # Verify that the checksums match $ tar -xzf teleport-v {{ teleport.version }} -linux-amd64-bin.tar.gz $ cd teleport $ ./install $ which teleport /usr/local/bin/teleport DEB $ curl https://get.gravitational.com/teleport_ {{ teleport.version }} _amd64.deb.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport_ {{ teleport.version }} _amd64.deb $ sha256sum teleport_ {{ teleport.version }} _amd64.deb # Verify that the checksums match $ dpkg -i teleport_ {{ teleport.version }} _amd64.deb $ which teleport /usr/local/bin/teleport RPM $ curl https://get.gravitational.com/teleport- {{ teleport.version }} -1.x86_64.rpm.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport- {{ teleport.version }} -1.x86_64.rpm $ sha256sum teleport- {{ teleport.version }} -1.x86_64.rpm # Verify that the checksums match $ rpm -i teleport- {{ teleport.version }} -1.x86_64.rpm $ which teleport /usr/local/bin/teleport Amazon Linux 2 $ yum install https://get.gravitational.com/teleport- {{ teleport.version }} -1.x86_64.rpm.sha256 $ which teleport /usr/local/bin/teleport $ cd teleport $ sudo ./install $ which teleport /usr/local/bin/teleport ARMv7 (32-bit) $ curl https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-arm-bin.tar.gz.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-arm-bin.tar.gz $ shasum -a 256 teleport-v {{ teleport.version }} -linux-arm-bin.tar.gz # Verify that the checksums match $ tar -xzf teleport-v {{ teleport.version }} -linux-arm-bin.tar.gz $ cd teleport $ ./install $ which teleport /usr/local/bin/teleport ARM64/ARMv8 (64-bit) $ curl https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-arm64-bin.tar.gz.sha256 # <checksum> <filename> $ curl -O https://get.gravitational.com/teleport-v {{ teleport.version }} -linux-arm64-bin.tar.gz $ shasum -a 256 teleport-v {{ teleport.version }} -linux-arm64-bin.tar.gz # Verify that the checksums match $ tar -xzf teleport-v {{ teleport.version }} -linux-arm64-bin.tar.gz $ cd teleport $ ./install $ which teleport /usr/local/bin/teleport","title":"Linux"},{"location":"installation/#docker","text":"Please follow our OSS Docker Quickstart or Enterprise Docker Quickstart for install and setup instructions. $ docker pull quay.io/gravitational/teleport: {{ teleport.version }}","title":"Docker"},{"location":"installation/#helm","text":"Please follow our Helm Chart Readme for install and setup instructions. $ helm repo add gravitational https://charts.gravitational.io $ helm install teleport gravitational/teleport","title":"Helm"},{"location":"installation/#macos","text":"Homebrew $ brew install teleport Download Download MacOS .pkg installer (tsh client only, signed) file, double-click to run the Installer. Note This method only installs the tsh client for interacting with Teleport clusters. If you need the teleport server or tctl admin tool, use the \"Terminal\" method instead. Terminal $ curl -O https://get.gravitational.com/teleport- {{ teleport.version }} .pkg $ sudo installer -pkg teleport- {{ teleport.version }} .pkg -target / # Installs on Macintosh HD Password: installer: Package name is teleport- {{ teleport.version }} installer: Upgrading at base path / installer: The upgrade was successful. $ which teleport /usr/local/bin/teleport","title":"MacOS"},{"location":"installation/#windows-tsh-client-only","text":"As of version v3.0.1 we have tsh client binary available for Windows 64-bit architecture - teleport and tctl are not supported. Powershell > curl https://get.gravitational.com/teleport-v {{ teleport.version }} -windows-amd64-bin.zip.sha256 # <checksum> <filename> > curl -O teleport-v {{ teleport.version }} -windows-amd64-bin.zip https://get.gravitational.com/teleport-v {{ teleport.version }} -windows-amd64-bin.zip > echo %PATH% # Edit %PATH% if necessary > certUtil -hashfile teleport-v {{ teleport.version }} -windows-amd64-bin.zip SHA256 SHA256 hash of teleport-v {{ teleport.version }} -windows-amd64-bin.zip: # <checksum> <filename> CertUtil: -hashfile command completed successfully. # Verify that the checksums match # Move `tsh` to your %PATH%","title":"Windows (tsh client only)"},{"location":"installation/#installing-from-source","text":"Gravitational Teleport is written in Go language. It requires Golang v{{ teleport.golang }} or newer. Check the repo README for the latest requirements.","title":"Installing from Source"},{"location":"installation/#install-go","text":"If you don't already have Golang installed you can see installation instructions here . If you are new to Go there are a few quick set up things to note: Go installs all dependencies for all projects in a single directory determined by the $GOPATH variable. The default directory is GOPATH=$HOME/go but you can set it to any directory you wish. If you plan to use Golang for more than just this installation you may want to echo \"export GOPATH=$HOME/go\" >> ~/.bashrc (or your shell config).","title":"Install Go"},{"location":"installation/#build-teleport","text":"# get the source & build: $ mkdir -p $GOPATH /src/github.com/gravitational $ cd $GOPATH /src/github.com/gravitational $ git clone https://github.com/gravitational/teleport.git $ cd teleport # Make sure you have `zip` installed - the Makefile uses it $ make full # create the default data directory before running `teleport` $ sudo mkdir -p /var/lib/teleport $ sudo chown $USER /var/lib/teleport If the build succeeds, the binaries teleport, tsh , and tctl are now in the directory $GOPATH/src/github.com/gravitational/teleport/build","title":"Build Teleport"},{"location":"installation/#checksums","text":"Gravitational Teleport provides a checksum from the Downloads . This should be used to verify the integrity of our binary. If you download Teleport via an automated system, you can programmatically obtain the checksum by adding .sha256 to the binary. This is the method shown in the installation examples. $ export version = v {{ teleport.version }} $ export os = linux # 'darwin' 'linux' or 'windows' $ export arch = amd64 # '386' 'arm' on linux or 'amd64' for all distros $ curl https://get.gravitational.com/teleport- $version - $os - $arch -bin.tar.gz.sha256 # <checksum> <filename>","title":"Checksums"},{"location":"kubernetes-ssh/","text":"Teleport Kubernetes Access Guide Teleport has the ability to act as a compliance gateway for managing privileged access to Kubernetes clusters. This enables the following capabilities: A Teleport Proxy can act as a single authentication endpoint for both SSH and Kubernetes. Users can authenticate against a Teleport proxy using Teleport's tsh login command and retrieve credentials for both SSH and Kubernetes API. Users RBAC roles are always synchronized between SSH and Kubernetes, making it easier to implement policies like developers must not access production data . Teleport's session recording and audit log extend to Kubernetes, as well. Regular kubectl exec commands are logged into the audit log and the interactive commands are recorded as regular sessions that can be stored and replayed in the future. Teleport Proxy Service By default, the Kubernetes integration is turned off in Teleport. The configuration setting to enable the integration in the proxy service section in the /etc/teleport.yaml config file, as shown below: # snippet from /etc/teleport.yaml on the Teleport proxy service: proxy_service : # create the 'kubernetes' section and set 'enabled' to 'yes': kubernetes : enabled : yes public_addr : [ teleport.example.com : 3026 ] listen_addr : 0.0.0.0:3026 Let's take a closer look at the available Kubernetes settings: public_addr defines the publicly accessible address which Kubernetes API clients like kubectl will connect to. This address will be placed inside of kubeconfig on a client's machine when a client executes tsh login command to retrieve its certificate. If you intend to run multiple Teleport proxies behind a load balancer, this must be the load balancer's public address. listen_addr defines which network interface and port the Teleport proxy server should bind to. It defaults to port 3026 on all NICs. Connecting the Teleport proxy to Kubernetes There are two options for setting up Teleport to access Kubernetes: Option 1: Deploy Inside Kubernetes as a pod Deploy Teleport Proxy service as a Kubernetes pod inside the Kubernetes cluster you want the proxy to have access to. # snippet from /etc/teleport.yaml on the Teleport proxy service: proxy_service : # create the 'kubernetes' section and set 'enabled' to 'yes': kubernetes : enabled : yes If you're using Helm, we've a chart that you can use. Run these commands: $ helm repo add gravitational https://charts.gravitational.io $ helm install teleport gravitational/teleport You will still need a correctly configured values.yaml file for this to work. See our Helm Docs for more information. Option 2: Deploy Outside of Kubernetes Deploy the Teleport proxy service outside of Kubernetes and update the Teleport Proxy configuration with Kubernetes credentials. Update the Teleport Proxy configuration with Kubernetes credentials. In this case, we need to update /etc/teleport.yaml for the proxy service as shown below: # snippet from /etc/teleport.yaml on the Teleport proxy service: proxy_service : # create the 'kubernetes' section and set 'enabled' to 'yes': kubernetes : enabled : yes # The address for the proxy process to accept k8s requests. listen_addr : 0.0.0.0:3026 # The address used by the clients after tsh login. If you run a load balancer # in front of this proxy, use the address of that balancer here. Otherwise, # use the address of the host running this proxy. public_addr : [ teleport.example.com : 3026 ] kubeconfig_file : /path/to/.kube/config To generate the kubeconfig_file for the Teleport proxy service: Configure your kubectl to point at the Kubernetes cluster and have admin-level access. Use this script to generate kubeconfig : # Download the script. $ curl -o get-kubeconfig.sh https://raw.githubusercontent.com/gravitational/teleport/master/examples/k8s-auth/get-kubeconfig.sh # Make it executable. $ chmod +x get-kubeconfig.sh # Run the script, it will write the generated kubeconfig to the current # directory. $ ./get-kubeconfig.sh # Check that the generated kubeconfig has the right permissions. # The output should look similar to this. $ kubectl --kubeconfig kubeconfig auth can-i --list Resources Non-Resource URLs Resource Names Verbs selfsubjectaccessreviews.authorization.k8s.io [] [] [ create create ] selfsubjectrulesreviews.authorization.k8s.io [] [] [ create create ] [ /api/* ] [] [ get ] ... [] [ ... ] groups [] [] [ impersonate ] serviceaccounts [] [] [ impersonate ] users [] [] [ impersonate ] Copy the generated kubeconfig file to the host running the Teleport proxy service. Update kubeconfig_file path in teleport.yaml to where you copied the kubeconfig . Alternatively, you can use your existing local config from ~/.kube/config . However, it will result in Teleport proxy using your personal Kubernetes credentials. This is risky: your credentials can expire or get revoked (such as when leaving your company). Impersonation Note If you used the script from Option 2 above, you can skip this step. The script already configured impersonation permissions. The next step is to configure the Teleport Proxy to be able to impersonate Kubernetes principals within a given group using Kubernetes Impersonation Headers . If Teleport is running inside the cluster using a Kubernetes ServiceAccount , here's an example of the permissions that the ServiceAccount will need to be able to use impersonation (change teleport-serviceaccount to the name of the ServiceAccount that's being used): apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : teleport-impersonation rules : - apiGroups : - \"\" resources : - users - groups - serviceaccounts verbs : - impersonate - apiGroups : - \"authorization.k8s.io\" resources : - selfsubjectaccessreviews verbs : - create --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : teleport roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : teleport-impersonation subjects : - kind : ServiceAccount # this should be changed to the name of the Kubernetes ServiceAccount being used name : teleport-serviceaccount namespace : default There is also an example of this usage within the example Teleport Helm chart . If Teleport is running outside of the Kubernetes cluster, you will need to ensure that the principal used to connect to Kubernetes via the kubeconfig file has the same impersonation permissions as are described in the ClusterRole above. Kubernetes RBAC Once you perform the steps above, your Teleport instance should become a fully functional Kubernetes API proxy. The next step is to configure Teleport to assign the correct Kubernetes groups to Teleport users. Mapping Kubernetes groups to Teleport users depends on how Teleport is configured. In this guide we'll look at two common configurations: Open source, Teleport Community edition configured to authenticate users via Github . In this case, we'll need to map Github teams to Kubernetes groups. Commercial, Teleport Enterprise edition configured to authenticate users via Okta SSO . In this case, we'll need to map users' groups that come from Okta to Kubernetes groups. Kubernetes Groups and Users Teleport provides support for Kubernetes Groups, using kubernetes_groups: [\"system:masters\"] . Kubernetes Users, using kubernetes_users: ['barent', 'jane'] . If a Kubernetes user isn't set the user will impersonate themselves. When adding new local users you have to specify which Kubernetes groups they belong to: # Adding a Teleport local user to map to a Kubernetes group. $ tctl users add joe --k8s-groups = \"system:masters\" # Adding a Teleport local user to map to a Kubernetes user. $ tctl users add jenkins --k8s-users = \"jenkins\" # Enterprise users should manage k8s-users and k8s-groups via RBAC, see Okta Auth # example below Github Auth When configuring Teleport to authenticate against Github, you have to create a Teleport connector for Github, like the one shown below. Notice the kubernetes_groups setting which assigns Kubernetes groups to a given Github team: kind : github version : v3 metadata : # connector name that will be used with `tsh --auth=github login` name : github spec : # client ID of Github OAuth app client_id : <client-id> # client secret of Github OAuth app client_secret : <client-secret> # connector display name that will be shown on web UI login screen display : Github # callback URL that will be called after successful authentication redirect_url : https://teleport.example.com:3080/v1/webapi/github/callback # mapping of org/team memberships onto allowed logins and roles teams_to_logins : - organization : octocats # Github organization name team : admin # Github team name within that organization # allowed UNIX logins for team octocats/admin: logins : - root # list of Kubernetes groups this Github team is allowed to connect to kubernetes_groups : [ \"system:masters\" ] # Optional: If not set, users will impersonate themselves. # kubernetes_users: ['barent'] To obtain client ID and client secret from Github, please follow Github documentation on how to create and register an OAuth app. Be sure to set the \"Authorization callback URL\" to the same value as redirect_url in the resource spec. Finally, create the Github connector with the command: tctl create -f github.yaml . Now, when Teleport users execute the Teleport's tsh login command, they will be prompted to login through the Github SSO and upon successful authentication, they have access to Kubernetes. # Login via Github SSO and retrieve SSH+Kubernetes certificates: $ tsh login --proxy=teleport.example.com --auth=github login # Use Kubernetes API! $ kubectl exec -ti <pod-name> The kubectl exec request will be routed through the Teleport proxy and Teleport will log the audit record and record the session. Note For more information on integrating Teleport with Github SSO, please see the Github section in the Admin Manual . Okta Auth With Okta (or any other SAML/OIDC/Active Directory provider), you must update Teleport's roles to include the mapping to Kubernetes groups. Let's assume you have the Teleport role called \"admin\". Add kubernetes_groups setting to it as shown below: # NOTE: the role definition is edited to remove the unnecessary fields kind : role version : v3 metadata : name : admin spec : allow : # if kubernetes integration is enabled, this setting configures which # kubernetes groups the users of this role will be assigned to. # note that you can refer to a SAML/OIDC trait via the \"external\" property bag, # this allows you to specify Kubernetes group membership in an identity manager: kubernetes_groups : [ \"system:masters\" , \"{% raw %}{{external.trait_name}}{% endraw %}\" ] ] To add kubernetes_groups setting to an existing Teleport role, you can either use the Web UI or tctl : # Dump the \"admin\" role into a file: $ tctl get roles/admin > admin.yaml # Edit the file, add kubernetes_groups setting # and then execute: $ tctl create -f admin.yaml Advanced Usage {% raw %}{{ external.trait_name }}{% endraw %} example is shown to demonstrate how to fetch the Kubernetes groups dynamically from Okta during login. In this case, you need to define Kubernetes group membership in Okta (as a trait) and use that trait name in the Teleport role. Teleport 4.3 has an option to extract the local part from an email claim. This can be helpful since some operating systems don't support the @ symbol. This means by using logins: ['{% raw %}{{email.local(external.email)}}{% endraw %}'] the resulting output will be dave.smith if the email was dave.smith@acme.com. Once setup is complete, when users execute tsh login and go through the usual Okta login sequence, their kubeconfig will be updated with their Kubernetes credentials. Note For more information on integrating Teleport with Okta, please see the Okta integration guide . Using Teleport Kubernetes with Automation Teleport can integrate with CI/CD tooling for greater visibility and auditability of these tools. For this we recommend creating a local Teleport user, then exporting a kubeconfig using tctl auth sign An example setup is below. # Create a new local user for Jenkins $ tctl users add jenkins # Option 1: Creates a token for 1 year $ tctl auth sign --user = jenkins --format = kubernetes --out = kubeconfig --ttl = 8760h # Recommended Option 2: Creates a token for 25hrs $ tctl auth sign --user = jenkins --format = kubernetes --out = kubeconfig --ttl = 25h The credentials have been written to kubeconfig $ cat kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZ.... # This kubeconfig can now be exported and will provide access to the automation tooling. # Uses kubectl to get pods, using the provided kubeconfig. $ kubectl --kubeconfig /path/to/kubeconfig get pods How long should TTL be? In the above example we've provided two options. One with 1yr (8760h) time to live and one for just 25hrs. As proponents of short lived SSH certificates we recommend the same for automation. Handling secrets is out of scope of our docs, but at a high level we recommend using providers secrets managers. Such as AWS Secrets Manager , GCP Secrets Manager , or on prem using a project like Vault . Then running a nightly job on the auth server to sign and publish a new kubeconfig. In our example, we've added 1hr, and during this time both kubeconfigs will be valid. Taking this a step further you could build a system to request a very short lived token for each CI run. We plan to make this easier for operators to integrate in the future by exposing and documenting more of our API. AWS EKS We've a complete guide on setting up Teleport with EKS. Please see the Using Teleport with EKS Guide . Multiple Kubernetes Clusters You can take advantage of the Trusted Clusters feature of Teleport to federate trust across multiple Kubernetes clusters. When multiple trusted clusters are present behind a Teleport proxy, the kubeconfig generated by tsh login will contain the Kubernetes API endpoint determined by the <cluster> argument to tsh login . For example, consider the following setup: There are three Teleport/Kubernetes clusters: \"main\", \"east\" and \"west\". These are the names set in cluster_name setting in their configuration files. The clusters \"east\" and \"west\" are trusted clusters for \"main\". Users always authenticate against \"main\" but use their certificates to access SSH nodes and Kubernetes API in all three clusters. The DNS name of the main proxy server is \"main.example.com\" In this scenario, users usually login using this command: # Using login without arguments $ tsh --proxy = main.example.com login # user's `kubeconfig` now contains one entry for the main Kubernetes # endpoint, i.e. `proxy.example.com` . # Receive a certificate for \"east\": $ tsh --proxy = main.example.com login east # user's `kubeconfig` now contains the entry for the \"east\" Kubernetes # endpoint, i.e. `east.proxy.example.com` .","title":"Kubernetes Guide"},{"location":"kubernetes-ssh/#teleport-kubernetes-access-guide","text":"Teleport has the ability to act as a compliance gateway for managing privileged access to Kubernetes clusters. This enables the following capabilities: A Teleport Proxy can act as a single authentication endpoint for both SSH and Kubernetes. Users can authenticate against a Teleport proxy using Teleport's tsh login command and retrieve credentials for both SSH and Kubernetes API. Users RBAC roles are always synchronized between SSH and Kubernetes, making it easier to implement policies like developers must not access production data . Teleport's session recording and audit log extend to Kubernetes, as well. Regular kubectl exec commands are logged into the audit log and the interactive commands are recorded as regular sessions that can be stored and replayed in the future.","title":"Teleport Kubernetes Access Guide"},{"location":"kubernetes-ssh/#teleport-proxy-service","text":"By default, the Kubernetes integration is turned off in Teleport. The configuration setting to enable the integration in the proxy service section in the /etc/teleport.yaml config file, as shown below: # snippet from /etc/teleport.yaml on the Teleport proxy service: proxy_service : # create the 'kubernetes' section and set 'enabled' to 'yes': kubernetes : enabled : yes public_addr : [ teleport.example.com : 3026 ] listen_addr : 0.0.0.0:3026 Let's take a closer look at the available Kubernetes settings: public_addr defines the publicly accessible address which Kubernetes API clients like kubectl will connect to. This address will be placed inside of kubeconfig on a client's machine when a client executes tsh login command to retrieve its certificate. If you intend to run multiple Teleport proxies behind a load balancer, this must be the load balancer's public address. listen_addr defines which network interface and port the Teleport proxy server should bind to. It defaults to port 3026 on all NICs.","title":"Teleport Proxy Service"},{"location":"kubernetes-ssh/#connecting-the-teleport-proxy-to-kubernetes","text":"There are two options for setting up Teleport to access Kubernetes:","title":"Connecting the Teleport proxy to Kubernetes"},{"location":"kubernetes-ssh/#option-1-deploy-inside-kubernetes-as-a-pod","text":"Deploy Teleport Proxy service as a Kubernetes pod inside the Kubernetes cluster you want the proxy to have access to. # snippet from /etc/teleport.yaml on the Teleport proxy service: proxy_service : # create the 'kubernetes' section and set 'enabled' to 'yes': kubernetes : enabled : yes If you're using Helm, we've a chart that you can use. Run these commands: $ helm repo add gravitational https://charts.gravitational.io $ helm install teleport gravitational/teleport You will still need a correctly configured values.yaml file for this to work. See our Helm Docs for more information.","title":"Option 1: Deploy Inside Kubernetes as a pod"},{"location":"kubernetes-ssh/#option-2-deploy-outside-of-kubernetes","text":"Deploy the Teleport proxy service outside of Kubernetes and update the Teleport Proxy configuration with Kubernetes credentials. Update the Teleport Proxy configuration with Kubernetes credentials. In this case, we need to update /etc/teleport.yaml for the proxy service as shown below: # snippet from /etc/teleport.yaml on the Teleport proxy service: proxy_service : # create the 'kubernetes' section and set 'enabled' to 'yes': kubernetes : enabled : yes # The address for the proxy process to accept k8s requests. listen_addr : 0.0.0.0:3026 # The address used by the clients after tsh login. If you run a load balancer # in front of this proxy, use the address of that balancer here. Otherwise, # use the address of the host running this proxy. public_addr : [ teleport.example.com : 3026 ] kubeconfig_file : /path/to/.kube/config To generate the kubeconfig_file for the Teleport proxy service: Configure your kubectl to point at the Kubernetes cluster and have admin-level access. Use this script to generate kubeconfig : # Download the script. $ curl -o get-kubeconfig.sh https://raw.githubusercontent.com/gravitational/teleport/master/examples/k8s-auth/get-kubeconfig.sh # Make it executable. $ chmod +x get-kubeconfig.sh # Run the script, it will write the generated kubeconfig to the current # directory. $ ./get-kubeconfig.sh # Check that the generated kubeconfig has the right permissions. # The output should look similar to this. $ kubectl --kubeconfig kubeconfig auth can-i --list Resources Non-Resource URLs Resource Names Verbs selfsubjectaccessreviews.authorization.k8s.io [] [] [ create create ] selfsubjectrulesreviews.authorization.k8s.io [] [] [ create create ] [ /api/* ] [] [ get ] ... [] [ ... ] groups [] [] [ impersonate ] serviceaccounts [] [] [ impersonate ] users [] [] [ impersonate ] Copy the generated kubeconfig file to the host running the Teleport proxy service. Update kubeconfig_file path in teleport.yaml to where you copied the kubeconfig . Alternatively, you can use your existing local config from ~/.kube/config . However, it will result in Teleport proxy using your personal Kubernetes credentials. This is risky: your credentials can expire or get revoked (such as when leaving your company).","title":"Option 2: Deploy Outside of Kubernetes"},{"location":"kubernetes-ssh/#impersonation","text":"Note If you used the script from Option 2 above, you can skip this step. The script already configured impersonation permissions. The next step is to configure the Teleport Proxy to be able to impersonate Kubernetes principals within a given group using Kubernetes Impersonation Headers . If Teleport is running inside the cluster using a Kubernetes ServiceAccount , here's an example of the permissions that the ServiceAccount will need to be able to use impersonation (change teleport-serviceaccount to the name of the ServiceAccount that's being used): apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : teleport-impersonation rules : - apiGroups : - \"\" resources : - users - groups - serviceaccounts verbs : - impersonate - apiGroups : - \"authorization.k8s.io\" resources : - selfsubjectaccessreviews verbs : - create --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : teleport roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : teleport-impersonation subjects : - kind : ServiceAccount # this should be changed to the name of the Kubernetes ServiceAccount being used name : teleport-serviceaccount namespace : default There is also an example of this usage within the example Teleport Helm chart . If Teleport is running outside of the Kubernetes cluster, you will need to ensure that the principal used to connect to Kubernetes via the kubeconfig file has the same impersonation permissions as are described in the ClusterRole above.","title":"Impersonation"},{"location":"kubernetes-ssh/#kubernetes-rbac","text":"Once you perform the steps above, your Teleport instance should become a fully functional Kubernetes API proxy. The next step is to configure Teleport to assign the correct Kubernetes groups to Teleport users. Mapping Kubernetes groups to Teleport users depends on how Teleport is configured. In this guide we'll look at two common configurations: Open source, Teleport Community edition configured to authenticate users via Github . In this case, we'll need to map Github teams to Kubernetes groups. Commercial, Teleport Enterprise edition configured to authenticate users via Okta SSO . In this case, we'll need to map users' groups that come from Okta to Kubernetes groups.","title":"Kubernetes RBAC"},{"location":"kubernetes-ssh/#kubernetes-groups-and-users","text":"Teleport provides support for Kubernetes Groups, using kubernetes_groups: [\"system:masters\"] . Kubernetes Users, using kubernetes_users: ['barent', 'jane'] . If a Kubernetes user isn't set the user will impersonate themselves. When adding new local users you have to specify which Kubernetes groups they belong to: # Adding a Teleport local user to map to a Kubernetes group. $ tctl users add joe --k8s-groups = \"system:masters\" # Adding a Teleport local user to map to a Kubernetes user. $ tctl users add jenkins --k8s-users = \"jenkins\" # Enterprise users should manage k8s-users and k8s-groups via RBAC, see Okta Auth # example below","title":"Kubernetes Groups and Users"},{"location":"kubernetes-ssh/#github-auth","text":"When configuring Teleport to authenticate against Github, you have to create a Teleport connector for Github, like the one shown below. Notice the kubernetes_groups setting which assigns Kubernetes groups to a given Github team: kind : github version : v3 metadata : # connector name that will be used with `tsh --auth=github login` name : github spec : # client ID of Github OAuth app client_id : <client-id> # client secret of Github OAuth app client_secret : <client-secret> # connector display name that will be shown on web UI login screen display : Github # callback URL that will be called after successful authentication redirect_url : https://teleport.example.com:3080/v1/webapi/github/callback # mapping of org/team memberships onto allowed logins and roles teams_to_logins : - organization : octocats # Github organization name team : admin # Github team name within that organization # allowed UNIX logins for team octocats/admin: logins : - root # list of Kubernetes groups this Github team is allowed to connect to kubernetes_groups : [ \"system:masters\" ] # Optional: If not set, users will impersonate themselves. # kubernetes_users: ['barent'] To obtain client ID and client secret from Github, please follow Github documentation on how to create and register an OAuth app. Be sure to set the \"Authorization callback URL\" to the same value as redirect_url in the resource spec. Finally, create the Github connector with the command: tctl create -f github.yaml . Now, when Teleport users execute the Teleport's tsh login command, they will be prompted to login through the Github SSO and upon successful authentication, they have access to Kubernetes. # Login via Github SSO and retrieve SSH+Kubernetes certificates: $ tsh login --proxy=teleport.example.com --auth=github login # Use Kubernetes API! $ kubectl exec -ti <pod-name> The kubectl exec request will be routed through the Teleport proxy and Teleport will log the audit record and record the session. Note For more information on integrating Teleport with Github SSO, please see the Github section in the Admin Manual .","title":"Github Auth"},{"location":"kubernetes-ssh/#okta-auth","text":"With Okta (or any other SAML/OIDC/Active Directory provider), you must update Teleport's roles to include the mapping to Kubernetes groups. Let's assume you have the Teleport role called \"admin\". Add kubernetes_groups setting to it as shown below: # NOTE: the role definition is edited to remove the unnecessary fields kind : role version : v3 metadata : name : admin spec : allow : # if kubernetes integration is enabled, this setting configures which # kubernetes groups the users of this role will be assigned to. # note that you can refer to a SAML/OIDC trait via the \"external\" property bag, # this allows you to specify Kubernetes group membership in an identity manager: kubernetes_groups : [ \"system:masters\" , \"{% raw %}{{external.trait_name}}{% endraw %}\" ] ] To add kubernetes_groups setting to an existing Teleport role, you can either use the Web UI or tctl : # Dump the \"admin\" role into a file: $ tctl get roles/admin > admin.yaml # Edit the file, add kubernetes_groups setting # and then execute: $ tctl create -f admin.yaml Advanced Usage {% raw %}{{ external.trait_name }}{% endraw %} example is shown to demonstrate how to fetch the Kubernetes groups dynamically from Okta during login. In this case, you need to define Kubernetes group membership in Okta (as a trait) and use that trait name in the Teleport role. Teleport 4.3 has an option to extract the local part from an email claim. This can be helpful since some operating systems don't support the @ symbol. This means by using logins: ['{% raw %}{{email.local(external.email)}}{% endraw %}'] the resulting output will be dave.smith if the email was dave.smith@acme.com. Once setup is complete, when users execute tsh login and go through the usual Okta login sequence, their kubeconfig will be updated with their Kubernetes credentials. Note For more information on integrating Teleport with Okta, please see the Okta integration guide .","title":"Okta Auth"},{"location":"kubernetes-ssh/#using-teleport-kubernetes-with-automation","text":"Teleport can integrate with CI/CD tooling for greater visibility and auditability of these tools. For this we recommend creating a local Teleport user, then exporting a kubeconfig using tctl auth sign An example setup is below. # Create a new local user for Jenkins $ tctl users add jenkins # Option 1: Creates a token for 1 year $ tctl auth sign --user = jenkins --format = kubernetes --out = kubeconfig --ttl = 8760h # Recommended Option 2: Creates a token for 25hrs $ tctl auth sign --user = jenkins --format = kubernetes --out = kubeconfig --ttl = 25h The credentials have been written to kubeconfig $ cat kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZ.... # This kubeconfig can now be exported and will provide access to the automation tooling. # Uses kubectl to get pods, using the provided kubeconfig. $ kubectl --kubeconfig /path/to/kubeconfig get pods How long should TTL be? In the above example we've provided two options. One with 1yr (8760h) time to live and one for just 25hrs. As proponents of short lived SSH certificates we recommend the same for automation. Handling secrets is out of scope of our docs, but at a high level we recommend using providers secrets managers. Such as AWS Secrets Manager , GCP Secrets Manager , or on prem using a project like Vault . Then running a nightly job on the auth server to sign and publish a new kubeconfig. In our example, we've added 1hr, and during this time both kubeconfigs will be valid. Taking this a step further you could build a system to request a very short lived token for each CI run. We plan to make this easier for operators to integrate in the future by exposing and documenting more of our API.","title":"Using Teleport Kubernetes with Automation"},{"location":"kubernetes-ssh/#aws-eks","text":"We've a complete guide on setting up Teleport with EKS. Please see the Using Teleport with EKS Guide .","title":"AWS EKS"},{"location":"kubernetes-ssh/#multiple-kubernetes-clusters","text":"You can take advantage of the Trusted Clusters feature of Teleport to federate trust across multiple Kubernetes clusters. When multiple trusted clusters are present behind a Teleport proxy, the kubeconfig generated by tsh login will contain the Kubernetes API endpoint determined by the <cluster> argument to tsh login . For example, consider the following setup: There are three Teleport/Kubernetes clusters: \"main\", \"east\" and \"west\". These are the names set in cluster_name setting in their configuration files. The clusters \"east\" and \"west\" are trusted clusters for \"main\". Users always authenticate against \"main\" but use their certificates to access SSH nodes and Kubernetes API in all three clusters. The DNS name of the main proxy server is \"main.example.com\" In this scenario, users usually login using this command: # Using login without arguments $ tsh --proxy = main.example.com login # user's `kubeconfig` now contains one entry for the main Kubernetes # endpoint, i.e. `proxy.example.com` . # Receive a certificate for \"east\": $ tsh --proxy = main.example.com login east # user's `kubeconfig` now contains the entry for the \"east\" Kubernetes # endpoint, i.e. `east.proxy.example.com` .","title":"Multiple Kubernetes Clusters"},{"location":"metrics-logs-reference/","text":"Metrics Teleport Prometheus Endpoint Teleport provides HTTP endpoints for monitoring purposes. They are disabled by default, but you can enable them using the --diag-addr flag to teleport start : $ teleport start --diag-addr = 127 .0.0.1:3000 Now you can see the monitoring information by visiting several endpoints: http://127.0.0.1:3000/metrics is the list of internal metrics Teleport is tracking. It is compatible with Prometheus collectors. http://127.0.0.1:3000/healthz returns \"OK\" if the process is healthy or 503 otherwise. http://127.0.0.1:3000/readyz is similar to /healthz , but it returns \"OK\" only after the node successfully joined the cluster, i.e.it draws the difference between \"healthy\" and \"ready\". http://127.0.0.1:3000/debug/pprof/ is Golang's standard profiler. It's only available when -d flag is given in addition to --diag-addr Name Type Component Description backend_batch_read_requests_total counter cache Number of read requests to the backend backend_batch_read_seconds histogram cache Latency for batch read operations backend_batch_write_requests_total counter cache Number of batch write requests to the backend backend_batch_read_seconds histogram cache Latency for batch read operations backend_batch_write_requests_total counter cache Number of batch write requests to the backend backend_batch_write_seconds histogram cache Latency for backend batch write operations backend_read_requests_total counter cache Number of read requests to the backend backend_read_seconds histogram cache Latency for read operations backend_write_requests_total counter cache Number of write requests to the backend backend_write_seconds histogram cache Latency for backend write operations etcd_backend_batch_read_requests counter etcd Number of read requests to the etcd database etcd_backend_batch_read_seconds histogram etcd Latency for etcd read operations etcd_backend_read_requests counter etcd Number of read requests to the etcd database etcd_backend_read_seconds histogram etcd Latency for etcd read operations etcd_backend_tx_requests counter etcd Number of transaction requests to the database etcd_backend_tx_seconds histogram etcd Latency for etcd transaction operations etcd_backend_write_requests counter etcd Number of write requests to the database etcd_backend_write_seconds histogram etcd Latency for etcd write operations firestore_events_backend_batch_read_requests counter GCP Cloud Firestore Number of batch read requests to Cloud Firestore events firestore_events_backend_batch_read_seconds histogram GCP Cloud Firestore Latency for Cloud Firestore events batch read operations firestore_events_backend_batch_write_requests counter GCP Cloud Firestore Number of batch write requests to Cloud Firestore events firestore_events_backend_batch_write_seconds histogram GCP Cloud Firestore Latency for Cloud Firestore events batch write operations gcs_event_storage_downloads counter GCP GCS Number of downloads from the GCS backend gcs_event_storage_downloads_seconds histogram Internal GoLang Latency for GCS download operations gcs_event_storage_uploads counter Internal GoLang Number of uploads to the GCS backend gcs_event_storage_uploads_seconds histogram Internal GoLang Latency for GCS upload operations go_gc_duration_seconds summary Internal GoLang A summary of the GC invocation durations. go_goroutines gauge Internal GoLang Number of goroutines that currently exist. go_info gauge Internal GoLang Information about the Go environment. go_memstats_alloc_bytes gauge Internal GoLang Number of bytes allocated and still in use. go_memstats_alloc_bytes_total counter Internal GoLang Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge Internal GoLang Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter Internal GoLang Total number of frees. go_memstats_gc_cpu_fraction gauge Internal GoLang The fraction of this program's available CPU time used by the GC since the program started. go_memstats_gc_sys_bytes gauge Internal GoLang Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge Internal GoLang Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge Internal GoLang Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge Internal GoLang Number of heap bytes that are in use. go_memstats_heap_objects gauge Internal GoLang Number of allocated objects. go_memstats_heap_released_bytes gauge Internal GoLang Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge Internal GoLang Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge Internal GoLang Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter Internal GoLang Total number of pointer lookups. go_memstats_mallocs_total counter Internal GoLang Total number of mallocs. go_memstats_mcache_inuse_bytes gauge Internal GoLang Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge Internal GoLang Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge Internal GoLang Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge Internal GoLang Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge Internal GoLang Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge Internal GoLang Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge Internal GoLang Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge Internal GoLang Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge Internal GoLang Number of bytes obtained from system. go_threads gauge Internal GoLang Number of OS threads created. process_cpu_seconds_total counter Internal GoLang Total user and system CPU time spent in seconds. process_max_fds gauge Internal GoLang Maximum number of open file descriptors. process_open_fds gauge Internal GoLang Number of open file descriptors. process_resident_memory_bytes gauge Internal GoLang Resident memory size in bytes. process_start_time_seconds gauge Internal GoLang Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge Internal GoLang Virtual memory size in bytes. process_virtual_memory_max_bytes gauge Internal GoLang Maximum amount of virtual memory available in bytes. promhttp_metric_handler_requests_in_flight gauge prometheus Current number of scrapes being served. promhttp_metric_handler_requests_total counter prometheus Total number of scrapes by HTTP status code. reversetunnel_connected_proxies gauge Teleport Number of known proxies being sought. rx counter Teleport Number of bytes received. server_interactive_sessions_total gauge Teleport Number of active sessions trusted_clusters gauge Teleport Number of tunnels per state tx counter Teleport Number of bytes transmitted. audit_failed_disk_monitoring counter Teleport Audit Log Number of times disk monitoring failed. audit_failed_emit_events counter Teleport Audit Log Number of times emitting audit event failed. audit_percentage_disk_space_used gauge Teleport Audit Log Percentage disk space used. audit_failed_emit_events counter Teleport Audit Log Number of times emitting audit event failed. audit_percentage_disk_space_used gauge Teleport Audit Log Percentage disk space used. audit_server_open_files gauge Teleport Audit Log Number of open audit files auth_generate_requests gauge Teleport Auth Number of current generate requests auth_generate_requests_throttled_total counter Teleport Auth Number of throttled requests to generate new server keys auth_generate_requests_total counter Teleport Auth Number of requests to generate new server keys auth_generate_seconds histogram Teleport Auth Latency for generate requests","title":"Metrics"},{"location":"metrics-logs-reference/#metrics","text":"","title":"Metrics"},{"location":"metrics-logs-reference/#teleport-prometheus-endpoint","text":"Teleport provides HTTP endpoints for monitoring purposes. They are disabled by default, but you can enable them using the --diag-addr flag to teleport start : $ teleport start --diag-addr = 127 .0.0.1:3000 Now you can see the monitoring information by visiting several endpoints: http://127.0.0.1:3000/metrics is the list of internal metrics Teleport is tracking. It is compatible with Prometheus collectors. http://127.0.0.1:3000/healthz returns \"OK\" if the process is healthy or 503 otherwise. http://127.0.0.1:3000/readyz is similar to /healthz , but it returns \"OK\" only after the node successfully joined the cluster, i.e.it draws the difference between \"healthy\" and \"ready\". http://127.0.0.1:3000/debug/pprof/ is Golang's standard profiler. It's only available when -d flag is given in addition to --diag-addr Name Type Component Description backend_batch_read_requests_total counter cache Number of read requests to the backend backend_batch_read_seconds histogram cache Latency for batch read operations backend_batch_write_requests_total counter cache Number of batch write requests to the backend backend_batch_read_seconds histogram cache Latency for batch read operations backend_batch_write_requests_total counter cache Number of batch write requests to the backend backend_batch_write_seconds histogram cache Latency for backend batch write operations backend_read_requests_total counter cache Number of read requests to the backend backend_read_seconds histogram cache Latency for read operations backend_write_requests_total counter cache Number of write requests to the backend backend_write_seconds histogram cache Latency for backend write operations etcd_backend_batch_read_requests counter etcd Number of read requests to the etcd database etcd_backend_batch_read_seconds histogram etcd Latency for etcd read operations etcd_backend_read_requests counter etcd Number of read requests to the etcd database etcd_backend_read_seconds histogram etcd Latency for etcd read operations etcd_backend_tx_requests counter etcd Number of transaction requests to the database etcd_backend_tx_seconds histogram etcd Latency for etcd transaction operations etcd_backend_write_requests counter etcd Number of write requests to the database etcd_backend_write_seconds histogram etcd Latency for etcd write operations firestore_events_backend_batch_read_requests counter GCP Cloud Firestore Number of batch read requests to Cloud Firestore events firestore_events_backend_batch_read_seconds histogram GCP Cloud Firestore Latency for Cloud Firestore events batch read operations firestore_events_backend_batch_write_requests counter GCP Cloud Firestore Number of batch write requests to Cloud Firestore events firestore_events_backend_batch_write_seconds histogram GCP Cloud Firestore Latency for Cloud Firestore events batch write operations gcs_event_storage_downloads counter GCP GCS Number of downloads from the GCS backend gcs_event_storage_downloads_seconds histogram Internal GoLang Latency for GCS download operations gcs_event_storage_uploads counter Internal GoLang Number of uploads to the GCS backend gcs_event_storage_uploads_seconds histogram Internal GoLang Latency for GCS upload operations go_gc_duration_seconds summary Internal GoLang A summary of the GC invocation durations. go_goroutines gauge Internal GoLang Number of goroutines that currently exist. go_info gauge Internal GoLang Information about the Go environment. go_memstats_alloc_bytes gauge Internal GoLang Number of bytes allocated and still in use. go_memstats_alloc_bytes_total counter Internal GoLang Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge Internal GoLang Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter Internal GoLang Total number of frees. go_memstats_gc_cpu_fraction gauge Internal GoLang The fraction of this program's available CPU time used by the GC since the program started. go_memstats_gc_sys_bytes gauge Internal GoLang Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge Internal GoLang Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge Internal GoLang Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge Internal GoLang Number of heap bytes that are in use. go_memstats_heap_objects gauge Internal GoLang Number of allocated objects. go_memstats_heap_released_bytes gauge Internal GoLang Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge Internal GoLang Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge Internal GoLang Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter Internal GoLang Total number of pointer lookups. go_memstats_mallocs_total counter Internal GoLang Total number of mallocs. go_memstats_mcache_inuse_bytes gauge Internal GoLang Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge Internal GoLang Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge Internal GoLang Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge Internal GoLang Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge Internal GoLang Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge Internal GoLang Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge Internal GoLang Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge Internal GoLang Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge Internal GoLang Number of bytes obtained from system. go_threads gauge Internal GoLang Number of OS threads created. process_cpu_seconds_total counter Internal GoLang Total user and system CPU time spent in seconds. process_max_fds gauge Internal GoLang Maximum number of open file descriptors. process_open_fds gauge Internal GoLang Number of open file descriptors. process_resident_memory_bytes gauge Internal GoLang Resident memory size in bytes. process_start_time_seconds gauge Internal GoLang Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge Internal GoLang Virtual memory size in bytes. process_virtual_memory_max_bytes gauge Internal GoLang Maximum amount of virtual memory available in bytes. promhttp_metric_handler_requests_in_flight gauge prometheus Current number of scrapes being served. promhttp_metric_handler_requests_total counter prometheus Total number of scrapes by HTTP status code. reversetunnel_connected_proxies gauge Teleport Number of known proxies being sought. rx counter Teleport Number of bytes received. server_interactive_sessions_total gauge Teleport Number of active sessions trusted_clusters gauge Teleport Number of tunnels per state tx counter Teleport Number of bytes transmitted. audit_failed_disk_monitoring counter Teleport Audit Log Number of times disk monitoring failed. audit_failed_emit_events counter Teleport Audit Log Number of times emitting audit event failed. audit_percentage_disk_space_used gauge Teleport Audit Log Percentage disk space used. audit_failed_emit_events counter Teleport Audit Log Number of times emitting audit event failed. audit_percentage_disk_space_used gauge Teleport Audit Log Percentage disk space used. audit_server_open_files gauge Teleport Audit Log Number of open audit files auth_generate_requests gauge Teleport Auth Number of current generate requests auth_generate_requests_throttled_total counter Teleport Auth Number of throttled requests to generate new server keys auth_generate_requests_total counter Teleport Auth Number of requests to generate new server keys auth_generate_seconds histogram Teleport Auth Latency for generate requests","title":"Teleport Prometheus Endpoint"},{"location":"openssh-teleport/","text":"Using Teleport with OpenSSH Teleport is fully compatible with OpenSSH and can be quickly setup to record and audit all SSH activity. Using Teleport and OpenSSH has the advantage of getting you up and running, but in the long run we would recommend replacing sshd with teleport . We've outlined these reasons in OpenSSH vs Teleport SSH for Servers? Teleport is a standards-compliant SSH proxy and it can work in environments with existing SSH implementations, such as OpenSSH. This section will cover: Configuring OpenSSH server sshd to join a Teleport cluster. Existing fleets of OpenSSH servers can be configured to accept SSH certificates dynamically issued by a Teleport CA. Configuring OpenSSH client ssh to login into nodes inside a Teleport cluster. !!! warning The minimum OpenSSH version which will work with Teleport is version 6.9. You can view your OpenSSH version with `ssh -V`. Architecture The recording proxy mode, although less secure, was added to allow Teleport users to enable session recording for OpenSSH's servers running sshd , which is helpful when gradually transitioning large server fleets to Teleport. We consider the \"recording proxy mode\" to be less secure for two reasons: It grants additional privileges to the Teleport proxy. In the default \"node recording\" mode, the proxy stores no secrets and cannot \"see\" the decrypted data. This makes a proxy less critical to the security of the overall cluster. But if an attacker gains physical access to a proxy node running in the \"recording\" mode, they will be able to see the decrypted traffic and client keys stored in proxy's process memory. Recording proxy mode requires the use of SSH agent forwarding. Agent forwarding is required because without it, a proxy will not be able to establish the 2nd connection to the destination node. Teleport proxy should be available to clients, and be setup with TLS. Teleport OpenSSH supports: FQDN ec2-user@ip-172-31-14-137.us-west-2.compute.internal IPv4 ubuntu@184.45.45.30 IPv6 root@2001:db8::2 Setting up OpenSSH Recording Proxy Mode The first step is install and setup Teleport, we recommend starting with our Quickstart and admin manual . To enable session recording for sshd nodes, the cluster must be switched to \"recording proxy\" mode . In this mode, the recording will be done on the proxy level: # snippet from /etc/teleport.yaml auth_service : # Session Recording must be set to Proxy to work with OpenSSH session_recording : \"proxy\" # can also be \"off\" and \"node\" (default) Next, sshd must be told to allow users to log in with certificates generated by the Teleport User CA. Start by exporting the Teleport CA public key: Export the Teleport CA certificate into a file: # tctl needs to be ran on the auth server. $ tctl auth export --type = user > teleport_user_ca.pub To allow access for all users: Edit teleport_user_ca.pub and remove cert-authority from the start of the line. Copy teleport_user_ca.pub to /etc/ssh/teleport_user_ca.pub Update sshd configuration (usually /etc/ssh/sshd_config ) to point to this file: TrustedUserCAKeys /etc/ssh/teleport_user_ca.pub Now sshd will trust users who present a Teleport-issued certificate. The next step is to configure host authentication. When in recording mode, Teleport will check that the host certificate of any node a user connects to is signed by a Teleport CA. By default this is a strict check. If the node presents just a key, or a certificate signed by a different CA, Teleport will reject this connection with the error message saying \"ssh: handshake failed: remote host presented a public key, expected a host certificate\" You can disable strict host checks as shown below. However, this opens the possibility for Man-in-the-Middle (MITM) attacks and is not recommended. # snippet from /etc/teleport.yaml auth_service : proxy_checks_host_keys : no The recommended solution is to ask Teleport to issue valid host certificates for all OpenSSH nodes. To generate a host certificate, run this on your Teleport auth server: # Creating host certs, with an array of every host to be accessed. # Wildcard certs aren't supported by OpenSSH, must be full FQDN. # Management of the host certificates can become complex, this is another # reason we recommend using Teleport SSH on nodes. $ tctl auth sign \\ --host = api.example.com,ssh.example.com,64.225.88.175,64.225.88.178 \\ --format = openssh \\ --out = api.example.com The credentials have been written to api.example.com, api.example.com-cert.pub # You can use ssh-keygen to verify the contents. $ sudo ssh-keygen -L -f api.example.com-cert.pub #api.example.com-cert.pub: # Type: ssh-rsa-cert-v01@openssh.com host certificate # Public key: RSA-CERT SHA256:ireEc5HWFjhYPUhmztaFud7EgsopO8l+GpxNMd3wMSk # Signing CA: RSA SHA256:/6HSHsoU5u+r85M26Ut+M9gl+HventwSwrbTvP/cmvo # Key ID: \"\" # Serial: 0 # Valid: after 2020-07-29T20:26:24 # Principals: # api.example.com # ssh.example.com # 64.225.88.175 # 64.225.88.178 # Critical Options: (none) # Extensions: # x-teleport-authority UNKNOWN OPTION (len 47) # x-teleport-role UNKNOWN OPTION (len 8) Then add the following lines to /etc/ssh/sshd_config on all OpenSSH nodes, and restart sshd . HostKey /etc/ssh/api.example.com HostCertificate /etc/ssh/api.example.com-cert.pub Now you can use tsh ssh --port=22 user@api.example.com to login into any sshd node in the cluster and the session will be recorded. # tsh ssh to use default ssh port:22 $ tsh ssh --port = 22 user@host.example.com # Example for a Amazon EC2 Host # tsh ssh --port=22 ec2-user@ec2-54-EXAMPLE.us-west-2.compute.amazonaws.com If you want to use OpenSSH ssh client for logging into sshd servers behind a proxy in \"recording mode\", you have to tell the ssh client to use the jump host and enable SSH agent forwarding, otherwise a recording proxy will not be able to terminate the SSH connection to record it: # Note that agent forwarding is enabled twice: one from a client to a proxy # (mandatory if using a recording proxy), and then optionally from a proxy # to the end server if you want your agent running on the end server or not $ ssh -o \"ForwardAgent yes\" \\ -o \"ProxyCommand ssh -o 'ForwardAgent yes' -p 3023 %r@p.example.com -s proxy:%h:%p\" \\ user@host.example.com Tip To avoid typing all this and use the usual ssh user@host.example.com , users can update their ~/.ssh/config file. Setup SSH agent forwarding It's important to remember that SSH agent forwarding must be enabled on the client. Verify that a Teleport certificate is loaded into the agent after logging in: # Login as Joe $ tsh login --proxy=proxy.example.com --user=joe # see if the certificate is present (look for \"teleport:joe\") at the end of the cert $ ssh-add -L GNOME Keyring SSH Agent and GPG Agent It is well-known that Gnome Keyring SSH agent, used by many popular Linux desktops like Ubuntu, and gpg-agent from GnuPG do not support SSH certificates. We recommend using the ssh-agent from OpenSSH. Alternatively, you can disable SSH agent integration entirely using the --no-use-local-ssh-agent flag or TELEPORT_USE_LOCAL_SSH_AGENT=false environment variable with tsh . Using OpenSSH Client It is possible to use the OpenSSH client ssh to connect to nodes within a Teleport cluster. Teleport supports SSH subsystems and includes a proxy subsystem that can be used like netcat is with ProxyCommand to connect through a jump host. On your client machine, you need to import the public key of Teleport's host certificate. This will allow your OpenSSH client to verify that host certificates are signed by Teleport's trusted host CA: # on the Teleport auth server $ tctl auth export --type=host > teleport_host_ca.pub # on the machine where you want to run the ssh client $ cat teleport_host_ca.pub >> ~/.ssh/known_hosts If you have multiple Teleport clusters, you have to export and set up these certificate authorities for each cluster individually. OpenSSH and Trusted Clusters If you use recording proxy mode and trusted clusters , you need to set up the certificate authority from the root cluster to match all nodes, even those that belong to leaf clusters. For example, if your node naming scheme is *.root.example.com , *.leaf1.example.com , *.leaf2.example.com , then the @certificate-authority entry should match *.example.com and use the CA from the root auth server only. Make sure you are running OpenSSH's ssh-agent , and have logged in to the Teleport proxy: $ eval ` ssh-agent ` $ tsh --proxy = root.example.com login ssh-agent will print environment variables into the console. Either eval the output as in the example above, or copy and paste the output into the shell you will be using to connect to a Teleport node. The output exports the SSH_AUTH_SOCK and SSH_AGENT_PID environment variables that allow OpenSSH clients to find the SSH agent. Lastly, configure the OpenSSH client to use the Teleport proxy when connecting to nodes with matching names. Edit ~/.ssh/config for your user or /etc/ssh/ssh_config for global changes: # root.example.com is the jump host (proxy). credentials will be obtained from the # openssh agent. Host root.example.com HostName 192 .168.1.2 Port 3023 # connect to nodes in the root.example.com cluster through the jump # host (proxy) using the same. credentials will be obtained from the # openssh agent. Host *.root.example.com HostName %h Port 3022 ProxyCommand ssh -p 3023 %r@root.example.com -s proxy:%h:%p # when connecting to a node within a trusted cluster with name \"leaf1.example.com\", # add the name of the cluster to the invocation of the proxy subsystem. Host *.leaf1.example.com HostName %h Port 3022 ProxyCommand ssh -p 3023 %r@root.example.com -s proxy:%h:%p@leaf1.example.com When everything is configured properly, you can use ssh to connect to any node behind root.example.com : $ ssh root@database.root.example.com Note Teleport uses OpenSSH certificates instead of keys which means you cannot ordinarily connect to a Teleport node by IP address. You have to connect by DNS name. This is because OpenSSH ensures the DNS name of the node you are connecting is listed under the Principals section of the OpenSSH certificate to verify you are connecting to the correct node. To connect to the OpenSSH server via tsh , add --port=<ssh port> with the tsh ssh command: Example ssh to database.work.example.com as root with a OpenSSH server on port 22 via tsh : tsh ssh --port = 22 dev@database.root.example.com Warning The principal/username ( dev@ in the example above) being used to connect must be listed in the Teleport user/role configuration. OpenSSH Rate Limiting When using a Teleport proxy in \"recording mode\", be aware of OpenSSH built-in rate limiting. On large numbers of proxy connections you may encounter errors like: channel 0 : open failed: connect failed: ssh: handshake failed: EOF See MaxStartups setting in man sshd_config . This setting means that by default OpenSSH only allows 10 unauthenticated connections at a time and starts dropping connections 30% of the time when the number of connections goes over 10. When it hits 100 authentication connections, all new connections are dropped. To increase the concurrency level, increase the value to something like MaxStartups 50:30:100 . This allows 50 concurrent connections and a max of 100. Revoke a SSH Certificate To revoke the current Teleport CA and generate a new one, run tctl auth rotate . Unless you've highly automated your infrastructure, we would suggest you proceed with caution as this will invalidate the user and host CAs, meaning that the new CAs will need to be exported to every OpenSSH-based machine again using tctl auth export as above.","title":"OpenSSH Guide"},{"location":"openssh-teleport/#using-teleport-with-openssh","text":"Teleport is fully compatible with OpenSSH and can be quickly setup to record and audit all SSH activity. Using Teleport and OpenSSH has the advantage of getting you up and running, but in the long run we would recommend replacing sshd with teleport . We've outlined these reasons in OpenSSH vs Teleport SSH for Servers? Teleport is a standards-compliant SSH proxy and it can work in environments with existing SSH implementations, such as OpenSSH. This section will cover: Configuring OpenSSH server sshd to join a Teleport cluster. Existing fleets of OpenSSH servers can be configured to accept SSH certificates dynamically issued by a Teleport CA. Configuring OpenSSH client ssh to login into nodes inside a Teleport cluster. !!! warning The minimum OpenSSH version which will work with Teleport is version 6.9. You can view your OpenSSH version with `ssh -V`.","title":"Using Teleport with OpenSSH"},{"location":"openssh-teleport/#architecture","text":"The recording proxy mode, although less secure, was added to allow Teleport users to enable session recording for OpenSSH's servers running sshd , which is helpful when gradually transitioning large server fleets to Teleport. We consider the \"recording proxy mode\" to be less secure for two reasons: It grants additional privileges to the Teleport proxy. In the default \"node recording\" mode, the proxy stores no secrets and cannot \"see\" the decrypted data. This makes a proxy less critical to the security of the overall cluster. But if an attacker gains physical access to a proxy node running in the \"recording\" mode, they will be able to see the decrypted traffic and client keys stored in proxy's process memory. Recording proxy mode requires the use of SSH agent forwarding. Agent forwarding is required because without it, a proxy will not be able to establish the 2nd connection to the destination node. Teleport proxy should be available to clients, and be setup with TLS. Teleport OpenSSH supports: FQDN ec2-user@ip-172-31-14-137.us-west-2.compute.internal IPv4 ubuntu@184.45.45.30 IPv6 root@2001:db8::2","title":"Architecture"},{"location":"openssh-teleport/#setting-up-openssh-recording-proxy-mode","text":"The first step is install and setup Teleport, we recommend starting with our Quickstart and admin manual . To enable session recording for sshd nodes, the cluster must be switched to \"recording proxy\" mode . In this mode, the recording will be done on the proxy level: # snippet from /etc/teleport.yaml auth_service : # Session Recording must be set to Proxy to work with OpenSSH session_recording : \"proxy\" # can also be \"off\" and \"node\" (default) Next, sshd must be told to allow users to log in with certificates generated by the Teleport User CA. Start by exporting the Teleport CA public key: Export the Teleport CA certificate into a file: # tctl needs to be ran on the auth server. $ tctl auth export --type = user > teleport_user_ca.pub To allow access for all users: Edit teleport_user_ca.pub and remove cert-authority from the start of the line. Copy teleport_user_ca.pub to /etc/ssh/teleport_user_ca.pub Update sshd configuration (usually /etc/ssh/sshd_config ) to point to this file: TrustedUserCAKeys /etc/ssh/teleport_user_ca.pub Now sshd will trust users who present a Teleport-issued certificate. The next step is to configure host authentication. When in recording mode, Teleport will check that the host certificate of any node a user connects to is signed by a Teleport CA. By default this is a strict check. If the node presents just a key, or a certificate signed by a different CA, Teleport will reject this connection with the error message saying \"ssh: handshake failed: remote host presented a public key, expected a host certificate\" You can disable strict host checks as shown below. However, this opens the possibility for Man-in-the-Middle (MITM) attacks and is not recommended. # snippet from /etc/teleport.yaml auth_service : proxy_checks_host_keys : no The recommended solution is to ask Teleport to issue valid host certificates for all OpenSSH nodes. To generate a host certificate, run this on your Teleport auth server: # Creating host certs, with an array of every host to be accessed. # Wildcard certs aren't supported by OpenSSH, must be full FQDN. # Management of the host certificates can become complex, this is another # reason we recommend using Teleport SSH on nodes. $ tctl auth sign \\ --host = api.example.com,ssh.example.com,64.225.88.175,64.225.88.178 \\ --format = openssh \\ --out = api.example.com The credentials have been written to api.example.com, api.example.com-cert.pub # You can use ssh-keygen to verify the contents. $ sudo ssh-keygen -L -f api.example.com-cert.pub #api.example.com-cert.pub: # Type: ssh-rsa-cert-v01@openssh.com host certificate # Public key: RSA-CERT SHA256:ireEc5HWFjhYPUhmztaFud7EgsopO8l+GpxNMd3wMSk # Signing CA: RSA SHA256:/6HSHsoU5u+r85M26Ut+M9gl+HventwSwrbTvP/cmvo # Key ID: \"\" # Serial: 0 # Valid: after 2020-07-29T20:26:24 # Principals: # api.example.com # ssh.example.com # 64.225.88.175 # 64.225.88.178 # Critical Options: (none) # Extensions: # x-teleport-authority UNKNOWN OPTION (len 47) # x-teleport-role UNKNOWN OPTION (len 8) Then add the following lines to /etc/ssh/sshd_config on all OpenSSH nodes, and restart sshd . HostKey /etc/ssh/api.example.com HostCertificate /etc/ssh/api.example.com-cert.pub Now you can use tsh ssh --port=22 user@api.example.com to login into any sshd node in the cluster and the session will be recorded. # tsh ssh to use default ssh port:22 $ tsh ssh --port = 22 user@host.example.com # Example for a Amazon EC2 Host # tsh ssh --port=22 ec2-user@ec2-54-EXAMPLE.us-west-2.compute.amazonaws.com If you want to use OpenSSH ssh client for logging into sshd servers behind a proxy in \"recording mode\", you have to tell the ssh client to use the jump host and enable SSH agent forwarding, otherwise a recording proxy will not be able to terminate the SSH connection to record it: # Note that agent forwarding is enabled twice: one from a client to a proxy # (mandatory if using a recording proxy), and then optionally from a proxy # to the end server if you want your agent running on the end server or not $ ssh -o \"ForwardAgent yes\" \\ -o \"ProxyCommand ssh -o 'ForwardAgent yes' -p 3023 %r@p.example.com -s proxy:%h:%p\" \\ user@host.example.com Tip To avoid typing all this and use the usual ssh user@host.example.com , users can update their ~/.ssh/config file.","title":"Setting up OpenSSH Recording Proxy Mode"},{"location":"openssh-teleport/#setup-ssh-agent-forwarding","text":"It's important to remember that SSH agent forwarding must be enabled on the client. Verify that a Teleport certificate is loaded into the agent after logging in: # Login as Joe $ tsh login --proxy=proxy.example.com --user=joe # see if the certificate is present (look for \"teleport:joe\") at the end of the cert $ ssh-add -L GNOME Keyring SSH Agent and GPG Agent It is well-known that Gnome Keyring SSH agent, used by many popular Linux desktops like Ubuntu, and gpg-agent from GnuPG do not support SSH certificates. We recommend using the ssh-agent from OpenSSH. Alternatively, you can disable SSH agent integration entirely using the --no-use-local-ssh-agent flag or TELEPORT_USE_LOCAL_SSH_AGENT=false environment variable with tsh .","title":"Setup SSH agent forwarding"},{"location":"openssh-teleport/#using-openssh-client","text":"It is possible to use the OpenSSH client ssh to connect to nodes within a Teleport cluster. Teleport supports SSH subsystems and includes a proxy subsystem that can be used like netcat is with ProxyCommand to connect through a jump host. On your client machine, you need to import the public key of Teleport's host certificate. This will allow your OpenSSH client to verify that host certificates are signed by Teleport's trusted host CA: # on the Teleport auth server $ tctl auth export --type=host > teleport_host_ca.pub # on the machine where you want to run the ssh client $ cat teleport_host_ca.pub >> ~/.ssh/known_hosts If you have multiple Teleport clusters, you have to export and set up these certificate authorities for each cluster individually. OpenSSH and Trusted Clusters If you use recording proxy mode and trusted clusters , you need to set up the certificate authority from the root cluster to match all nodes, even those that belong to leaf clusters. For example, if your node naming scheme is *.root.example.com , *.leaf1.example.com , *.leaf2.example.com , then the @certificate-authority entry should match *.example.com and use the CA from the root auth server only. Make sure you are running OpenSSH's ssh-agent , and have logged in to the Teleport proxy: $ eval ` ssh-agent ` $ tsh --proxy = root.example.com login ssh-agent will print environment variables into the console. Either eval the output as in the example above, or copy and paste the output into the shell you will be using to connect to a Teleport node. The output exports the SSH_AUTH_SOCK and SSH_AGENT_PID environment variables that allow OpenSSH clients to find the SSH agent. Lastly, configure the OpenSSH client to use the Teleport proxy when connecting to nodes with matching names. Edit ~/.ssh/config for your user or /etc/ssh/ssh_config for global changes: # root.example.com is the jump host (proxy). credentials will be obtained from the # openssh agent. Host root.example.com HostName 192 .168.1.2 Port 3023 # connect to nodes in the root.example.com cluster through the jump # host (proxy) using the same. credentials will be obtained from the # openssh agent. Host *.root.example.com HostName %h Port 3022 ProxyCommand ssh -p 3023 %r@root.example.com -s proxy:%h:%p # when connecting to a node within a trusted cluster with name \"leaf1.example.com\", # add the name of the cluster to the invocation of the proxy subsystem. Host *.leaf1.example.com HostName %h Port 3022 ProxyCommand ssh -p 3023 %r@root.example.com -s proxy:%h:%p@leaf1.example.com When everything is configured properly, you can use ssh to connect to any node behind root.example.com : $ ssh root@database.root.example.com Note Teleport uses OpenSSH certificates instead of keys which means you cannot ordinarily connect to a Teleport node by IP address. You have to connect by DNS name. This is because OpenSSH ensures the DNS name of the node you are connecting is listed under the Principals section of the OpenSSH certificate to verify you are connecting to the correct node. To connect to the OpenSSH server via tsh , add --port=<ssh port> with the tsh ssh command: Example ssh to database.work.example.com as root with a OpenSSH server on port 22 via tsh : tsh ssh --port = 22 dev@database.root.example.com Warning The principal/username ( dev@ in the example above) being used to connect must be listed in the Teleport user/role configuration.","title":"Using OpenSSH Client"},{"location":"openssh-teleport/#openssh-rate-limiting","text":"When using a Teleport proxy in \"recording mode\", be aware of OpenSSH built-in rate limiting. On large numbers of proxy connections you may encounter errors like: channel 0 : open failed: connect failed: ssh: handshake failed: EOF See MaxStartups setting in man sshd_config . This setting means that by default OpenSSH only allows 10 unauthenticated connections at a time and starts dropping connections 30% of the time when the number of connections goes over 10. When it hits 100 authentication connections, all new connections are dropped. To increase the concurrency level, increase the value to something like MaxStartups 50:30:100 . This allows 50 concurrent connections and a max of 100.","title":"OpenSSH Rate Limiting"},{"location":"openssh-teleport/#revoke-a-ssh-certificate","text":"To revoke the current Teleport CA and generate a new one, run tctl auth rotate . Unless you've highly automated your infrastructure, we would suggest you proceed with caution as this will invalidate the user and host CAs, meaning that the new CAs will need to be exported to every OpenSSH-based machine again using tctl auth export as above.","title":"Revoke a SSH Certificate"},{"location":"production/","text":"Production Guide This guide provides more in-depth details for running Teleport in Production. Prerequisites Read the Architecture Overview Read through the Installation Guide to see the available packages and binaries available. Read the CLI Docs for teleport Designing Your Cluster Before installing anything there are a few things you should think about. Where will you host Teleport? On-premises Cloud VMs such as AWS EC2 or GCE An existing Kubernetes Cluster What does your existing network configuration look like? Are you able to administer the network firewall rules yourself or do you need to work with a network admin? Are these nodes accessible to the public Internet or behind NAT? Which users, ( Roles or ClusterRoles on k8s) are set up on the existing system? Can you add new users or Roles yourself or do you need to work with a system administrator? Firewall Configuration Teleport services listen on several ports. This table shows the default port numbers. Port Service Description Ingress Egress 3080 Proxy HTTPS port clients connect to. Used to authenticate tsh users and web users into the cluster. Allow inbound connections from HTTP and SSH clients. Allow outbound connections to HTTP and SSH clients. 3023 Proxy SSH port clients connect to after authentication. A proxy will forward this connection to port 3022 on the destination node. Allow inbound traffic from SSH clients. Allow outbound traffic to SSH clients. 3022 Node SSH port to the Node Service. This is Teleport's equivalent of port 22 for SSH. Allow inbound traffic from proxy host. Allow outbound traffic to the proxy host. 3025 Auth SSH port used by the Auth Service to serve its Auth API to other nodes in a cluster. Allow inbound connections from all cluster nodes. Allow outbound traffic to cluster nodes. 3024 Proxy SSH port used to create \"reverse SSH tunnels\" from behind-firewall environments into a trusted proxy server. Installation We have a detailed installation guide which shows how to install all available binaries or install from source . Reference that guide to learn the best way to install Teleport for your system and the come back here to finish your production install. Filesystem Layout By default a Teleport node has the following files present. The location of all of them is configurable. Default path Purpose /etc/teleport.yaml Teleport configuration file. /usr/local/bin/teleport Teleport daemon binary. /usr/local/bin/tctl Teleport admin tool. It is only needed for auth servers. /usr/local/bin/tsh Teleport CLI client tool. It is needed on any node that needs to connect to the cluster. /var/lib/teleport Teleport data directory. Nodes keep their keys and certificates there. Auth servers store the audit log and the cluster keys there, but the audit log storage can be further configured via auth_service section in the config file. Running Teleport in Production Systemd Unit File In production, we recommend starting teleport daemon via an init system like systemd . If systemd and unit files are new to you, check out this helpful guide . Here's an example systemd unit file for the Teleport Proxy, Node and Auth Service . There are a couple of important things to notice about this file: The start command in the unit file specifies --config as a file and there are very few flags passed to the teleport binary. Most of the configuration for Teleport should be done in the configuration file . The ExecReload command allows admins to run systemctl reload teleport . This will attempt to perform a graceful restart of Teleport but it only works if network-based backend storage like DynamoDB or etc 3.3 is configured . Graceful Restarts will fork a new process to handle new incoming requests and leave the old daemon process running until existing clients disconnect. Start the Teleport Service You can start Teleport via systemd unit by enabling the .service file with the systemctl tool. # sudo cp teleport/examples/systemd/teleport.service /etc/systemd/system $ cd /etc/systemd/system # Use your text editor of choice to create the .service file # Here we use vim $ vi teleport.service # use the file linked above as is, or customize as needed # save the file $ systemctl enable teleport $ systemctl start teleport # show the status of the unit $ systemctl status teleport # follow tail of service logs $ journalctl -fu teleport # If you modify teleport.service later you will need to # reload the systemctl daemon and reload teleport # to apply your changes $ systemctl daemon-reload $ systemctl reload teleport You can also perform restarts or upgrades by sending kill signals to a Teleport daemon manually. Signal Teleport Daemon Behavior USR1 Dumps diagnostics/debugging information into syslog. TERM , INT or KILL Immediate non-graceful shutdown. All existing connections will be dropped. USR2 Forks a new Teleport daemon to serve new connections. HUP Forks a new Teleport daemon to serve new connections and initiates the graceful shutdown of the existing process when there are no more clients connected to it. This is the signal sent to trigger a graceful restart. Adding Nodes to the Cluster We've written a dedicated guide on Adding Nodes to your Cluster which shows how to generate or set join tokens and use them to add nodes. Security Considerations SSL/TLS for Teleport Proxy TLS stands for Transport Layer Security (TLS), and its now-deprecated predecessor, Secure Sockets Layer (SSL). Teleport requires TLS authentication to ensure that communication between nodes, clients and web proxy remains secure and comes from a trusted source. During our quickstart guide we skip over setting up TLS so that you can quickly try Teleport. Obtaining a TLS certificate is easy and is free with thanks to Let's Encrypt . If you use certbot , you get this list of files provided: README cert.pem chain.pem fullchain.pem privkey.pem The files that are needed for Teleport are these: https_key_file: /path/to/certs/privkey.pem https_cert_file: /path/to/certs/fullchain.pem If you already have a certificate these should be uploaded to the Teleport Proxy and can be set via https_key_file and https_cert_file . Make sure any certificates files uploaded contain a full certificate chain, complete with any intermediate certificates required - this guide may help. # This section configures the 'proxy service' proxy_service : # Turns 'proxy' role on. Default is 'yes' enabled : yes # The DNS name the proxy HTTPS endpoint as accessible by cluster users. # Defaults to the proxy's hostname if not specified. If running multiple # proxies behind a load balancer, this name must point to the load balancer # (see public_addr section below) public_addr : proxy.example.com:3080 # TLS certificate for the HTTPS connection. Configuring these properly is # critical for Teleport security. https_key_file : /var/lib/teleport/webproxy_key.pem https_cert_file : /var/lib/teleport/webproxy_cert.pem When setting up on Teleport on AWS or GCP, we recommend leveraging their certificate managers. ACM on AWS Google-managed SSL certificates on GCP When setting up Teleport with a Cloud Provider, it can be common to terminate TLS at the load balancer, then use an autoscaling group for the proxy nodes. When setting up the proxy nodes start Teleport with: teleport start --insecure --roles=proxy --config=/etc/teleport.yaml See Teleport Proxy HA for more info. CA Pinning Teleport nodes use the HTTPS protocol to offer the join tokens to the auth server. In a zero-trust environment, you must assume that an attacker can hijack the IP address of the auth server. To prevent this from happening, you need to supply every new node with an additional bit of information about the auth server. This technique is called \"CA pinning\". It works by asking the auth server to produce a \"CA pin\", which is a hashed value of its private key, i.e. it cannot be forged by an attacker. To get the current CA pin run this on the auth server: $ tctl status Cluster staging.example.com User CA never updated Host CA never updated CA pin sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1 The CA pin at the bottom needs to be passed to the new nodes when they're starting for the first time, i.e. when they join a cluster: Via CLI: $ teleport start \\ --roles = node \\ --token = 1ac590d36493acdaa2387bc1c492db1a \\ --ca-pin = sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1 \\ --auth-server = 10 .12.0.6:3025 or via /etc/teleport.yaml on a node: teleport : auth_token : \"1ac590d36493acdaa2387bc1c492db1a\" ca_pin : \"sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1\" auth_servers : - \"10.12.0.6:3025\" Warning If a CA pin is not provided, Teleport node will join a cluster but it will print a WARN message (warning) into its standard error output. Warning The CA pin becomes invalid if a Teleport administrator performs the CA rotation by executing tctl auth rotate . Secure Data Storage By default the teleport daemon uses the local directory /var/lib/teleport to store its data. This applies to any role or service including Auth, Node, or Proxy. While an Auth node hosts the most sensitive data you will want to prevent unauthorized access to this directory. Make sure that regular/non-admin users do not have access to this folder, particularly on the Auth server. Change the ownership of the directory with chown $ sudo teleport start If you are logged in as root you may want to create a new OS-level user first. On Linux, create a new user called <username> with the following commands: $ adduser <username> $ su <username>","title":"Production Guide"},{"location":"production/#production-guide","text":"This guide provides more in-depth details for running Teleport in Production.","title":"Production Guide"},{"location":"production/#prerequisites","text":"Read the Architecture Overview Read through the Installation Guide to see the available packages and binaries available. Read the CLI Docs for teleport","title":"Prerequisites"},{"location":"production/#designing-your-cluster","text":"Before installing anything there are a few things you should think about. Where will you host Teleport? On-premises Cloud VMs such as AWS EC2 or GCE An existing Kubernetes Cluster What does your existing network configuration look like? Are you able to administer the network firewall rules yourself or do you need to work with a network admin? Are these nodes accessible to the public Internet or behind NAT? Which users, ( Roles or ClusterRoles on k8s) are set up on the existing system? Can you add new users or Roles yourself or do you need to work with a system administrator?","title":"Designing Your Cluster"},{"location":"production/#firewall-configuration","text":"Teleport services listen on several ports. This table shows the default port numbers. Port Service Description Ingress Egress 3080 Proxy HTTPS port clients connect to. Used to authenticate tsh users and web users into the cluster. Allow inbound connections from HTTP and SSH clients. Allow outbound connections to HTTP and SSH clients. 3023 Proxy SSH port clients connect to after authentication. A proxy will forward this connection to port 3022 on the destination node. Allow inbound traffic from SSH clients. Allow outbound traffic to SSH clients. 3022 Node SSH port to the Node Service. This is Teleport's equivalent of port 22 for SSH. Allow inbound traffic from proxy host. Allow outbound traffic to the proxy host. 3025 Auth SSH port used by the Auth Service to serve its Auth API to other nodes in a cluster. Allow inbound connections from all cluster nodes. Allow outbound traffic to cluster nodes. 3024 Proxy SSH port used to create \"reverse SSH tunnels\" from behind-firewall environments into a trusted proxy server.","title":"Firewall Configuration"},{"location":"production/#installation","text":"We have a detailed installation guide which shows how to install all available binaries or install from source . Reference that guide to learn the best way to install Teleport for your system and the come back here to finish your production install.","title":"Installation"},{"location":"production/#filesystem-layout","text":"By default a Teleport node has the following files present. The location of all of them is configurable. Default path Purpose /etc/teleport.yaml Teleport configuration file. /usr/local/bin/teleport Teleport daemon binary. /usr/local/bin/tctl Teleport admin tool. It is only needed for auth servers. /usr/local/bin/tsh Teleport CLI client tool. It is needed on any node that needs to connect to the cluster. /var/lib/teleport Teleport data directory. Nodes keep their keys and certificates there. Auth servers store the audit log and the cluster keys there, but the audit log storage can be further configured via auth_service section in the config file.","title":"Filesystem Layout"},{"location":"production/#running-teleport-in-production","text":"","title":"Running Teleport in Production"},{"location":"production/#systemd-unit-file","text":"In production, we recommend starting teleport daemon via an init system like systemd . If systemd and unit files are new to you, check out this helpful guide . Here's an example systemd unit file for the Teleport Proxy, Node and Auth Service . There are a couple of important things to notice about this file: The start command in the unit file specifies --config as a file and there are very few flags passed to the teleport binary. Most of the configuration for Teleport should be done in the configuration file . The ExecReload command allows admins to run systemctl reload teleport . This will attempt to perform a graceful restart of Teleport but it only works if network-based backend storage like DynamoDB or etc 3.3 is configured . Graceful Restarts will fork a new process to handle new incoming requests and leave the old daemon process running until existing clients disconnect.","title":"Systemd Unit File"},{"location":"production/#start-the-teleport-service","text":"You can start Teleport via systemd unit by enabling the .service file with the systemctl tool. # sudo cp teleport/examples/systemd/teleport.service /etc/systemd/system $ cd /etc/systemd/system # Use your text editor of choice to create the .service file # Here we use vim $ vi teleport.service # use the file linked above as is, or customize as needed # save the file $ systemctl enable teleport $ systemctl start teleport # show the status of the unit $ systemctl status teleport # follow tail of service logs $ journalctl -fu teleport # If you modify teleport.service later you will need to # reload the systemctl daemon and reload teleport # to apply your changes $ systemctl daemon-reload $ systemctl reload teleport You can also perform restarts or upgrades by sending kill signals to a Teleport daemon manually. Signal Teleport Daemon Behavior USR1 Dumps diagnostics/debugging information into syslog. TERM , INT or KILL Immediate non-graceful shutdown. All existing connections will be dropped. USR2 Forks a new Teleport daemon to serve new connections. HUP Forks a new Teleport daemon to serve new connections and initiates the graceful shutdown of the existing process when there are no more clients connected to it. This is the signal sent to trigger a graceful restart.","title":"Start the Teleport Service"},{"location":"production/#adding-nodes-to-the-cluster","text":"We've written a dedicated guide on Adding Nodes to your Cluster which shows how to generate or set join tokens and use them to add nodes.","title":"Adding Nodes to the Cluster"},{"location":"production/#security-considerations","text":"","title":"Security Considerations"},{"location":"production/#ssltls-for-teleport-proxy","text":"TLS stands for Transport Layer Security (TLS), and its now-deprecated predecessor, Secure Sockets Layer (SSL). Teleport requires TLS authentication to ensure that communication between nodes, clients and web proxy remains secure and comes from a trusted source. During our quickstart guide we skip over setting up TLS so that you can quickly try Teleport. Obtaining a TLS certificate is easy and is free with thanks to Let's Encrypt . If you use certbot , you get this list of files provided: README cert.pem chain.pem fullchain.pem privkey.pem The files that are needed for Teleport are these: https_key_file: /path/to/certs/privkey.pem https_cert_file: /path/to/certs/fullchain.pem If you already have a certificate these should be uploaded to the Teleport Proxy and can be set via https_key_file and https_cert_file . Make sure any certificates files uploaded contain a full certificate chain, complete with any intermediate certificates required - this guide may help. # This section configures the 'proxy service' proxy_service : # Turns 'proxy' role on. Default is 'yes' enabled : yes # The DNS name the proxy HTTPS endpoint as accessible by cluster users. # Defaults to the proxy's hostname if not specified. If running multiple # proxies behind a load balancer, this name must point to the load balancer # (see public_addr section below) public_addr : proxy.example.com:3080 # TLS certificate for the HTTPS connection. Configuring these properly is # critical for Teleport security. https_key_file : /var/lib/teleport/webproxy_key.pem https_cert_file : /var/lib/teleport/webproxy_cert.pem When setting up on Teleport on AWS or GCP, we recommend leveraging their certificate managers. ACM on AWS Google-managed SSL certificates on GCP When setting up Teleport with a Cloud Provider, it can be common to terminate TLS at the load balancer, then use an autoscaling group for the proxy nodes. When setting up the proxy nodes start Teleport with: teleport start --insecure --roles=proxy --config=/etc/teleport.yaml See Teleport Proxy HA for more info.","title":"SSL/TLS for Teleport Proxy"},{"location":"production/#ca-pinning","text":"Teleport nodes use the HTTPS protocol to offer the join tokens to the auth server. In a zero-trust environment, you must assume that an attacker can hijack the IP address of the auth server. To prevent this from happening, you need to supply every new node with an additional bit of information about the auth server. This technique is called \"CA pinning\". It works by asking the auth server to produce a \"CA pin\", which is a hashed value of its private key, i.e. it cannot be forged by an attacker. To get the current CA pin run this on the auth server: $ tctl status Cluster staging.example.com User CA never updated Host CA never updated CA pin sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1 The CA pin at the bottom needs to be passed to the new nodes when they're starting for the first time, i.e. when they join a cluster: Via CLI: $ teleport start \\ --roles = node \\ --token = 1ac590d36493acdaa2387bc1c492db1a \\ --ca-pin = sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1 \\ --auth-server = 10 .12.0.6:3025 or via /etc/teleport.yaml on a node: teleport : auth_token : \"1ac590d36493acdaa2387bc1c492db1a\" ca_pin : \"sha256:7e12c17c20d9cb504bbcb3f0236be3f446861f1396dcbb44425fe28ec1c108f1\" auth_servers : - \"10.12.0.6:3025\" Warning If a CA pin is not provided, Teleport node will join a cluster but it will print a WARN message (warning) into its standard error output. Warning The CA pin becomes invalid if a Teleport administrator performs the CA rotation by executing tctl auth rotate .","title":"CA Pinning"},{"location":"production/#secure-data-storage","text":"By default the teleport daemon uses the local directory /var/lib/teleport to store its data. This applies to any role or service including Auth, Node, or Proxy. While an Auth node hosts the most sensitive data you will want to prevent unauthorized access to this directory. Make sure that regular/non-admin users do not have access to this folder, particularly on the Auth server. Change the ownership of the directory with chown $ sudo teleport start If you are logged in as root you may want to create a new OS-level user first. On Linux, create a new user called <username> with the following commands: $ adduser <username> $ su <username>","title":"Secure Data Storage"},{"location":"quickstart-docker/","text":"Run Teleport using Docker We provide pre-built Docker images for every version of Teleport. These images are hosted on quay.io. All tags under quay.io/gravitational/teleport are Teleport Community images We currently only offer Docker images for x86_64 architectures. Note You will need a recent version of Docker installed to follow this section of the quick start guide. Warning This setup will not let you 'SSH into' the node that is running Teleport without additional configuration. Pick your image This table gives an idea of how our image naming scheme works. We offer images which point to a static version of Teleport, as well as images which are automatically rebuilt every night. These nightly images point to the latest version of Teleport from the three most recent release branches. They are stable, and we recommend their use to easily keep your Teleport installation up to date. Image name Teleport version Image automatically updated? Image base quay.io/gravitational/teleport:{{ version }} The latest version of Teleport Community {{ version }} Yes Ubuntu 20.04 quay.io/gravitational/teleport:{{ teleport.version }} The version specified in the image's tag (i.e. {{ teleport.version }}) No Ubuntu 20.04 For testing, we always recommend that you use the latest release version of Teleport, which is currently {{teleport.latest_oss_docker_image}} . Quickstart using docker-compose Note You will need a recent version of docker-compose installed to follow this section of the quick start guide. The easiest way to start Teleport quickly is to use docker-compose with our teleport-quickstart.yml file: # download the quickstart file from our Github repo curl -Lso teleport-quickstart.yml https://raw.githubusercontent.com/gravitational/teleport/master/docker/teleport-quickstart.yml # start teleport quickstart using docker-compose docker-compose -f teleport-quickstart.yml up The docker-compose quickstart will automatically create a config file for you at ./docker/teleport/config/teleport.yaml This config is mounted into the container under /etc/teleport/teleport.yaml It will also start teleport using this config file, with Teleport's data directory set to ./docker/teleport/data and mounted under /var/lib/teleport By default, docker-compose will output Teleport's logs to the console for you to observe. If you would rather run the Teleport container in the background, use docker-compose -f teleport-quickstart.yml up -d You can stop the Teleport container using docker-compose -f teleport-quickstart.yml down Quickstart using docker run If you'd prefer to complete these steps manually, here's some sample docker run commands: # create local config and data directories for teleport, which will be mounted into the container mkdir -p ~/teleport/config ~/teleport/data # generate a sample teleport config and write it to the local config directory # this container will write the config and immediately exit - this is expected docker run --hostname localhost --rm \\ --entrypoint = /bin/sh \\ -v ~/teleport/config:/etc/teleport \\ {{ teleport.latest_oss_docker_image }} -c \"teleport configure > /etc/teleport/teleport.yaml\" # start teleport with mounted config and data directories, plus all ports docker run --hostname localhost --name teleport \\ -v ~/teleport/config:/etc/teleport \\ -v ~/teleport/data:/var/lib/teleport \\ -p 3023 :3023 -p 3025 :3025 -p 3080 :3080 \\ {{ teleport.latest_oss_docker_image }} Creating a Teleport user when using Docker quickstart To create a user inside your Teleport container, use docker exec . This example command will create a Teleport user called testuser which is allowed to log in as either OS user root or ubuntu . Feel free to change these to suit your needs - there are more instructions above in Step 3 if you'd like additional details): docker exec teleport tctl users add testuser root,ubuntu When you run this command, Teleport will output a URL which you must open to complete the user signup process: User testuser has been created but requires a password. Share this URL with the user to complete user setup, link is valid for 1h0m0s: https://localhost:3080/web/invite/4f2718a52ce107568b191f222ba069f7 NOTE: Make sure localhost:3080 points at a Teleport proxy which users can access. You can now follow this guide from Step 4 onwards to create your user and log into Teleport.","title":"Teleport Quick Start guide with Docker"},{"location":"quickstart-docker/#run-teleport-using-docker","text":"We provide pre-built Docker images for every version of Teleport. These images are hosted on quay.io. All tags under quay.io/gravitational/teleport are Teleport Community images We currently only offer Docker images for x86_64 architectures. Note You will need a recent version of Docker installed to follow this section of the quick start guide. Warning This setup will not let you 'SSH into' the node that is running Teleport without additional configuration.","title":"Run Teleport using Docker"},{"location":"quickstart-docker/#pick-your-image","text":"This table gives an idea of how our image naming scheme works. We offer images which point to a static version of Teleport, as well as images which are automatically rebuilt every night. These nightly images point to the latest version of Teleport from the three most recent release branches. They are stable, and we recommend their use to easily keep your Teleport installation up to date. Image name Teleport version Image automatically updated? Image base quay.io/gravitational/teleport:{{ version }} The latest version of Teleport Community {{ version }} Yes Ubuntu 20.04 quay.io/gravitational/teleport:{{ teleport.version }} The version specified in the image's tag (i.e. {{ teleport.version }}) No Ubuntu 20.04 For testing, we always recommend that you use the latest release version of Teleport, which is currently {{teleport.latest_oss_docker_image}} .","title":"Pick your image"},{"location":"quickstart-docker/#quickstart-using-docker-compose","text":"Note You will need a recent version of docker-compose installed to follow this section of the quick start guide. The easiest way to start Teleport quickly is to use docker-compose with our teleport-quickstart.yml file: # download the quickstart file from our Github repo curl -Lso teleport-quickstart.yml https://raw.githubusercontent.com/gravitational/teleport/master/docker/teleport-quickstart.yml # start teleport quickstart using docker-compose docker-compose -f teleport-quickstart.yml up The docker-compose quickstart will automatically create a config file for you at ./docker/teleport/config/teleport.yaml This config is mounted into the container under /etc/teleport/teleport.yaml It will also start teleport using this config file, with Teleport's data directory set to ./docker/teleport/data and mounted under /var/lib/teleport By default, docker-compose will output Teleport's logs to the console for you to observe. If you would rather run the Teleport container in the background, use docker-compose -f teleport-quickstart.yml up -d You can stop the Teleport container using docker-compose -f teleport-quickstart.yml down","title":"Quickstart using docker-compose"},{"location":"quickstart-docker/#quickstart-using-docker-run","text":"If you'd prefer to complete these steps manually, here's some sample docker run commands: # create local config and data directories for teleport, which will be mounted into the container mkdir -p ~/teleport/config ~/teleport/data # generate a sample teleport config and write it to the local config directory # this container will write the config and immediately exit - this is expected docker run --hostname localhost --rm \\ --entrypoint = /bin/sh \\ -v ~/teleport/config:/etc/teleport \\ {{ teleport.latest_oss_docker_image }} -c \"teleport configure > /etc/teleport/teleport.yaml\" # start teleport with mounted config and data directories, plus all ports docker run --hostname localhost --name teleport \\ -v ~/teleport/config:/etc/teleport \\ -v ~/teleport/data:/var/lib/teleport \\ -p 3023 :3023 -p 3025 :3025 -p 3080 :3080 \\ {{ teleport.latest_oss_docker_image }}","title":"Quickstart using docker run"},{"location":"quickstart-docker/#creating-a-teleport-user-when-using-docker-quickstart","text":"To create a user inside your Teleport container, use docker exec . This example command will create a Teleport user called testuser which is allowed to log in as either OS user root or ubuntu . Feel free to change these to suit your needs - there are more instructions above in Step 3 if you'd like additional details): docker exec teleport tctl users add testuser root,ubuntu When you run this command, Teleport will output a URL which you must open to complete the user signup process: User testuser has been created but requires a password. Share this URL with the user to complete user setup, link is valid for 1h0m0s: https://localhost:3080/web/invite/4f2718a52ce107568b191f222ba069f7 NOTE: Make sure localhost:3080 points at a Teleport proxy which users can access. You can now follow this guide from Step 4 onwards to create your user and log into Teleport.","title":"Creating a Teleport user when using Docker quickstart"},{"location":"quickstart/","text":"Teleport Quick Start This tutorial will guide you through the steps needed to install and run Teleport on a single node, which could be your local machine but we recommend a VM. Prerequisites In this tutorial you will start a UI which must be accessible via a browser. If you run this tutorial on a remote machine without a GUI, first make sure that this machine's IP can be reached over your network and that it accepts incoming traffic on port 3080 . We recommend that you read the Architecture Guide before working through this tutorial. If you'd like to dive right in though this is the best place to start! This guide is only meant to demonstrate how to run Teleport in a sandbox or demo environment, and showcase a few basic tasks you can do with Teleport. Step 1: Install Teleport There are several ways to install Teleport. Take a look at the Teleport Installation page to pick the method convenient for you. Step 2: Start Teleport First, create a directory for Teleport to keep its data (by default it's /var/lib/teleport ): $ mkdir -p /var/lib/teleport Now we are ready to start Teleport. Start the teleport daemon: $ teleport start # if you are not `root` you may need `sudo` Background Process Avoid suspending your current shell session by running the process in the background like so: teleport start > teleport.log 2>&1 & . Access the process logs with less teleport.log . Debugging/Verbose Output If you encounter errors with any teleport , tsh or tctl command you can enable verbose logging with the -d, --debug flag. By default, Teleport services bind to 0.0.0.0. If you ran Teleport without any configuration or flags you should see this output in your console or logfile: [AUTH] Auth service is starting on 0.0.0.0:3025 [PROXY] Reverse tunnel service is starting on 0.0.0.0:3024 [PROXY] Web proxy service is starting on 0.0.0.0:3080 [PROXY] SSH proxy service is starting on 0.0.0.0:3023 [SSH] Service is starting on 0.0.0.0:3022 Congratulations - you are now running Teleport! Step 3: Create a User Signup Token We've got Teleport running but there are no users recognized by Teleport Auth yet. Let's create one for your OS user. In this example the OS user is teleport and the hostname of the node is grav-00 . OS User Mappings The OS user teleport must exist! On Linux, if it does not already exist, create it with adduser teleport . If you do not have the permission to create new users on the VM, run tctl users add teleport <your-username> to explicitly map teleport to an existing OS user. If you do not map to a real OS user you will get authentication errors later on in this tutorial! # A new Teleport user will be assigned a # mapping to an OS user of the same name # This is the same as running `tctl users add teleport teleport` [ teleport@grav-00 ~ ] $ tctl users add teleport User teleport has been created but requires a password. Share this URL with the user to complete user setup, link is valid for 1h0m0s: https://grav-00:3080/web/invite/3a8e9fb6a5093a47b547c0f32e3a98d4 NOTE: Make sure grav-00:3080 points at a Teleport proxy which users can access. If you want to map to a different OS user, electric for instance, you can specify like so: tctl users add teleport electric . You can also assign multiple mappings like this tctl users add teleport electric,joe,root . You now have a signup token for the Teleport User teleport and will need to open this URL in a web browser to complete the registration process. Step 4: Register a User If the machine where you ran these commands has a web browser installed, you should be able to open the URL and connect to Teleport Proxy right away. If you are working on a remote machine, you may need to access the Teleport Proxy via the host machine and port 3080 in a web browser. One simple way to do this is to temporarily append [HOST_IP] grav-00 to /etc/hosts . Warning We haven't provisioned any SSL certs for Teleport yet. Your browser will throw a warning: Your connection is not private . Click Advanced , and Proceed to [HOST_IP] (unsafe) to preview the Teleport UI. Teleport enforces two-factor authentication by default . If you do not already have Google Authenticator , Authy or another 2FA client installed, you will need to install it on your smart phone. Then you can scan the QR code on the Teleport login web page, pick a password and enter the two-factor token. After completing registration you will be logged in automatically Step 5: Log in through the CLI Let's login using the tsh command line tool. Just as in the previous step, you will need to be able to resolve the hostname of the cluster to a network accessible IP. Warning For the purposes of this quickstart we are using the --insecure flag which allows us to skip configuring the HTTP/TLS certificate for Teleport proxy. Caution : the --insecure flag does not skip TLS validation for the Auth Server. The self-signed Auth Server certificate expects to be accessed via one of a set of hostnames (ex. grav-00 ). If you attempt to access via localhost you will probably get this error: principal \"localhost\" not in the set of valid principals for given certificate . To resolve this error find your hostname with the hostname command and use that instead of localhost . Never use --insecure in production unless you terminate SSL at a load balancer. You must configure a HTTP/TLS certificate for the Proxy. Learn more in our SSL/TLS for Teleport Proxy - Production Guide # here grav-00 is a resolvable hostname on the same network # --proxy can be an IP, hostname, or URL [ teleport@grav-00 ~ ] $ tsh --proxy = grav-00 --insecure login WARNING: You are using insecure connection to SSH proxy https://grav-00:3080 Enter password for Teleport user teleport: Enter your OTP token: XXXXXX WARNING: You are using insecure connection to SSH proxy https://grav-00:3080 > Profile URL: https://grav-00:3080 Logged in as: teleport Cluster: grav-00 Roles: admin* Logins: teleport Valid until : 2019 -10-05 02 :01:36 +0000 UTC [ valid for 12h0m0s ] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty * RBAC is only available in Teleport Enterprise https://gravitational.com/teleport/docs/enterprise Step 6: Start A Recorded Session At this point you have authenticated with Teleport Auth and can now start a recorded SSH session. You logged in as the teleport user in the last step so the --user is defaulted to teleport . $ tsh ssh --proxy = grav-00 grav-00 $ echo 'howdy' howdy # run whatever you want here, this is a regular SSH session. Note: The tsh client always requires the --proxy flag Your command prompt may not look different, but you are now in a new SSH session which has been authenticated by Teleport! Try a few things to get familiar with recorded sessions: Navigate to https://[HOST]:3080/web/sessions in your web browser to see the list of current and past sessions on the cluster. The session you just created should be listed. After you end a session (type $ exit in session), replay it in your browser. Join the session in your web browser. Here we've started two recorded sessions on the node grav-00 : one via the web browser and one in the command line. Notice that there are distinct SSH sessions even though we logged in with the root user. In the next step you'll learn how to join a shared session. Step 7: Join a Session on the CLI One of the most important features of Teleport is the ability to share a session between users. If you joined your active session in your browser in the previous step you will see the complete session history of the recorded session in the web terminal. Joining a session via a browser is often the easiest way to see what another user is up to, but if you have access to the proxy server from your local machine (or any machine) you can also join a session on the CLI. # This is the recorded session you started in Step 6 $ tsh ssh --proxy = grav-00 grav-00 $ echo 'howdy' howdy # you might have run more stuff here... $ teleport status Cluster Name: grav-00 Host UUID : a3f67090-99cc-45cf-8f70-478d176b970e Session ID : cd908432-950a-4493-a561-9c272b0e0ea6 Session URL : https://grav-00:3080/web/cluster/grav-00/node/a3f67090-99cc-45cf-8f70-478d176b970e/teleport/cd908432-950a-4493-a561-9c272b0e0ea6 Copy the Session ID and open a new SSH session. %~$ tsh join -d --proxy grav-00 --insecure cd908432-950a-4493-a561-9c272b0e0ea6 # you will be asked to re-authenticate your user $ echo 'howdy' howdy # you might have run more stuff here... $ teleport status Cluster Name: grav-00 Host UUID : a3f67090-99cc-45cf-8f70-478d176b970e Session ID : cd908432-950a-4493-a561-9c272b0e0ea6 Session URL : https://grav-00:3080/web/cluster/grav-00/node/a3f67090-99cc-45cf-8f70-478d176b970e/teleport/cd908432-950a-4493-a561-9c272b0e0ea6 $ echo \"Awesome!\" # check out your shared ssh session between two CLI windows Next Steps Congratulations! You've completed the Teleport Quickstart. In this guide you've learned how to install Teleport on a single-node and seen a few of the most practical features in action. When you're ready to learn how to set up Teleport for your team, we recommend that you read our Admin Guide to get all the important details. This guide will lay out everything you need to safely run Teleport in production, including SSL certificates, security considerations, and YAML configuration. Guides If you like to learn by doing, check out our collection of step-by-step guides for common Teleport tasks. Install Teleport Share Sessions Manage Users Label Nodes Teleport with OpenSSH","title":"Quick Start Guide"},{"location":"quickstart/#teleport-quick-start","text":"This tutorial will guide you through the steps needed to install and run Teleport on a single node, which could be your local machine but we recommend a VM.","title":"Teleport Quick Start"},{"location":"quickstart/#prerequisites","text":"In this tutorial you will start a UI which must be accessible via a browser. If you run this tutorial on a remote machine without a GUI, first make sure that this machine's IP can be reached over your network and that it accepts incoming traffic on port 3080 . We recommend that you read the Architecture Guide before working through this tutorial. If you'd like to dive right in though this is the best place to start! This guide is only meant to demonstrate how to run Teleport in a sandbox or demo environment, and showcase a few basic tasks you can do with Teleport.","title":"Prerequisites"},{"location":"quickstart/#step-1-install-teleport","text":"There are several ways to install Teleport. Take a look at the Teleport Installation page to pick the method convenient for you.","title":"Step 1: Install Teleport"},{"location":"quickstart/#step-2-start-teleport","text":"First, create a directory for Teleport to keep its data (by default it's /var/lib/teleport ): $ mkdir -p /var/lib/teleport Now we are ready to start Teleport. Start the teleport daemon: $ teleport start # if you are not `root` you may need `sudo` Background Process Avoid suspending your current shell session by running the process in the background like so: teleport start > teleport.log 2>&1 & . Access the process logs with less teleport.log . Debugging/Verbose Output If you encounter errors with any teleport , tsh or tctl command you can enable verbose logging with the -d, --debug flag. By default, Teleport services bind to 0.0.0.0. If you ran Teleport without any configuration or flags you should see this output in your console or logfile: [AUTH] Auth service is starting on 0.0.0.0:3025 [PROXY] Reverse tunnel service is starting on 0.0.0.0:3024 [PROXY] Web proxy service is starting on 0.0.0.0:3080 [PROXY] SSH proxy service is starting on 0.0.0.0:3023 [SSH] Service is starting on 0.0.0.0:3022 Congratulations - you are now running Teleport!","title":"Step 2: Start Teleport"},{"location":"quickstart/#step-3-create-a-user-signup-token","text":"We've got Teleport running but there are no users recognized by Teleport Auth yet. Let's create one for your OS user. In this example the OS user is teleport and the hostname of the node is grav-00 . OS User Mappings The OS user teleport must exist! On Linux, if it does not already exist, create it with adduser teleport . If you do not have the permission to create new users on the VM, run tctl users add teleport <your-username> to explicitly map teleport to an existing OS user. If you do not map to a real OS user you will get authentication errors later on in this tutorial! # A new Teleport user will be assigned a # mapping to an OS user of the same name # This is the same as running `tctl users add teleport teleport` [ teleport@grav-00 ~ ] $ tctl users add teleport User teleport has been created but requires a password. Share this URL with the user to complete user setup, link is valid for 1h0m0s: https://grav-00:3080/web/invite/3a8e9fb6a5093a47b547c0f32e3a98d4 NOTE: Make sure grav-00:3080 points at a Teleport proxy which users can access. If you want to map to a different OS user, electric for instance, you can specify like so: tctl users add teleport electric . You can also assign multiple mappings like this tctl users add teleport electric,joe,root . You now have a signup token for the Teleport User teleport and will need to open this URL in a web browser to complete the registration process.","title":"Step 3: Create a User Signup Token"},{"location":"quickstart/#step-4-register-a-user","text":"If the machine where you ran these commands has a web browser installed, you should be able to open the URL and connect to Teleport Proxy right away. If you are working on a remote machine, you may need to access the Teleport Proxy via the host machine and port 3080 in a web browser. One simple way to do this is to temporarily append [HOST_IP] grav-00 to /etc/hosts . Warning We haven't provisioned any SSL certs for Teleport yet. Your browser will throw a warning: Your connection is not private . Click Advanced , and Proceed to [HOST_IP] (unsafe) to preview the Teleport UI. Teleport enforces two-factor authentication by default . If you do not already have Google Authenticator , Authy or another 2FA client installed, you will need to install it on your smart phone. Then you can scan the QR code on the Teleport login web page, pick a password and enter the two-factor token. After completing registration you will be logged in automatically","title":"Step 4: Register a User"},{"location":"quickstart/#step-5-log-in-through-the-cli","text":"Let's login using the tsh command line tool. Just as in the previous step, you will need to be able to resolve the hostname of the cluster to a network accessible IP. Warning For the purposes of this quickstart we are using the --insecure flag which allows us to skip configuring the HTTP/TLS certificate for Teleport proxy. Caution : the --insecure flag does not skip TLS validation for the Auth Server. The self-signed Auth Server certificate expects to be accessed via one of a set of hostnames (ex. grav-00 ). If you attempt to access via localhost you will probably get this error: principal \"localhost\" not in the set of valid principals for given certificate . To resolve this error find your hostname with the hostname command and use that instead of localhost . Never use --insecure in production unless you terminate SSL at a load balancer. You must configure a HTTP/TLS certificate for the Proxy. Learn more in our SSL/TLS for Teleport Proxy - Production Guide # here grav-00 is a resolvable hostname on the same network # --proxy can be an IP, hostname, or URL [ teleport@grav-00 ~ ] $ tsh --proxy = grav-00 --insecure login WARNING: You are using insecure connection to SSH proxy https://grav-00:3080 Enter password for Teleport user teleport: Enter your OTP token: XXXXXX WARNING: You are using insecure connection to SSH proxy https://grav-00:3080 > Profile URL: https://grav-00:3080 Logged in as: teleport Cluster: grav-00 Roles: admin* Logins: teleport Valid until : 2019 -10-05 02 :01:36 +0000 UTC [ valid for 12h0m0s ] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty * RBAC is only available in Teleport Enterprise https://gravitational.com/teleport/docs/enterprise","title":"Step 5: Log in through the CLI"},{"location":"quickstart/#step-6-start-a-recorded-session","text":"At this point you have authenticated with Teleport Auth and can now start a recorded SSH session. You logged in as the teleport user in the last step so the --user is defaulted to teleport . $ tsh ssh --proxy = grav-00 grav-00 $ echo 'howdy' howdy # run whatever you want here, this is a regular SSH session. Note: The tsh client always requires the --proxy flag Your command prompt may not look different, but you are now in a new SSH session which has been authenticated by Teleport! Try a few things to get familiar with recorded sessions: Navigate to https://[HOST]:3080/web/sessions in your web browser to see the list of current and past sessions on the cluster. The session you just created should be listed. After you end a session (type $ exit in session), replay it in your browser. Join the session in your web browser. Here we've started two recorded sessions on the node grav-00 : one via the web browser and one in the command line. Notice that there are distinct SSH sessions even though we logged in with the root user. In the next step you'll learn how to join a shared session.","title":"Step 6: Start A Recorded Session"},{"location":"quickstart/#step-7-join-a-session-on-the-cli","text":"One of the most important features of Teleport is the ability to share a session between users. If you joined your active session in your browser in the previous step you will see the complete session history of the recorded session in the web terminal. Joining a session via a browser is often the easiest way to see what another user is up to, but if you have access to the proxy server from your local machine (or any machine) you can also join a session on the CLI. # This is the recorded session you started in Step 6 $ tsh ssh --proxy = grav-00 grav-00 $ echo 'howdy' howdy # you might have run more stuff here... $ teleport status Cluster Name: grav-00 Host UUID : a3f67090-99cc-45cf-8f70-478d176b970e Session ID : cd908432-950a-4493-a561-9c272b0e0ea6 Session URL : https://grav-00:3080/web/cluster/grav-00/node/a3f67090-99cc-45cf-8f70-478d176b970e/teleport/cd908432-950a-4493-a561-9c272b0e0ea6 Copy the Session ID and open a new SSH session. %~$ tsh join -d --proxy grav-00 --insecure cd908432-950a-4493-a561-9c272b0e0ea6 # you will be asked to re-authenticate your user $ echo 'howdy' howdy # you might have run more stuff here... $ teleport status Cluster Name: grav-00 Host UUID : a3f67090-99cc-45cf-8f70-478d176b970e Session ID : cd908432-950a-4493-a561-9c272b0e0ea6 Session URL : https://grav-00:3080/web/cluster/grav-00/node/a3f67090-99cc-45cf-8f70-478d176b970e/teleport/cd908432-950a-4493-a561-9c272b0e0ea6 $ echo \"Awesome!\" # check out your shared ssh session between two CLI windows","title":"Step 7: Join a Session on the CLI"},{"location":"quickstart/#next-steps","text":"Congratulations! You've completed the Teleport Quickstart. In this guide you've learned how to install Teleport on a single-node and seen a few of the most practical features in action. When you're ready to learn how to set up Teleport for your team, we recommend that you read our Admin Guide to get all the important details. This guide will lay out everything you need to safely run Teleport in production, including SSL certificates, security considerations, and YAML configuration.","title":"Next Steps"},{"location":"quickstart/#guides","text":"If you like to learn by doing, check out our collection of step-by-step guides for common Teleport tasks. Install Teleport Share Sessions Manage Users Label Nodes Teleport with OpenSSH","title":"Guides"},{"location":"trustedclusters/","text":"Trusted Clusters The design of trusted clusters allows Teleport users to connect to compute infrastructure located behind firewalls without any open TCP ports. The real world usage examples of this capability include: Managed service providers (MSP) remotely managing infrastructure of their clients. Device manufacturers remotely maintaining computing appliances deployed on premises. Large cloud software vendors manage multiple data centers using a common proxy. Example of a MSP provider using trusted cluster to obtain access to clients clusters. If you haven't already looked at the introduction to Trusted Clusters in the Admin Guide we recommend you review that for an overview before continuing with this guide. The Trusted Clusters chapter in the Admin Guide offers an example of a simple configuration which: Uses a static cluster join token defined in a configuration file. Does not cover inter-cluster role based access control (RBAC). This guide's focus is on more in-depth coverage of trusted clusters features and will cover the following topics: How to add and remove trusted clusters using CLI commands. Enable/disable trust between clusters. Establish permissions mapping between clusters using Teleport roles. Teleport Node Tunneling If you have a large amount of devices on different networks, such as managed IoT devices or a couple of nodes on a different network you can utilize the Teleport Node Tunneling . Introduction As explained in the architecture document , Teleport can partition compute infrastructure into multiple clusters. A cluster is a group of SSH nodes connected to the cluster's auth server acting as a certificate authority (CA) for all users and nodes. To retrieve an SSH certificate, users must authenticate with a cluster through a proxy server . So, if users want to connect to nodes belonging to different clusters, they would normally have to use different --proxy flags for each cluster. This is not always convenient. The concept of leaf clusters allows Teleport administrators to connect multiple clusters together and establish trust between them. Trusted clusters allow users of one cluster, the root cluster to seamlessly SSH into the nodes of another cluster without having to \"hop\" between proxy servers. Moreover, users don't even need to have a direct connection to other clusters' proxy servers. The user experience looks like this: # login using the root \"root\" cluster credentials: $ tsh login --proxy=root.example.com # SSH into some host inside the \"root\" cluster: $ tsh ssh host # SSH into the host located in another cluster called \"leaf\" # The connection is established through root.example.com: $ tsh ssh --cluster=leaf host # See what other clusters are available $ tsh clusters Leaf clusters also have their own restrictions on user access, i.e. permissions mapping takes place. Once connection has been established it's easy to switch from the \"root\" root cluster Let's take a look at how a connection is established between the \"root\" cluster and the \"leaf\" cluster: This setup works as follows: The \"leaf\" creates an outbound reverse SSH tunnel to \"root\" and keeps the tunnel open. Accessibility only works in one direction. The \"leaf\" cluster allows users from \"root\" to access its nodes but users in the \"leaf\" cluster can not access the \"root\" cluster. When a user tries to connect to a node inside \"leaf\" using root's proxy, the reverse tunnel from step 1 is used to establish this connection shown as the green line above. Load Balancers The scheme above also works even if the \"root\" cluster uses multiple proxies behind a load balancer (LB) or a DNS entry with multiple values. This works by \"leaf\" establishing a tunnel to every proxy in \"root\". This requires that an LB uses round-robin or a similar balancing algorithm. Do not use sticky load balancing algorithms (a.k.a. \"session affinity\" or \"sticky sessions\") with Teleport proxies. Join Tokens Lets start with the diagram of how connection between two clusters is established: The first step in establishing a secure tunnel between two clusters is for the leaf cluster \"leaf\" to connect to the root cluster \"root\". When this happens for the first time , clusters know nothing about each other, thus a shared secret needs to exist in order for \"root\" to accept the connection from \"leaf\". This shared secret is called a \"join token\". There are two ways to create join tokens: to statically define them in a configuration file, or to create them on the fly using tctl tool. Important It is important to realize that join tokens are only used to establish the connection for the first time. The clusters will exchange certificates and won't be using the token to re-establish the connection in the future. Static Join Tokens To create a static join token, update the configuration file on \"root\" cluster to look like this: # fragment of /etc/teleport.yaml: auth_service : enabled : true tokens : - trusted_cluster:join-token This token can be used unlimited number of times. Dynamic Join Tokens Creating a token dynamically with a CLI tool offers the advantage of applying a time to live (TTL) interval on it, i.e. it will be impossible to re-use such token after a specified period of time. To create a token using the CLI tool, execute this command on the auth server of cluster \"root\": # generates a new token to allow an inbound from a trusting cluster: $ tctl tokens add --type=trusted_cluster --ttl=5m The cluster invite token: ba4825847f0378bcdfe18113c4998498 This token will expire in 5 minutes # you can also list the outstanding non-expired tokens: $ tctl tokens ls # ... or delete/revoke an invitation: $ tctl tokens rm ba4825847f0378bcdfe18113c4998498 Users of Teleport will recognize that this is the same way you would add any node to a cluster. The token created above can be used multiple times and has an expiration time of 5 minutes. Now, the administrator of \"leaf\" must create the following resource file: # cluster.yaml kind : trusted_cluster version : v2 metadata : # the trusted cluster name MUST match the 'cluster_name' setting of the # root cluster name : root spec : # this field allows to create tunnels that are disabled, but can be enabled later. enabled : true # the token expected by the \"root\" cluster: token : ba4825847f0378bcdfe18113c4998498 # the address in 'host:port' form of the reverse tunnel listening port on the # \"root\" proxy server: tunnel_addr : root.example.com:3024 # the address in 'host:port' form of the web listening port on the # \"root\" proxy server: web_proxy_addr : root.example.com:3080 # the role mapping allows to map user roles from one cluster to another # (enterprise editions of Teleport only) role_map : - remote : \"admin\" # users who have \"admin\" role on \"root\" local : [ \"auditor\" ] # will be assigned \"auditor\" role when logging into \"leaf\" Then, use tctl create to add the file: $ tctl create cluster.yaml At this point the users of the \"root\" cluster should be able to see \"leaf\" in the list of available clusters. HTTPS configuration If the web_proxy_addr endpoint of the root cluster uses a self-signed or invalid HTTPS certificate, you will get an error: \"the trusted cluster uses misconfigured HTTP/TLS certificate\" . For ease of testing, the Teleport daemon on \"leaf\" can be started with the --insecure CLI flag to accept self-signed certificates. Make sure to configure HTTPS properly and remove the insecure flag for production use. Security Implications Consider the security implications when deciding which token method to use. Short lived tokens decrease the window for attack but make automation a bit more complicated. RBAC Version Warning The RBAC section is applicable only to Teleport Enterprise. The open source version does not support SSH roles. When a leaf cluster \"leaf\" from the diagram above establishes trust with the root cluster \"root\", it needs a way to configure which users from \"root\" should be allowed in and what permissions should they have. Teleport Enterprise uses role mapping to achieve this. Consider the following: Both clusters \"root\" and \"leaf\" have their own locally defined roles. Every user in Teleport Enterprise is assigned a role. When creating a trusted cluster resource, the administrator of \"leaf\" must define how roles from \"root\" map to roles on \"leaf\". To update role map for an existing trusted cluster delete and re-create the trusted cluster with the updated role map Example Lets make a few assumptions for this example: The cluster \"root\" has two roles: user for regular users and admin for local administrators. We want administrators from \"root\" (but not regular users!) to have restricted access to \"leaf\". We want to deny them access to machines with \"environment=production\" label. First, we need to create a special role for root users on \"leaf\": # save this into root-user-role.yaml on the leaf cluster and execute: # tctl create root-user-role.yaml kind : role version : v3 metadata : name : local-admin spec : allow : node_labels : '*' : '*' deny : node_labels : \"environment\" : \"production\" Now, we need to establish trust between roles \"root:admin\" and \"leaf:admin\". This is done by creating a trusted cluster resource on \"leaf\" which looks like this: # save this as root-cluster.yaml on the auth server of \"leaf\" and then execute: # tctl create root-cluster.yaml kind : trusted_cluster version : v1 metadata : name : \"name-of-root-cluster\" spec : enabled : true role_map : - remote : admin # admin <-> admin works for community edition. Enterprise users # have great control over RBAC. local : [ admin ] token : \"join-token-from-root\" tunnel_addr : root.example.com:3024 web_proxy_addr : root.example.com:3080 What if we wanted to let any user from \"root\" to be allowed to connect to nodes on \"leaf\"? In this case we can use a wildcard * in the role_map like this: role_map : - remote : \"*\" local : [ admin ] role_map : - remote : 'cluster-*' local : [ clusteradmin ] You can even use regular expressions to map user roles from one cluster to another, you can even capture parts of the remote role name and use reference it to name the local role: # in this example, remote users with remote role called 'remote-one' will be # mapped to a local role called 'local-one', and `remote-two` becomes `local-two`, etc: - remote : \"^remote-(.*)$\" local : [ local-$1 ] NOTE: The regexp matching is activated only when the expression starts with ^ and ends with $ Trusted Cluster UI For customers using Teleport Enterprise, they can easily configure leaf nodes using the Teleport Proxy UI. Creating Trust from the Leaf node to the root node. Updating Trusted Cluster Role Map In order to update the role map for a trusted cluster, first we will need to remove the cluster by executing: $ tctl rm tc/root-cluster Then following updating the role map, we can re-create the cluster by executing: $ tctl create root-user-updated-role.yaml Using Trusted Clusters Now an admin from the \"root\" cluster can see and access the \"leaf\" cluster: # log into the root cluster: $ tsh --proxy=root.example.com login admin # see the list of available clusters $ tsh clusters Cluster Name Status ------------ ------ root online leaf online # see the list of machines (nodes) behind the leaf cluster: $ tsh ls --cluster=leaf Node Name Node ID Address Labels --------- ------------------ -------------- ----------- db1.leaf cf7cc5cd-935e-46f1 10.0.5.2:3022 role=db-leader db2.leaf 3879d133-fe81-3212 10.0.5.3:3022 role=db-follower # SSH into any node in \"leaf\": $ tsh ssh --cluster=leaf user@db1.leaf Note Trusted clusters work only one way. So, in the example above users from \"leaf\" cannot see or connect to the nodes in \"root\". Disabling Trust To temporarily disable trust between clusters, i.e. to disconnect the \"leaf\" cluster from \"root\", edit the YAML definition of the trusted cluster resource and set enabled to \"false\", then update it: $ tctl create --force cluster.yaml Remove Leaf Cluster relationship from both sides Once established, to fully remove a trust relationship between two clusters, do the following: Remove the relationship from the leaf cluster: tctl rm tc/root.example.com ( tc = trusted cluster) Remove the relationship from the root cluster: tctl rm lc/leaf.example.com . Remove Leaf Cluster relationship from the root Remove the relationship from the root cluster: tctl rm rc/leaf.example.com . Note The leaf.example.com cluster will continue to try and ping the root cluster, but will not be able to connect. To re-establish the trusted cluster relationship, the trusted cluster has to be created again from the leaf cluster. Remove Leaf Cluster relationship from the leaf Remove the relationship from the leaf cluster: tctl rm tc/root.example.com . Sharing Kubernetes groups between Trusted Clusters Below is an example of how to share a kubernetes group between trusted clusters. In this example, we have a root trusted cluster with a role root and kubernetes groups: kubernetes_groups : [ \"system:masters\" ] SSH logins: logins : [ \"root\" ] The leaf cluster can choose to map this root cluster to its own cluster. The admin cluster in the trusted cluster config: role_map : - remote : \"root\" local : [ admin ] The role admin of the leaf cluster can now be set up to use the root cluster role logins and kubernetes_groups using the following variables: Note In order to pass logins from a root trusted cluster to a leaf cluster, you must use the variable {% raw %}{{internal.logins}}{% endraw %} . How does it work? At a first glance, Trusted Clusters in combination with RBAC may seem complicated. However, it is based on certificate-based SSH authentication which is fairly easy to reason about: One can think of an SSH certificate as a \"permit\" issued and time-stamped by a certificate authority. A certificate contains four important pieces of data: List of allowed UNIX logins a user can use. They are called \"principals\" in the certificate. Signature of the certificate authority who issued it (the auth server) Metadata (certificate extensions): additional data protected by the signature above. Teleport uses the metadata to store the list of user roles and SSH options like \"permit-agent-forwarding\". The expiration date. Try executing tsh status right after tsh login to see all these fields in the client certificate. When a user from \"root\" tries to connect to a node inside \"leaf\", her certificate is presented to the auth server of \"leaf\" and it performs the following checks: Checks that the certificate signature matches one of the trusted clusters. Tries to find a local role which maps to the list of principals found in the certificate. Checks if the local role allows the requested identity (UNIX login) to have access. Checks that the certificate has not expired. Troubleshooting There are three common types of problems Teleport administrators can run into when configuring trust between two clusters: HTTPS configuration : when the root cluster uses a self-signed or invalid HTTPS certificate. Connectivity problems : when a leaf cluster \"leaf\" does not show up in tsh clusters output on \"root\". Access problems : when users from \"root\" get \"access denied\" error messages trying to connect to nodes on \"leaf\". HTTPS configuration If the web_proxy_addr endpoint of the root cluster uses a self-signed or invalid HTTPS certificate, you will get an error: \"the trusted cluster uses misconfigured HTTP/TLS certificate\". For ease of testing, the teleport daemon on \"leaf\" can be started with the --insecure CLI flag to accept self-signed certificates. Make sure to configure HTTPS properly and remove the insecure flag for production use. Connectivity Problems To troubleshoot connectivity problems, enable verbose output for the auth servers on both clusters. Usually this can be done by adding --debug flag to teleport start --debug . You can also do this by updating the configuration file for both auth servers: # snippet from /etc/teleport.yaml teleport : log : output : stderr severity : DEBUG On systemd-based distributions you can watch the log output via: $ sudo journalctl -fu teleport Most of the time you will find out that either a join token is mismatched/expired, or the network addresses for tunnel_addr or web_proxy_addr cannot be reached due to pre-existing firewall rules or how your network security groups are configured on AWS. Access Problems Troubleshooting access denied messages can be challenging. A Teleport administrator should check to see the following: Which roles a user is assigned on \"root\" when they retrieve their SSH certificate via tsh login . You can inspect the retrieved certificate with tsh status command on the client side. Which roles a user is assigned on \"leaf\" when the role mapping takes place. The role mapping result is reflected in the Teleport audit log. By default, it is stored in /var/lib/teleport/log on a auth server of a cluster. Check the audit log messages on both clusters to get answers for the questions above.","title":"Trusted Clusters"},{"location":"trustedclusters/#trusted-clusters","text":"The design of trusted clusters allows Teleport users to connect to compute infrastructure located behind firewalls without any open TCP ports. The real world usage examples of this capability include: Managed service providers (MSP) remotely managing infrastructure of their clients. Device manufacturers remotely maintaining computing appliances deployed on premises. Large cloud software vendors manage multiple data centers using a common proxy. Example of a MSP provider using trusted cluster to obtain access to clients clusters. If you haven't already looked at the introduction to Trusted Clusters in the Admin Guide we recommend you review that for an overview before continuing with this guide. The Trusted Clusters chapter in the Admin Guide offers an example of a simple configuration which: Uses a static cluster join token defined in a configuration file. Does not cover inter-cluster role based access control (RBAC). This guide's focus is on more in-depth coverage of trusted clusters features and will cover the following topics: How to add and remove trusted clusters using CLI commands. Enable/disable trust between clusters. Establish permissions mapping between clusters using Teleport roles. Teleport Node Tunneling If you have a large amount of devices on different networks, such as managed IoT devices or a couple of nodes on a different network you can utilize the Teleport Node Tunneling .","title":"Trusted Clusters"},{"location":"trustedclusters/#introduction","text":"As explained in the architecture document , Teleport can partition compute infrastructure into multiple clusters. A cluster is a group of SSH nodes connected to the cluster's auth server acting as a certificate authority (CA) for all users and nodes. To retrieve an SSH certificate, users must authenticate with a cluster through a proxy server . So, if users want to connect to nodes belonging to different clusters, they would normally have to use different --proxy flags for each cluster. This is not always convenient. The concept of leaf clusters allows Teleport administrators to connect multiple clusters together and establish trust between them. Trusted clusters allow users of one cluster, the root cluster to seamlessly SSH into the nodes of another cluster without having to \"hop\" between proxy servers. Moreover, users don't even need to have a direct connection to other clusters' proxy servers. The user experience looks like this: # login using the root \"root\" cluster credentials: $ tsh login --proxy=root.example.com # SSH into some host inside the \"root\" cluster: $ tsh ssh host # SSH into the host located in another cluster called \"leaf\" # The connection is established through root.example.com: $ tsh ssh --cluster=leaf host # See what other clusters are available $ tsh clusters Leaf clusters also have their own restrictions on user access, i.e. permissions mapping takes place. Once connection has been established it's easy to switch from the \"root\" root cluster Let's take a look at how a connection is established between the \"root\" cluster and the \"leaf\" cluster: This setup works as follows: The \"leaf\" creates an outbound reverse SSH tunnel to \"root\" and keeps the tunnel open. Accessibility only works in one direction. The \"leaf\" cluster allows users from \"root\" to access its nodes but users in the \"leaf\" cluster can not access the \"root\" cluster. When a user tries to connect to a node inside \"leaf\" using root's proxy, the reverse tunnel from step 1 is used to establish this connection shown as the green line above. Load Balancers The scheme above also works even if the \"root\" cluster uses multiple proxies behind a load balancer (LB) or a DNS entry with multiple values. This works by \"leaf\" establishing a tunnel to every proxy in \"root\". This requires that an LB uses round-robin or a similar balancing algorithm. Do not use sticky load balancing algorithms (a.k.a. \"session affinity\" or \"sticky sessions\") with Teleport proxies.","title":"Introduction"},{"location":"trustedclusters/#join-tokens","text":"Lets start with the diagram of how connection between two clusters is established: The first step in establishing a secure tunnel between two clusters is for the leaf cluster \"leaf\" to connect to the root cluster \"root\". When this happens for the first time , clusters know nothing about each other, thus a shared secret needs to exist in order for \"root\" to accept the connection from \"leaf\". This shared secret is called a \"join token\". There are two ways to create join tokens: to statically define them in a configuration file, or to create them on the fly using tctl tool. Important It is important to realize that join tokens are only used to establish the connection for the first time. The clusters will exchange certificates and won't be using the token to re-establish the connection in the future.","title":"Join Tokens"},{"location":"trustedclusters/#static-join-tokens","text":"To create a static join token, update the configuration file on \"root\" cluster to look like this: # fragment of /etc/teleport.yaml: auth_service : enabled : true tokens : - trusted_cluster:join-token This token can be used unlimited number of times.","title":"Static Join Tokens"},{"location":"trustedclusters/#dynamic-join-tokens","text":"Creating a token dynamically with a CLI tool offers the advantage of applying a time to live (TTL) interval on it, i.e. it will be impossible to re-use such token after a specified period of time. To create a token using the CLI tool, execute this command on the auth server of cluster \"root\": # generates a new token to allow an inbound from a trusting cluster: $ tctl tokens add --type=trusted_cluster --ttl=5m The cluster invite token: ba4825847f0378bcdfe18113c4998498 This token will expire in 5 minutes # you can also list the outstanding non-expired tokens: $ tctl tokens ls # ... or delete/revoke an invitation: $ tctl tokens rm ba4825847f0378bcdfe18113c4998498 Users of Teleport will recognize that this is the same way you would add any node to a cluster. The token created above can be used multiple times and has an expiration time of 5 minutes. Now, the administrator of \"leaf\" must create the following resource file: # cluster.yaml kind : trusted_cluster version : v2 metadata : # the trusted cluster name MUST match the 'cluster_name' setting of the # root cluster name : root spec : # this field allows to create tunnels that are disabled, but can be enabled later. enabled : true # the token expected by the \"root\" cluster: token : ba4825847f0378bcdfe18113c4998498 # the address in 'host:port' form of the reverse tunnel listening port on the # \"root\" proxy server: tunnel_addr : root.example.com:3024 # the address in 'host:port' form of the web listening port on the # \"root\" proxy server: web_proxy_addr : root.example.com:3080 # the role mapping allows to map user roles from one cluster to another # (enterprise editions of Teleport only) role_map : - remote : \"admin\" # users who have \"admin\" role on \"root\" local : [ \"auditor\" ] # will be assigned \"auditor\" role when logging into \"leaf\" Then, use tctl create to add the file: $ tctl create cluster.yaml At this point the users of the \"root\" cluster should be able to see \"leaf\" in the list of available clusters. HTTPS configuration If the web_proxy_addr endpoint of the root cluster uses a self-signed or invalid HTTPS certificate, you will get an error: \"the trusted cluster uses misconfigured HTTP/TLS certificate\" . For ease of testing, the Teleport daemon on \"leaf\" can be started with the --insecure CLI flag to accept self-signed certificates. Make sure to configure HTTPS properly and remove the insecure flag for production use.","title":"Dynamic Join Tokens"},{"location":"trustedclusters/#security-implications","text":"Consider the security implications when deciding which token method to use. Short lived tokens decrease the window for attack but make automation a bit more complicated.","title":"Security Implications"},{"location":"trustedclusters/#rbac","text":"Version Warning The RBAC section is applicable only to Teleport Enterprise. The open source version does not support SSH roles. When a leaf cluster \"leaf\" from the diagram above establishes trust with the root cluster \"root\", it needs a way to configure which users from \"root\" should be allowed in and what permissions should they have. Teleport Enterprise uses role mapping to achieve this. Consider the following: Both clusters \"root\" and \"leaf\" have their own locally defined roles. Every user in Teleport Enterprise is assigned a role. When creating a trusted cluster resource, the administrator of \"leaf\" must define how roles from \"root\" map to roles on \"leaf\". To update role map for an existing trusted cluster delete and re-create the trusted cluster with the updated role map","title":"RBAC"},{"location":"trustedclusters/#example","text":"Lets make a few assumptions for this example: The cluster \"root\" has two roles: user for regular users and admin for local administrators. We want administrators from \"root\" (but not regular users!) to have restricted access to \"leaf\". We want to deny them access to machines with \"environment=production\" label. First, we need to create a special role for root users on \"leaf\": # save this into root-user-role.yaml on the leaf cluster and execute: # tctl create root-user-role.yaml kind : role version : v3 metadata : name : local-admin spec : allow : node_labels : '*' : '*' deny : node_labels : \"environment\" : \"production\" Now, we need to establish trust between roles \"root:admin\" and \"leaf:admin\". This is done by creating a trusted cluster resource on \"leaf\" which looks like this: # save this as root-cluster.yaml on the auth server of \"leaf\" and then execute: # tctl create root-cluster.yaml kind : trusted_cluster version : v1 metadata : name : \"name-of-root-cluster\" spec : enabled : true role_map : - remote : admin # admin <-> admin works for community edition. Enterprise users # have great control over RBAC. local : [ admin ] token : \"join-token-from-root\" tunnel_addr : root.example.com:3024 web_proxy_addr : root.example.com:3080 What if we wanted to let any user from \"root\" to be allowed to connect to nodes on \"leaf\"? In this case we can use a wildcard * in the role_map like this: role_map : - remote : \"*\" local : [ admin ] role_map : - remote : 'cluster-*' local : [ clusteradmin ] You can even use regular expressions to map user roles from one cluster to another, you can even capture parts of the remote role name and use reference it to name the local role: # in this example, remote users with remote role called 'remote-one' will be # mapped to a local role called 'local-one', and `remote-two` becomes `local-two`, etc: - remote : \"^remote-(.*)$\" local : [ local-$1 ] NOTE: The regexp matching is activated only when the expression starts with ^ and ends with $","title":"Example"},{"location":"trustedclusters/#trusted-cluster-ui","text":"For customers using Teleport Enterprise, they can easily configure leaf nodes using the Teleport Proxy UI. Creating Trust from the Leaf node to the root node.","title":"Trusted Cluster UI"},{"location":"trustedclusters/#updating-trusted-cluster-role-map","text":"In order to update the role map for a trusted cluster, first we will need to remove the cluster by executing: $ tctl rm tc/root-cluster Then following updating the role map, we can re-create the cluster by executing: $ tctl create root-user-updated-role.yaml","title":"Updating Trusted Cluster Role Map"},{"location":"trustedclusters/#using-trusted-clusters","text":"Now an admin from the \"root\" cluster can see and access the \"leaf\" cluster: # log into the root cluster: $ tsh --proxy=root.example.com login admin # see the list of available clusters $ tsh clusters Cluster Name Status ------------ ------ root online leaf online # see the list of machines (nodes) behind the leaf cluster: $ tsh ls --cluster=leaf Node Name Node ID Address Labels --------- ------------------ -------------- ----------- db1.leaf cf7cc5cd-935e-46f1 10.0.5.2:3022 role=db-leader db2.leaf 3879d133-fe81-3212 10.0.5.3:3022 role=db-follower # SSH into any node in \"leaf\": $ tsh ssh --cluster=leaf user@db1.leaf Note Trusted clusters work only one way. So, in the example above users from \"leaf\" cannot see or connect to the nodes in \"root\".","title":"Using Trusted Clusters"},{"location":"trustedclusters/#disabling-trust","text":"To temporarily disable trust between clusters, i.e. to disconnect the \"leaf\" cluster from \"root\", edit the YAML definition of the trusted cluster resource and set enabled to \"false\", then update it: $ tctl create --force cluster.yaml","title":"Disabling Trust"},{"location":"trustedclusters/#remove-leaf-cluster-relationship-from-both-sides","text":"Once established, to fully remove a trust relationship between two clusters, do the following: Remove the relationship from the leaf cluster: tctl rm tc/root.example.com ( tc = trusted cluster) Remove the relationship from the root cluster: tctl rm lc/leaf.example.com .","title":"Remove Leaf Cluster relationship from both sides"},{"location":"trustedclusters/#remove-leaf-cluster-relationship-from-the-root","text":"Remove the relationship from the root cluster: tctl rm rc/leaf.example.com . Note The leaf.example.com cluster will continue to try and ping the root cluster, but will not be able to connect. To re-establish the trusted cluster relationship, the trusted cluster has to be created again from the leaf cluster.","title":"Remove Leaf Cluster relationship from the root"},{"location":"trustedclusters/#remove-leaf-cluster-relationship-from-the-leaf","text":"Remove the relationship from the leaf cluster: tctl rm tc/root.example.com .","title":"Remove Leaf Cluster relationship from the leaf"},{"location":"trustedclusters/#sharing-kubernetes-groups-between-trusted-clusters","text":"Below is an example of how to share a kubernetes group between trusted clusters. In this example, we have a root trusted cluster with a role root and kubernetes groups: kubernetes_groups : [ \"system:masters\" ] SSH logins: logins : [ \"root\" ] The leaf cluster can choose to map this root cluster to its own cluster. The admin cluster in the trusted cluster config: role_map : - remote : \"root\" local : [ admin ] The role admin of the leaf cluster can now be set up to use the root cluster role logins and kubernetes_groups using the following variables: Note In order to pass logins from a root trusted cluster to a leaf cluster, you must use the variable {% raw %}{{internal.logins}}{% endraw %} .","title":"Sharing Kubernetes groups between Trusted Clusters"},{"location":"trustedclusters/#how-does-it-work","text":"At a first glance, Trusted Clusters in combination with RBAC may seem complicated. However, it is based on certificate-based SSH authentication which is fairly easy to reason about: One can think of an SSH certificate as a \"permit\" issued and time-stamped by a certificate authority. A certificate contains four important pieces of data: List of allowed UNIX logins a user can use. They are called \"principals\" in the certificate. Signature of the certificate authority who issued it (the auth server) Metadata (certificate extensions): additional data protected by the signature above. Teleport uses the metadata to store the list of user roles and SSH options like \"permit-agent-forwarding\". The expiration date. Try executing tsh status right after tsh login to see all these fields in the client certificate. When a user from \"root\" tries to connect to a node inside \"leaf\", her certificate is presented to the auth server of \"leaf\" and it performs the following checks: Checks that the certificate signature matches one of the trusted clusters. Tries to find a local role which maps to the list of principals found in the certificate. Checks if the local role allows the requested identity (UNIX login) to have access. Checks that the certificate has not expired.","title":"How does it work?"},{"location":"trustedclusters/#troubleshooting","text":"There are three common types of problems Teleport administrators can run into when configuring trust between two clusters: HTTPS configuration : when the root cluster uses a self-signed or invalid HTTPS certificate. Connectivity problems : when a leaf cluster \"leaf\" does not show up in tsh clusters output on \"root\". Access problems : when users from \"root\" get \"access denied\" error messages trying to connect to nodes on \"leaf\".","title":"Troubleshooting"},{"location":"trustedclusters/#https-configuration","text":"If the web_proxy_addr endpoint of the root cluster uses a self-signed or invalid HTTPS certificate, you will get an error: \"the trusted cluster uses misconfigured HTTP/TLS certificate\". For ease of testing, the teleport daemon on \"leaf\" can be started with the --insecure CLI flag to accept self-signed certificates. Make sure to configure HTTPS properly and remove the insecure flag for production use.","title":"HTTPS configuration"},{"location":"trustedclusters/#connectivity-problems","text":"To troubleshoot connectivity problems, enable verbose output for the auth servers on both clusters. Usually this can be done by adding --debug flag to teleport start --debug . You can also do this by updating the configuration file for both auth servers: # snippet from /etc/teleport.yaml teleport : log : output : stderr severity : DEBUG On systemd-based distributions you can watch the log output via: $ sudo journalctl -fu teleport Most of the time you will find out that either a join token is mismatched/expired, or the network addresses for tunnel_addr or web_proxy_addr cannot be reached due to pre-existing firewall rules or how your network security groups are configured on AWS.","title":"Connectivity Problems"},{"location":"trustedclusters/#access-problems","text":"Troubleshooting access denied messages can be challenging. A Teleport administrator should check to see the following: Which roles a user is assigned on \"root\" when they retrieve their SSH certificate via tsh login . You can inspect the retrieved certificate with tsh status command on the client side. Which roles a user is assigned on \"leaf\" when the role mapping takes place. The role mapping result is reflected in the Teleport audit log. By default, it is stored in /var/lib/teleport/log on a auth server of a cluster. Check the audit log messages on both clusters to get answers for the questions above.","title":"Access Problems"},{"location":"user-manual/","text":"Teleport User Manual This User Manual covers usage of the Teleport client tool, tsh and Teleport's Web interface. In this document you will learn how to: Log into an interactive shell on remote cluster nodes. Copy files to and from cluster nodes. Connect to SSH clusters behind firewalls without any open ports, using SSH reverse tunnels. Explore a cluster and execute commands on specific nodes in a cluster. Share interactive shell sessions with colleagues or join someone else's session. Replay recorded interactive sessions. In addition to this document, you can always simply type tsh into your terminal for the CLI reference . Introduction For the impatient, here's an example of how a user would typically use tsh : # Login into a Teleport cluster. This command retrieves user's certificates # and saves them into ~/.tsh/teleport.example.com $ tsh login --proxy = teleport.example.com # SSH into a node, as usual: $ tsh ssh user@node # `tsh ssh` takes the same arguments as OpenSSH client: $ tsh ssh -o ForwardAgent = yes user@node $ tsh ssh -o AddKeysToAgent = yes user@node # you can even create a convenient symlink: $ ln -s /path/to/tsh /path/to/ssh # ... and now your 'ssh' command is calling Teleport's `tsh ssh` $ ssh user@host # This command removes SSH certificates from a user's machine: $ tsh logout In other words, Teleport was designed to be fully compatible with existing SSH-based workflows and does not require users to learn anything new, other than to call tsh login in the beginning. Installing tsh Windows Users: Download tsh Binary Mac Users: Download Mac Teleport Binary. Includes tsh Mac Users with Brew : brew install teleport Linux Users: Download Linux Teleport Binary. Includes tsh User Identities A user identity in Teleport exists in the scope of a cluster. The member nodes of a cluster may have multiple OS users on them. A Teleport administrator assigns allowed logins to every Teleport user account. When logging into a remote node, you will have to specify both logins. Teleport identity will have to be passed as --user flag, while the node login will be passed as login@host , using syntax compatible with traditional ssh . # Authenticate against the \"work\" cluster as joe and then # log into the node as root: $ tsh ssh --proxy = work.example.com --user = joe root@node CLI Docs - tsh ssh Logging In To retrieve a user's certificate, execute: # Full form: $ tsh login --proxy = proxy_host:<https_proxy_port>,<ssh_proxy_port> # Using default ports: $ tsh login --proxy = work.example.com # Using custom HTTPS port: $ tsh login --proxy = work.example.com:5000 # Using custom SSH proxy port, which is set on the Auth Server: $ tsh login --proxy = work.example.com:2002 CLI Docs - tsh login Port Description https_proxy_port the HTTPS port the proxy host is listening to (defaults to 3080). ssh_proxy_port the SSH port the proxy is listening to (defaults to 3023). The login command retrieves a user's certificate and stores it in ~/.tsh directory as well as in the ssh agent , if there is one running. This allows you to authenticate just once, maybe at the beginning of the day. Subsequent tsh ssh commands will run without asking for credentials until the temporary certificate expires. By default, Teleport issues user certificates with a TTL (time to live) of 12 hours. Tip It is recommended to always use tsh login before using any other tsh commands. This allows users to omit --proxy flag in subsequent tsh commands. For example tsh ssh user@host will work. A Teleport cluster can be configured for multiple user identity sources. For example, a cluster may have a local user called \"admin\" while regular users should authenticate via Github . In this case, you have to pass --auth flag to tsh login to specify which identity storage to use: # Login using the local Teleport 'admin' user: $ tsh --proxy = proxy.example.com --auth = local --user = admin login # Login using Github as an SSO provider, assuming the Github connector is called \"github\" $ tsh --proxy = proxy.example.com --auth = github --user = admin login When using an external identity provider to log in, tsh will need to open a web browser to complete the authentication flow. By default, tsh will use your system's default browser to open such links. If you wish to suppress this behaviour, you can use the --browser=none flag: # Don't open the system default browser when logging in $ tsh login --proxy=work.example.com --browser=none In this situation, a link will be printed to the screen. You can copy and paste this link into a browser of your choice to continue the login flow. CLI Docs - tsh login Inspecting SSH Certificate To inspect the SSH certificates in ~/.tsh , a user may execute the following command: $ tsh status > Profile URL: https://proxy.example.com:3080 Logged in as: johndoe Roles: admin* Logins: root, admin, guest Valid until : 2017 -04-25 15 :02:30 -0700 PDT [ valid for 1h0m0s ] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty CLI Docs - tsh status SSH Agent Support If there is an ssh agent running, tsh login will store the user certificate in the agent. This can be verified via: $ ssh-add -L SSH agent can be used to feed the certificate to other SSH clients, for example to OpenSSH ssh . If you wish to disable SSH agent integration, pass --no-use-local-ssh-agent to tsh . You can also set the TELEPORT_USE_LOCAL_SSH_AGENT environment variable to false in your shell profile to make this permanent. Identity Files tsh login can also save the user certificate into a file: # Authenticate user against proxy.example.com and save the user # certificate into joe.pem file $ tsh login --proxy = proxy.example.com --out = joe # Use joe.pem to login into a server 'db' $ tsh ssh --proxy = proxy.example.com -i joe joe@db By default, --out flag will create an identity file suitable for tsh -i but if compatibility with OpenSSH is needed, --format=openssh must be specified. In this case the identity will be saved into two files: joe and joe-cert.pub : $ tsh login --proxy = proxy.example.com --out = joe --format = openssh $ ls -lh total 8 .0K -rw------- 1 joe staff 1 .7K Aug 10 16 :16 joe -rw------- 1 joe staff 1 .5K Aug 10 16 :16 joe-cert.pub SSH Certificates for Automation Regular users of Teleport must request an auto-expiring SSH certificate, usually every day. This doesn't work for non-interactive scripts, like cron jobs or CI/CD pipeline. For such automation, it is recommended to create a separate Teleport user for bots and request a certificate for them with a long time to live (TTL). In this example we're creating a certificate with a TTL of 10 years for the jenkins user and storing it in jenkins.pem file, which can be later used with -i (identity) flag for tsh . # to be executed on a Teleport auth server $ tctl auth sign --ttl = 87600h --user = jenkins --out = jenkins.pem CLI Docs - tctl auth sign Now jenkins.pem can be copied to the jenkins server and passed to -i (identity file) flag of tsh . Essentially tctl auth sign is an admin's equivalent of tsh login --out and allows for unrestricted certificate TTL values. Exploring the Cluster In a Teleport cluster, all nodes periodically ping the cluster's auth server and update their status. This allows Teleport users to see which nodes are online with the tsh ls command: # This command lists all nodes in the cluster which you previously logged in via \"tsh login\": $ tsh ls # Output: Node Name Node ID Address Labels --------- ------- ------- ------ turing 11111111 -dddd-4132 10 .1.0.5:3022 os:linux turing 22222222 -cccc-8274 10 .1.0.6:3022 os:linux graviton 33333333 -aaaa-1284 10 .1.0.7:3022 os:osx CLI Docs - tsh ls tsh ls can apply a filter based on the node labels. # only show nodes with os label set to 'osx': $ tsh ls os = osx Node Name Node ID Address Labels --------- ------- ------- ------ graviton 33333333 -aaaa-1284 10 .1.0.7:3022 os:osx CLI Docs -tsh ls Interactive Shell To launch an interactive shell on a remote node or to execute a command, use tsh ssh . tsh tries to mimic the ssh experience as much as possible, so it supports the most popular ssh flags like -p , -l or -L . For example, if you have the following alias defined in your ~/.bashrc: alias ssh=\"tsh ssh\" then you can continue using familiar SSH syntax: # Have this alias configured, perhaps via ~/.bashrc $ alias ssh = \"/usr/local/bin/tsh ssh\" # Login into a cluster and retrieve your SSH certificate: $ tsh --proxy = proxy.example.com login # These commands execute `tsh ssh` under the hood: $ ssh user@node $ ssh -p 6122 user@node ls $ ssh -o ForwardAgent = yes user@node $ ssh -o AddKeysToAgent = yes user@node Proxy Ports A Teleport proxy uses two ports: 3080 for HTTPS and 3023 for proxying SSH connections. The HTTPS port is used to serve Web UI and also to implement 2nd factor auth for the tsh client. If a Teleport proxy is configured to listen on non-default ports, they must be specified via --proxy flag as shown: tsh --proxy=proxy.example.com:5000,5001 <subcommand> This means use port 5000 for HTTPS and 5001 for SSH . Port Forwarding tsh ssh supports the OpenSSH -L flag which forwards incoming connections from localhost to the specified remote host:port. The syntax of -L flag is as follows, where \"bind_ip\" defaults to 127.0.0.1 : -L [ bind_ip ] :listen_port:remote_host:remote_port Example: $ tsh ssh -L 5000 :web.remote:80 node This will connect to remote server node via proxy.example.com , then it will open a listening socket on localhost:5000 and will forward all incoming connections to web.remote:80 via this SSH tunnel. It is often convenient to establish port forwarding, execute a local command which uses the connection, and then disconnect. You can do this with the --local flag. Example: $ tsh ssh -L 5000 :google.com:80 --local node curl http://localhost:5000 This command: Connects to node Binds the local port 5000 to port 80 on google.com Executes curl command locally, which results in curl hitting google.com:80 via node SSH Jumphost While implementing ProxyJump for Teleport, we have extended the feature to tsh . $ tsh ssh -J proxy.example.com telenode Known limits: Only one jump host is supported ( -J supports chaining that Teleport does not utilise) and tsh will return with error in the case of two jumphosts, i.e. -J proxy-1.example.com,proxy-2.example.com will not work. When tsh ssh -J user@proxy is used, it overrides the SSH proxy defined in the tsh profile and port forwarding is used instead of the existing Teleport proxy subsystem. Resolving Node Names tsh supports multiple methods to resolve remote node names. Traditional: by IP address or via DNS. Nodename setting: teleport daemon supports nodename flag, which allows Teleport administrators to assign alternative node names. Labels: you can address a node by name=value pair. If we have two nodes, one with os:linux label and one node with os:osx , we can log into the OSX node with: $ tsh ssh os = osx This only works if there is only one remote node with the os:osx label, but you can still execute commands via SSH on multiple nodes using labels as a selector. This command will update all system packages on machines that run Linux: $ tsh ssh os = ubuntu apt-get update -y Short-lived Sessions The default TTL of a Teleport user certificate is 12 hours. This can be modified at login with the --ttl flag. This command logs you into the cluster with a very short-lived (1 minute) temporary certificate: $ tsh --ttl = 1 login You will be logged out after one minute, but if you want to log out immediately, you can always do: $ tsh logout Copying Files To securely copy files to and from cluster nodes, use the tsh scp command. It is designed to mimic traditional scp as much as possible: $ tsh scp example.txt root@node:/path/to/dest Again, you may want to create a bash alias like alias scp=\"tsh --proxy=work scp\" and use the familiar syntax: $ scp -P 61122 -r files root@node:/path/to/dest Sharing Sessions Suppose you are trying to troubleshoot a problem on a remote server. Sometimes it makes sense to ask another team member for help. Traditionally, this could be done by letting them know which node you're on, having them SSH in, start a terminal multiplexer like screen and join a session there. Teleport makes this more convenient. Let's log into a server named \"luna\" and ask Teleport for our current session status: $ tsh ssh luna >luna $ teleport status User ID : joe, logged in as joe from 10 .0.10.1 43026 3022 Session ID : 7645d523-60cb-436d-b732-99c5df14b7c4 Session URL: https://work:3080/web/sessions/7645d523-60cb-436d-b732-99c5df14b7c4 Now you can invite another user account to the \"work\" cluster. You can share the URL for access through a web browser, or you can share the session ID and she can join you through her terminal by typing: $ tsh join <session_ID> Note Joining sessions is not supported in recording proxy mode (where session_recording is set to proxy ). Connecting to SSH Clusters behind Firewalls Teleport supports creating clusters of servers located behind firewalls without any open listening TCP ports . This works by creating reverse SSH tunnels from behind-firewall environments into a Teleport proxy you have access to. This feature is called \"Trusted Clusters\" . Refer to the admin manual to learn how a trusted cluster can be configured. Assuming the \"work\" Teleport proxy server is configured with a few trusted clusters, a user may use the tsh clusters command to see a list of all clusters on the server: $ tsh --proxy = work clusters Cluster Name Status ------------ ------ staging online production offline CLI Docs - tsh clusters Now you can use the --cluster flag with any tsh command. For example, to list SSH nodes that are members of the \"production\" cluster, simply do: $ tsh --proxy = work ls --cluster = production Node Name Node ID Address Labels --------- ------- ------- ------ db-1 xxxxxxxxx 10 .0.20.31:3022 kernel:4.4 db-2 xxxxxxxxx 10 .0.20.41:3022 kernel:4.2 Similarly, if you want to SSH into db-1 inside the \"production\" cluster: $ tsh --proxy = work ssh --cluster = production db-1 This is possible even if nodes in the \"production\" cluster are located behind a firewall without open ports. This works because the \"production\" cluster establishes a reverse SSH tunnel back into \"work\" proxy, and this tunnel is used to establish inbound SSH connections. Web UI Teleport proxy serves the web UI on https://proxyhost:3080 . The UI allows you to see the list of online nodes in a cluster, open a web-based terminal to them, see recorded sessions, and replay them. You can also join other users in active sessions. Using OpenSSH Client There are a few differences between Teleport's tsh and OpenSSH's ssh but most of them can be mitigated. tsh always requires the --proxy flag because tsh needs to know which cluster you are connecting to. But if you execute tsh --proxy=xxx login , the current proxy will be saved in your ~/.tsh profile and won't be needed for other tsh commands. tsh ssh operates two usernames: one for the cluster and another for the node you are trying to log into. See User Identities section below. For convenience, tsh assumes $USER for both by default. But again, if you use tsh login before tsh ssh , your Teleport username will be stored in ~/.tsh . If you'd like to set the login name that should be used by default on the remote host, you can set the TELEPORT_LOGIN environment variable. Tip To avoid typing tsh ssh user@host when logging into servers, you can create a symlink ssh -> tsh and execute the symlink. It will behave exactly like a standard ssh command, i.e. ssh login@host . This is helpful with other tools that expect ssh to just work. Teleport is built using standard SSH constructs: keys, certificates and protocols. This means that a Teleport system is 100% compatible with both OpenSSH clients and servers. For an OpenSSH client ( ssh ) to work with a Teleport proxy, two conditions must be met: ssh must be configured to connect through a Teleport proxy. ssh needs to be given the SSH certificate issued by the tsh login command. SSH Proxy Configuration To configure ssh to use a Teleport proxy on proxy.example.com , a user must update the /etc/ssh/ssh_config or ~/.ssh/config . A few examples are shown below: # When \"ssh db\" is executed, OpenSSH will connect to proxy.example.com on port 3023 # and will request a proxied connection to \"db\" on port 3022 (default Teleport SSH port) Host db Port 3022 ProxyJump proxy.example.com:3023 The configuration above is all you need to ssh root@db if there's an SSH agent running on a client computer. You can verify it by executing ssh-add -L right after tsh login . If the SSH agent is running, the cluster certificates will be printed to stdout. If there is no ssh-agent available, the certificate must be passed to the OpenSSH client explicitly. When proxy is in \"Recording mode\" the following will happen with SSH: $ ssh -J user@teleport.proxy:3023 -p 3022 user@target -F ./forward.config Where forward.config enables agent forwarding: Host teleport.proxy ForwardAgent yes Passing Teleport SSH Certificate to OpenSSH Client If a user does not want to use an SSH agent or if the agent is not available, the certificate must be passed to ssh via IdentityFile option (see man ssh_config ). Consider this example: the Teleport user \"joe\" wants to login into the proxy named \"lab.example.com\". He executes the tsh login command: $ tsh --proxy = lab.example.com login --user = joe His identity is now stored in ~/.tsh/keys/lab.example.com , so his ~/.ssh/config needs to look like this: # ~/.ssh/config file: Host *.lab.example.com Port 3022 IdentityFile ~/.tsh/keys/lab.example.com/joe ProxyCommand ssh -i ~/.tsh/keys/lab.example.com/joe -p 3023 %r@lab.example.com -s proxy:%h:%p Now he can SSH into any machine behind lab.example.com using the OpenSSH client: $ ssh jenkins.lab.example.com Troubleshooting If you encounter strange behaviour, you may want to try to solve it by enabling the verbose logging by specifying -d flag when launching tsh . Also, you may want to reset it to a clean state by deleting temporary keys and other data from ~/.tsh Getting Help If you need help, please ask on our community forum . You can also open an issue on Github . For commercial support, you can create a ticket through the customer dashboard . For more information about custom features, or to try our Enterprise edition of Teleport, please reach out to us at sales@gravitational.com .","title":"User Manual"},{"location":"user-manual/#teleport-user-manual","text":"This User Manual covers usage of the Teleport client tool, tsh and Teleport's Web interface. In this document you will learn how to: Log into an interactive shell on remote cluster nodes. Copy files to and from cluster nodes. Connect to SSH clusters behind firewalls without any open ports, using SSH reverse tunnels. Explore a cluster and execute commands on specific nodes in a cluster. Share interactive shell sessions with colleagues or join someone else's session. Replay recorded interactive sessions. In addition to this document, you can always simply type tsh into your terminal for the CLI reference .","title":"Teleport User Manual"},{"location":"user-manual/#introduction","text":"For the impatient, here's an example of how a user would typically use tsh : # Login into a Teleport cluster. This command retrieves user's certificates # and saves them into ~/.tsh/teleport.example.com $ tsh login --proxy = teleport.example.com # SSH into a node, as usual: $ tsh ssh user@node # `tsh ssh` takes the same arguments as OpenSSH client: $ tsh ssh -o ForwardAgent = yes user@node $ tsh ssh -o AddKeysToAgent = yes user@node # you can even create a convenient symlink: $ ln -s /path/to/tsh /path/to/ssh # ... and now your 'ssh' command is calling Teleport's `tsh ssh` $ ssh user@host # This command removes SSH certificates from a user's machine: $ tsh logout In other words, Teleport was designed to be fully compatible with existing SSH-based workflows and does not require users to learn anything new, other than to call tsh login in the beginning.","title":"Introduction"},{"location":"user-manual/#installing-tsh","text":"Windows Users: Download tsh Binary Mac Users: Download Mac Teleport Binary. Includes tsh Mac Users with Brew : brew install teleport Linux Users: Download Linux Teleport Binary. Includes tsh","title":"Installing tsh"},{"location":"user-manual/#user-identities","text":"A user identity in Teleport exists in the scope of a cluster. The member nodes of a cluster may have multiple OS users on them. A Teleport administrator assigns allowed logins to every Teleport user account. When logging into a remote node, you will have to specify both logins. Teleport identity will have to be passed as --user flag, while the node login will be passed as login@host , using syntax compatible with traditional ssh . # Authenticate against the \"work\" cluster as joe and then # log into the node as root: $ tsh ssh --proxy = work.example.com --user = joe root@node CLI Docs - tsh ssh","title":"User Identities"},{"location":"user-manual/#logging-in","text":"To retrieve a user's certificate, execute: # Full form: $ tsh login --proxy = proxy_host:<https_proxy_port>,<ssh_proxy_port> # Using default ports: $ tsh login --proxy = work.example.com # Using custom HTTPS port: $ tsh login --proxy = work.example.com:5000 # Using custom SSH proxy port, which is set on the Auth Server: $ tsh login --proxy = work.example.com:2002 CLI Docs - tsh login Port Description https_proxy_port the HTTPS port the proxy host is listening to (defaults to 3080). ssh_proxy_port the SSH port the proxy is listening to (defaults to 3023). The login command retrieves a user's certificate and stores it in ~/.tsh directory as well as in the ssh agent , if there is one running. This allows you to authenticate just once, maybe at the beginning of the day. Subsequent tsh ssh commands will run without asking for credentials until the temporary certificate expires. By default, Teleport issues user certificates with a TTL (time to live) of 12 hours. Tip It is recommended to always use tsh login before using any other tsh commands. This allows users to omit --proxy flag in subsequent tsh commands. For example tsh ssh user@host will work. A Teleport cluster can be configured for multiple user identity sources. For example, a cluster may have a local user called \"admin\" while regular users should authenticate via Github . In this case, you have to pass --auth flag to tsh login to specify which identity storage to use: # Login using the local Teleport 'admin' user: $ tsh --proxy = proxy.example.com --auth = local --user = admin login # Login using Github as an SSO provider, assuming the Github connector is called \"github\" $ tsh --proxy = proxy.example.com --auth = github --user = admin login When using an external identity provider to log in, tsh will need to open a web browser to complete the authentication flow. By default, tsh will use your system's default browser to open such links. If you wish to suppress this behaviour, you can use the --browser=none flag: # Don't open the system default browser when logging in $ tsh login --proxy=work.example.com --browser=none In this situation, a link will be printed to the screen. You can copy and paste this link into a browser of your choice to continue the login flow. CLI Docs - tsh login","title":"Logging In"},{"location":"user-manual/#inspecting-ssh-certificate","text":"To inspect the SSH certificates in ~/.tsh , a user may execute the following command: $ tsh status > Profile URL: https://proxy.example.com:3080 Logged in as: johndoe Roles: admin* Logins: root, admin, guest Valid until : 2017 -04-25 15 :02:30 -0700 PDT [ valid for 1h0m0s ] Extensions: permit-agent-forwarding, permit-port-forwarding, permit-pty CLI Docs - tsh status","title":"Inspecting SSH Certificate"},{"location":"user-manual/#ssh-agent-support","text":"If there is an ssh agent running, tsh login will store the user certificate in the agent. This can be verified via: $ ssh-add -L SSH agent can be used to feed the certificate to other SSH clients, for example to OpenSSH ssh . If you wish to disable SSH agent integration, pass --no-use-local-ssh-agent to tsh . You can also set the TELEPORT_USE_LOCAL_SSH_AGENT environment variable to false in your shell profile to make this permanent.","title":"SSH Agent Support"},{"location":"user-manual/#identity-files","text":"tsh login can also save the user certificate into a file: # Authenticate user against proxy.example.com and save the user # certificate into joe.pem file $ tsh login --proxy = proxy.example.com --out = joe # Use joe.pem to login into a server 'db' $ tsh ssh --proxy = proxy.example.com -i joe joe@db By default, --out flag will create an identity file suitable for tsh -i but if compatibility with OpenSSH is needed, --format=openssh must be specified. In this case the identity will be saved into two files: joe and joe-cert.pub : $ tsh login --proxy = proxy.example.com --out = joe --format = openssh $ ls -lh total 8 .0K -rw------- 1 joe staff 1 .7K Aug 10 16 :16 joe -rw------- 1 joe staff 1 .5K Aug 10 16 :16 joe-cert.pub","title":"Identity Files"},{"location":"user-manual/#ssh-certificates-for-automation","text":"Regular users of Teleport must request an auto-expiring SSH certificate, usually every day. This doesn't work for non-interactive scripts, like cron jobs or CI/CD pipeline. For such automation, it is recommended to create a separate Teleport user for bots and request a certificate for them with a long time to live (TTL). In this example we're creating a certificate with a TTL of 10 years for the jenkins user and storing it in jenkins.pem file, which can be later used with -i (identity) flag for tsh . # to be executed on a Teleport auth server $ tctl auth sign --ttl = 87600h --user = jenkins --out = jenkins.pem CLI Docs - tctl auth sign Now jenkins.pem can be copied to the jenkins server and passed to -i (identity file) flag of tsh . Essentially tctl auth sign is an admin's equivalent of tsh login --out and allows for unrestricted certificate TTL values.","title":"SSH Certificates for Automation"},{"location":"user-manual/#exploring-the-cluster","text":"In a Teleport cluster, all nodes periodically ping the cluster's auth server and update their status. This allows Teleport users to see which nodes are online with the tsh ls command: # This command lists all nodes in the cluster which you previously logged in via \"tsh login\": $ tsh ls # Output: Node Name Node ID Address Labels --------- ------- ------- ------ turing 11111111 -dddd-4132 10 .1.0.5:3022 os:linux turing 22222222 -cccc-8274 10 .1.0.6:3022 os:linux graviton 33333333 -aaaa-1284 10 .1.0.7:3022 os:osx CLI Docs - tsh ls tsh ls can apply a filter based on the node labels. # only show nodes with os label set to 'osx': $ tsh ls os = osx Node Name Node ID Address Labels --------- ------- ------- ------ graviton 33333333 -aaaa-1284 10 .1.0.7:3022 os:osx CLI Docs -tsh ls","title":"Exploring the Cluster"},{"location":"user-manual/#interactive-shell","text":"To launch an interactive shell on a remote node or to execute a command, use tsh ssh . tsh tries to mimic the ssh experience as much as possible, so it supports the most popular ssh flags like -p , -l or -L . For example, if you have the following alias defined in your ~/.bashrc: alias ssh=\"tsh ssh\" then you can continue using familiar SSH syntax: # Have this alias configured, perhaps via ~/.bashrc $ alias ssh = \"/usr/local/bin/tsh ssh\" # Login into a cluster and retrieve your SSH certificate: $ tsh --proxy = proxy.example.com login # These commands execute `tsh ssh` under the hood: $ ssh user@node $ ssh -p 6122 user@node ls $ ssh -o ForwardAgent = yes user@node $ ssh -o AddKeysToAgent = yes user@node","title":"Interactive Shell"},{"location":"user-manual/#proxy-ports","text":"A Teleport proxy uses two ports: 3080 for HTTPS and 3023 for proxying SSH connections. The HTTPS port is used to serve Web UI and also to implement 2nd factor auth for the tsh client. If a Teleport proxy is configured to listen on non-default ports, they must be specified via --proxy flag as shown: tsh --proxy=proxy.example.com:5000,5001 <subcommand> This means use port 5000 for HTTPS and 5001 for SSH .","title":"Proxy Ports"},{"location":"user-manual/#port-forwarding","text":"tsh ssh supports the OpenSSH -L flag which forwards incoming connections from localhost to the specified remote host:port. The syntax of -L flag is as follows, where \"bind_ip\" defaults to 127.0.0.1 : -L [ bind_ip ] :listen_port:remote_host:remote_port Example: $ tsh ssh -L 5000 :web.remote:80 node This will connect to remote server node via proxy.example.com , then it will open a listening socket on localhost:5000 and will forward all incoming connections to web.remote:80 via this SSH tunnel. It is often convenient to establish port forwarding, execute a local command which uses the connection, and then disconnect. You can do this with the --local flag. Example: $ tsh ssh -L 5000 :google.com:80 --local node curl http://localhost:5000 This command: Connects to node Binds the local port 5000 to port 80 on google.com Executes curl command locally, which results in curl hitting google.com:80 via node","title":"Port Forwarding"},{"location":"user-manual/#ssh-jumphost","text":"While implementing ProxyJump for Teleport, we have extended the feature to tsh . $ tsh ssh -J proxy.example.com telenode Known limits: Only one jump host is supported ( -J supports chaining that Teleport does not utilise) and tsh will return with error in the case of two jumphosts, i.e. -J proxy-1.example.com,proxy-2.example.com will not work. When tsh ssh -J user@proxy is used, it overrides the SSH proxy defined in the tsh profile and port forwarding is used instead of the existing Teleport proxy subsystem.","title":"SSH Jumphost"},{"location":"user-manual/#resolving-node-names","text":"tsh supports multiple methods to resolve remote node names. Traditional: by IP address or via DNS. Nodename setting: teleport daemon supports nodename flag, which allows Teleport administrators to assign alternative node names. Labels: you can address a node by name=value pair. If we have two nodes, one with os:linux label and one node with os:osx , we can log into the OSX node with: $ tsh ssh os = osx This only works if there is only one remote node with the os:osx label, but you can still execute commands via SSH on multiple nodes using labels as a selector. This command will update all system packages on machines that run Linux: $ tsh ssh os = ubuntu apt-get update -y","title":"Resolving Node Names"},{"location":"user-manual/#short-lived-sessions","text":"The default TTL of a Teleport user certificate is 12 hours. This can be modified at login with the --ttl flag. This command logs you into the cluster with a very short-lived (1 minute) temporary certificate: $ tsh --ttl = 1 login You will be logged out after one minute, but if you want to log out immediately, you can always do: $ tsh logout","title":"Short-lived Sessions"},{"location":"user-manual/#copying-files","text":"To securely copy files to and from cluster nodes, use the tsh scp command. It is designed to mimic traditional scp as much as possible: $ tsh scp example.txt root@node:/path/to/dest Again, you may want to create a bash alias like alias scp=\"tsh --proxy=work scp\" and use the familiar syntax: $ scp -P 61122 -r files root@node:/path/to/dest","title":"Copying Files"},{"location":"user-manual/#sharing-sessions","text":"Suppose you are trying to troubleshoot a problem on a remote server. Sometimes it makes sense to ask another team member for help. Traditionally, this could be done by letting them know which node you're on, having them SSH in, start a terminal multiplexer like screen and join a session there. Teleport makes this more convenient. Let's log into a server named \"luna\" and ask Teleport for our current session status: $ tsh ssh luna >luna $ teleport status User ID : joe, logged in as joe from 10 .0.10.1 43026 3022 Session ID : 7645d523-60cb-436d-b732-99c5df14b7c4 Session URL: https://work:3080/web/sessions/7645d523-60cb-436d-b732-99c5df14b7c4 Now you can invite another user account to the \"work\" cluster. You can share the URL for access through a web browser, or you can share the session ID and she can join you through her terminal by typing: $ tsh join <session_ID> Note Joining sessions is not supported in recording proxy mode (where session_recording is set to proxy ).","title":"Sharing Sessions"},{"location":"user-manual/#connecting-to-ssh-clusters-behind-firewalls","text":"Teleport supports creating clusters of servers located behind firewalls without any open listening TCP ports . This works by creating reverse SSH tunnels from behind-firewall environments into a Teleport proxy you have access to. This feature is called \"Trusted Clusters\" . Refer to the admin manual to learn how a trusted cluster can be configured. Assuming the \"work\" Teleport proxy server is configured with a few trusted clusters, a user may use the tsh clusters command to see a list of all clusters on the server: $ tsh --proxy = work clusters Cluster Name Status ------------ ------ staging online production offline CLI Docs - tsh clusters Now you can use the --cluster flag with any tsh command. For example, to list SSH nodes that are members of the \"production\" cluster, simply do: $ tsh --proxy = work ls --cluster = production Node Name Node ID Address Labels --------- ------- ------- ------ db-1 xxxxxxxxx 10 .0.20.31:3022 kernel:4.4 db-2 xxxxxxxxx 10 .0.20.41:3022 kernel:4.2 Similarly, if you want to SSH into db-1 inside the \"production\" cluster: $ tsh --proxy = work ssh --cluster = production db-1 This is possible even if nodes in the \"production\" cluster are located behind a firewall without open ports. This works because the \"production\" cluster establishes a reverse SSH tunnel back into \"work\" proxy, and this tunnel is used to establish inbound SSH connections.","title":"Connecting to SSH Clusters behind Firewalls"},{"location":"user-manual/#web-ui","text":"Teleport proxy serves the web UI on https://proxyhost:3080 . The UI allows you to see the list of online nodes in a cluster, open a web-based terminal to them, see recorded sessions, and replay them. You can also join other users in active sessions.","title":"Web UI"},{"location":"user-manual/#using-openssh-client","text":"There are a few differences between Teleport's tsh and OpenSSH's ssh but most of them can be mitigated. tsh always requires the --proxy flag because tsh needs to know which cluster you are connecting to. But if you execute tsh --proxy=xxx login , the current proxy will be saved in your ~/.tsh profile and won't be needed for other tsh commands. tsh ssh operates two usernames: one for the cluster and another for the node you are trying to log into. See User Identities section below. For convenience, tsh assumes $USER for both by default. But again, if you use tsh login before tsh ssh , your Teleport username will be stored in ~/.tsh . If you'd like to set the login name that should be used by default on the remote host, you can set the TELEPORT_LOGIN environment variable. Tip To avoid typing tsh ssh user@host when logging into servers, you can create a symlink ssh -> tsh and execute the symlink. It will behave exactly like a standard ssh command, i.e. ssh login@host . This is helpful with other tools that expect ssh to just work. Teleport is built using standard SSH constructs: keys, certificates and protocols. This means that a Teleport system is 100% compatible with both OpenSSH clients and servers. For an OpenSSH client ( ssh ) to work with a Teleport proxy, two conditions must be met: ssh must be configured to connect through a Teleport proxy. ssh needs to be given the SSH certificate issued by the tsh login command.","title":"Using OpenSSH Client"},{"location":"user-manual/#ssh-proxy-configuration","text":"To configure ssh to use a Teleport proxy on proxy.example.com , a user must update the /etc/ssh/ssh_config or ~/.ssh/config . A few examples are shown below: # When \"ssh db\" is executed, OpenSSH will connect to proxy.example.com on port 3023 # and will request a proxied connection to \"db\" on port 3022 (default Teleport SSH port) Host db Port 3022 ProxyJump proxy.example.com:3023 The configuration above is all you need to ssh root@db if there's an SSH agent running on a client computer. You can verify it by executing ssh-add -L right after tsh login . If the SSH agent is running, the cluster certificates will be printed to stdout. If there is no ssh-agent available, the certificate must be passed to the OpenSSH client explicitly. When proxy is in \"Recording mode\" the following will happen with SSH: $ ssh -J user@teleport.proxy:3023 -p 3022 user@target -F ./forward.config Where forward.config enables agent forwarding: Host teleport.proxy ForwardAgent yes","title":"SSH Proxy Configuration"},{"location":"user-manual/#passing-teleport-ssh-certificate-to-openssh-client","text":"If a user does not want to use an SSH agent or if the agent is not available, the certificate must be passed to ssh via IdentityFile option (see man ssh_config ). Consider this example: the Teleport user \"joe\" wants to login into the proxy named \"lab.example.com\". He executes the tsh login command: $ tsh --proxy = lab.example.com login --user = joe His identity is now stored in ~/.tsh/keys/lab.example.com , so his ~/.ssh/config needs to look like this: # ~/.ssh/config file: Host *.lab.example.com Port 3022 IdentityFile ~/.tsh/keys/lab.example.com/joe ProxyCommand ssh -i ~/.tsh/keys/lab.example.com/joe -p 3023 %r@lab.example.com -s proxy:%h:%p Now he can SSH into any machine behind lab.example.com using the OpenSSH client: $ ssh jenkins.lab.example.com","title":"Passing Teleport SSH Certificate to OpenSSH Client"},{"location":"user-manual/#troubleshooting","text":"If you encounter strange behaviour, you may want to try to solve it by enabling the verbose logging by specifying -d flag when launching tsh . Also, you may want to reset it to a clean state by deleting temporary keys and other data from ~/.tsh","title":"Troubleshooting"},{"location":"user-manual/#getting-help","text":"If you need help, please ask on our community forum . You can also open an issue on Github . For commercial support, you can create a ticket through the customer dashboard . For more information about custom features, or to try our Enterprise edition of Teleport, please reach out to us at sales@gravitational.com .","title":"Getting Help"},{"location":"architecture/authentication/","text":"Teleport Authentication Service This document outlines the Teleport Authentication Service and Certificate Management. It explains how Users and Nodes are identified and granted access to Nodes and Services. Authentication vs. Authorization Teleport Auth handles both authentication and authorization. These topics are related but different, and they are often discussed jointly as \"Auth\". Authentication is proving an identity. \"I say I am Bob, and I really am Bob. See look I have Bob's purple hat.\" The job of an Authentication system is to define the criteria by which users must prove their identity. Is having a purple hat enough to show that a person is Bob? Maybe, maybe not. To identify users and nodes to Teleport Auth, we require them to present a cryptographically-signed certificate issued by the Teleport Auth Certificate Authority. Authorization is proving access to something: \"Bob has a purple hat, but also a debit card and the correct PIN code. Bob can access a bank account with the number 814000001344. Can Bob get $20 out of the ATM?\" The ATM's Authentication system would validate Bob's PIN Code, while the Authorization system would use a stored mapping from Bob to account #814000001344 to decide whether Bob could withdraw cash. Authorization defines and determines permissions that users have within a system, such as access to cash within a banking system, or data in a filesystem. Before users are granted access to nodes, the Auth Service checks their identity against a stored mapping in a database. SSH Certificates One can think of an SSH certificate as a \"permit\" issued and time-stamped by a trusted authority. In this case the authority is the Auth Server's Certificate Authority. A certificate contains four important pieces of data: List of principals (identities) this certificate belongs to. Signature of the certificate authority who issued it. The expiration date, also known as \"time-to-live\" or simply TTL. Additional data, such as the node role, stored as a certificate extension. Authentication in Teleport Teleport uses SSH certificates to authenticate nodes and users within a cluster. There are two CAs operating inside the Auth Server because nodes and users each need their own certificates. The Node CA issues certificates which identify a node (i.e. host, server, computer). These certificates are used to add new nodes to a cluster and identify connections coming from the node. The User CA issues certificates which identify a User. These certificates are used to authenticate users when they try to connect to a cluster node. Issuing Node Certificates Node Certificates identify a node within a cluster and establish the permissions of the node to access other Teleport services. The presence of a signed certificate on a node makes it a cluster member. To join a cluster for the first time, a node must present a \"join token\" to the auth server. The token can be static (configured via config file) or a dynamic, single-use token generated by tctl nodes add . Token TTL When using dynamic tokens, their default time to live (TTL) is 15 minutes, but it can be reduced (not increased) via tctl nodes add --ttl flag. When a new node joins the cluster, the auth server generates a new public/private keypair for the node and signs its certificate. This node certificate contains the node's role(s) ( proxy , auth or node ) as a certificate extension (opaque signed string). Using Node Certificates All nodes in a cluster can connect to the Auth Server's API implemented as an HTTP REST service running over the SSH tunnel. This API connection is authenticated with the node certificate and the encoded role is checked to enforce access control. For example, a client connection using a certificate with only the node role won't be able to add and delete users. This client connection would only be authorized to get auth servers registered in the cluster. Issuing User Certificates The Auth Server uses its User CA to issue user certificates. User certificates are stored on a user's machine in the ~/.tsh/<proxy_host> directory or also by the system's SSH agent if it is running. To get permission to join a cluster for the first time a user must provide their username, password, and 2nd-factor token. Users can log in with tsh login or via the Web UI. The Auth Server checks the username and password against its identity storage and checks the 2nd factor token. If the correct credentials were offered, the Auth Server will generate a signed certificate and return it to the client. For users, certificates are stored in ~/.tsh by default. If the client uses the Web UI the signed certificate is associated with a secure websocket session. In addition to a user's identity, user certificates also contain user roles and SSH options, like \"permit-agent-forwarding\" . This additional data is stored as a certificate extension and is protected by the CA signature. Using User Certificates When a client requests access to a node cluster, the Auth Server first checks that a certificate exists and hasn't expired. If it has expired, the client must re-authenticate with their username, password, and 2nd factor. If the certificate is still valid, the Auth Server validates the certificate's signature. The client is then granted access to the cluster. From here, the Proxy Server establishes a connection between client and node. Certificate Rotation By default, all user certificates have an expiration date, also known as time to live (TTL). This TTL can be configured by a Teleport administrator. However, the node certificates issued by an Auth Server are valid indefinitely by default. Teleport supports certificate rotation, i.e. the process of invalidating all previously-issued certificates for nodes and users regardless of their TTL. Certificate rotation is triggered by tctl auth rotate . When this command is invoked by a Teleport administrator on one of a cluster's Auth Servers, the following happens: A new certificate authority (CA) key is generated. The old CA will be considered valid alongside the new CA for some period of time. This period of time is called a grace period . During the grace period, all previously issued certificates will be considered valid, assuming their TTL isn't expired. After the grace period is over, the certificates issued by the old CA are no longer accepted. This process is repeated twice, once for the node CA and once for the user CA. Take a look at the Certificate Guide to learn how to do certificate rotation in practice. Auth API Clients can also connect to the auth API through the Teleport proxy to use a limited subset of the API to discover the member nodes of the cluster. Auth State The Auth service maintains state using a database of users, credentials, certificates, and audit logs. The default storage location is /var/lib/teleport or an admin-configured storage destination . There are three types of data stored by the auth server: Cluster State The auth server stores its own keys in a cluster state storage. All of cluster dynamic configuration is stored there as well, including: Node membership information and online/offline status for each node. List of active sessions. List of locally stored users RBAC configuration (roles and permissions). Other dynamic configuration. Audit Log When users log into a Teleport cluster, execute remote commands and logout, that activity is recorded in the audit log. See Audit Log for more details. More on this in the Audit Log section below . Recorded Sessions When Teleport users launch remote shells via tsh ssh command, their interactive sessions are recorded and stored by the auth server. Each recorded session is a file which is saved in /var/lib/teleport by default, but can also be saved in external storage, like an AWS S3 bucket. Audit Log The Teleport auth server keeps the audit log of SSH-related events that take place on any node within a Teleport cluster. Each node in a cluster emits audit events and submits them to the auth server. The events recorded include: successful user logins node IP addresses session time session IDs Compatibility Warning Because all SSH events like exec or session_start are by default reported by the Teleport node service, they will not be logged if you are using OpenSSH sshd daemon on your nodes. Recording proxy mode can be used to log audit events when using OpenSSH on your nodes. Only an SSH server can report what's happening to the Teleport auth server. The audit log is a JSON file which is by default stored on the auth server's filesystem under /var/lib/teleport/log . The format of the file is documented in the Admin Manual . Teleport users are encouraged to export the events into external, long term storage. Deployment Considerations If multiple Teleport auth servers are used to service the same cluster (HA mode) a network file system must be used for /var/lib/teleport/log to allow them to combine all audit events into the same audit log. Learn how to deploy Teleport in HA Mode. Storage Back-Ends Different types of cluster data can be configured with different storage back-ends as shown in the table below: Data Type Supported Back-ends Notes Cluster state dir , etcd , dynamodb , firestore Multi-server (HA) configuration is only supported using etcd , dynamodb , and firestore back-ends. Audit Log Events dir , dynamodb , firestore If dynamodb is used for the audit log events, s3 back-end must be used for the recorded sessions. Recorded Sessions dir , s3 s3 is mandatory if dynamodb is used for the audit log. For Google Cloud storage use audit_sessions_uri: 'gs:// Note The reason Teleport designers split the audit log events and the recorded sessions into different back-ends is because of the nature of the data. A recorded session is a compressed binary stream (blob) while the event is a well-defined JSON structure. dir works well enough for both in small deployments, but large clusters require specialized data stores: S3 is perfect for uploading session blobs, while DynamoDB or etcd are better suited to store the cluster state. The combination of DynamoDB + S3 is especially popular among AWS users because it allows them to run Teleport clusters completely devoid of local state. NOTE For high availability in production, a Teleport cluster can be serviced by multiple auth servers running in sync. Check HA configuration in the Admin Guide. More Concepts Architecture Overview Teleport Users Teleport Nodes Teleport Proxy","title":"Teleport Auth"},{"location":"architecture/authentication/#teleport-authentication-service","text":"This document outlines the Teleport Authentication Service and Certificate Management. It explains how Users and Nodes are identified and granted access to Nodes and Services.","title":"Teleport Authentication Service"},{"location":"architecture/authentication/#authentication-vs-authorization","text":"Teleport Auth handles both authentication and authorization. These topics are related but different, and they are often discussed jointly as \"Auth\". Authentication is proving an identity. \"I say I am Bob, and I really am Bob. See look I have Bob's purple hat.\" The job of an Authentication system is to define the criteria by which users must prove their identity. Is having a purple hat enough to show that a person is Bob? Maybe, maybe not. To identify users and nodes to Teleport Auth, we require them to present a cryptographically-signed certificate issued by the Teleport Auth Certificate Authority. Authorization is proving access to something: \"Bob has a purple hat, but also a debit card and the correct PIN code. Bob can access a bank account with the number 814000001344. Can Bob get $20 out of the ATM?\" The ATM's Authentication system would validate Bob's PIN Code, while the Authorization system would use a stored mapping from Bob to account #814000001344 to decide whether Bob could withdraw cash. Authorization defines and determines permissions that users have within a system, such as access to cash within a banking system, or data in a filesystem. Before users are granted access to nodes, the Auth Service checks their identity against a stored mapping in a database.","title":"Authentication vs. Authorization"},{"location":"architecture/authentication/#ssh-certificates","text":"One can think of an SSH certificate as a \"permit\" issued and time-stamped by a trusted authority. In this case the authority is the Auth Server's Certificate Authority. A certificate contains four important pieces of data: List of principals (identities) this certificate belongs to. Signature of the certificate authority who issued it. The expiration date, also known as \"time-to-live\" or simply TTL. Additional data, such as the node role, stored as a certificate extension.","title":"SSH Certificates"},{"location":"architecture/authentication/#authentication-in-teleport","text":"Teleport uses SSH certificates to authenticate nodes and users within a cluster. There are two CAs operating inside the Auth Server because nodes and users each need their own certificates. The Node CA issues certificates which identify a node (i.e. host, server, computer). These certificates are used to add new nodes to a cluster and identify connections coming from the node. The User CA issues certificates which identify a User. These certificates are used to authenticate users when they try to connect to a cluster node.","title":"Authentication in Teleport"},{"location":"architecture/authentication/#issuing-node-certificates","text":"Node Certificates identify a node within a cluster and establish the permissions of the node to access other Teleport services. The presence of a signed certificate on a node makes it a cluster member. To join a cluster for the first time, a node must present a \"join token\" to the auth server. The token can be static (configured via config file) or a dynamic, single-use token generated by tctl nodes add . Token TTL When using dynamic tokens, their default time to live (TTL) is 15 minutes, but it can be reduced (not increased) via tctl nodes add --ttl flag. When a new node joins the cluster, the auth server generates a new public/private keypair for the node and signs its certificate. This node certificate contains the node's role(s) ( proxy , auth or node ) as a certificate extension (opaque signed string).","title":"Issuing Node Certificates"},{"location":"architecture/authentication/#using-node-certificates","text":"All nodes in a cluster can connect to the Auth Server's API implemented as an HTTP REST service running over the SSH tunnel. This API connection is authenticated with the node certificate and the encoded role is checked to enforce access control. For example, a client connection using a certificate with only the node role won't be able to add and delete users. This client connection would only be authorized to get auth servers registered in the cluster.","title":"Using Node Certificates"},{"location":"architecture/authentication/#issuing-user-certificates","text":"The Auth Server uses its User CA to issue user certificates. User certificates are stored on a user's machine in the ~/.tsh/<proxy_host> directory or also by the system's SSH agent if it is running. To get permission to join a cluster for the first time a user must provide their username, password, and 2nd-factor token. Users can log in with tsh login or via the Web UI. The Auth Server checks the username and password against its identity storage and checks the 2nd factor token. If the correct credentials were offered, the Auth Server will generate a signed certificate and return it to the client. For users, certificates are stored in ~/.tsh by default. If the client uses the Web UI the signed certificate is associated with a secure websocket session. In addition to a user's identity, user certificates also contain user roles and SSH options, like \"permit-agent-forwarding\" . This additional data is stored as a certificate extension and is protected by the CA signature.","title":"Issuing User Certificates"},{"location":"architecture/authentication/#using-user-certificates","text":"When a client requests access to a node cluster, the Auth Server first checks that a certificate exists and hasn't expired. If it has expired, the client must re-authenticate with their username, password, and 2nd factor. If the certificate is still valid, the Auth Server validates the certificate's signature. The client is then granted access to the cluster. From here, the Proxy Server establishes a connection between client and node.","title":"Using User Certificates"},{"location":"architecture/authentication/#certificate-rotation","text":"By default, all user certificates have an expiration date, also known as time to live (TTL). This TTL can be configured by a Teleport administrator. However, the node certificates issued by an Auth Server are valid indefinitely by default. Teleport supports certificate rotation, i.e. the process of invalidating all previously-issued certificates for nodes and users regardless of their TTL. Certificate rotation is triggered by tctl auth rotate . When this command is invoked by a Teleport administrator on one of a cluster's Auth Servers, the following happens: A new certificate authority (CA) key is generated. The old CA will be considered valid alongside the new CA for some period of time. This period of time is called a grace period . During the grace period, all previously issued certificates will be considered valid, assuming their TTL isn't expired. After the grace period is over, the certificates issued by the old CA are no longer accepted. This process is repeated twice, once for the node CA and once for the user CA. Take a look at the Certificate Guide to learn how to do certificate rotation in practice.","title":"Certificate Rotation"},{"location":"architecture/authentication/#auth-api","text":"Clients can also connect to the auth API through the Teleport proxy to use a limited subset of the API to discover the member nodes of the cluster.","title":"Auth API"},{"location":"architecture/authentication/#auth-state","text":"The Auth service maintains state using a database of users, credentials, certificates, and audit logs. The default storage location is /var/lib/teleport or an admin-configured storage destination . There are three types of data stored by the auth server: Cluster State The auth server stores its own keys in a cluster state storage. All of cluster dynamic configuration is stored there as well, including: Node membership information and online/offline status for each node. List of active sessions. List of locally stored users RBAC configuration (roles and permissions). Other dynamic configuration. Audit Log When users log into a Teleport cluster, execute remote commands and logout, that activity is recorded in the audit log. See Audit Log for more details. More on this in the Audit Log section below . Recorded Sessions When Teleport users launch remote shells via tsh ssh command, their interactive sessions are recorded and stored by the auth server. Each recorded session is a file which is saved in /var/lib/teleport by default, but can also be saved in external storage, like an AWS S3 bucket.","title":"Auth State"},{"location":"architecture/authentication/#audit-log","text":"The Teleport auth server keeps the audit log of SSH-related events that take place on any node within a Teleport cluster. Each node in a cluster emits audit events and submits them to the auth server. The events recorded include: successful user logins node IP addresses session time session IDs Compatibility Warning Because all SSH events like exec or session_start are by default reported by the Teleport node service, they will not be logged if you are using OpenSSH sshd daemon on your nodes. Recording proxy mode can be used to log audit events when using OpenSSH on your nodes. Only an SSH server can report what's happening to the Teleport auth server. The audit log is a JSON file which is by default stored on the auth server's filesystem under /var/lib/teleport/log . The format of the file is documented in the Admin Manual . Teleport users are encouraged to export the events into external, long term storage. Deployment Considerations If multiple Teleport auth servers are used to service the same cluster (HA mode) a network file system must be used for /var/lib/teleport/log to allow them to combine all audit events into the same audit log. Learn how to deploy Teleport in HA Mode.","title":"Audit Log"},{"location":"architecture/authentication/#storage-back-ends","text":"Different types of cluster data can be configured with different storage back-ends as shown in the table below: Data Type Supported Back-ends Notes Cluster state dir , etcd , dynamodb , firestore Multi-server (HA) configuration is only supported using etcd , dynamodb , and firestore back-ends. Audit Log Events dir , dynamodb , firestore If dynamodb is used for the audit log events, s3 back-end must be used for the recorded sessions. Recorded Sessions dir , s3 s3 is mandatory if dynamodb is used for the audit log. For Google Cloud storage use audit_sessions_uri: 'gs:// Note The reason Teleport designers split the audit log events and the recorded sessions into different back-ends is because of the nature of the data. A recorded session is a compressed binary stream (blob) while the event is a well-defined JSON structure. dir works well enough for both in small deployments, but large clusters require specialized data stores: S3 is perfect for uploading session blobs, while DynamoDB or etcd are better suited to store the cluster state. The combination of DynamoDB + S3 is especially popular among AWS users because it allows them to run Teleport clusters completely devoid of local state. NOTE For high availability in production, a Teleport cluster can be serviced by multiple auth servers running in sync. Check HA configuration in the Admin Guide.","title":"Storage Back-Ends"},{"location":"architecture/authentication/#more-concepts","text":"Architecture Overview Teleport Users Teleport Nodes Teleport Proxy","title":"More Concepts"},{"location":"architecture/nodes/","text":"Teleport Nodes Teleport calls any computing device (server, VM, AWS intance, etc) a \"node\". The Node Service A node becomes a Teleport Node when the node joins a cluster with a \"join\" token. Read about how nodes are issued certificates in the Auth Guide . A Teleport Node runs the teleport daemon with the node role. This process handles incoming connection requests, authentication, and remote command execution on the node, similar to the function of OpenSSH's sshd . All cluster Nodes keep the Auth Server updated on their status with periodic ping messages. They report their IP addresses and values of their assigned labels. Nodes can access the list of all Nodes in their cluster via the Auth Server API . Tip In most environments we advise replacing the OpenSSH daemon sshd with the Teleport Node Service unless there are existing workflows relying on ssh or in special cases such as embedded devices that can't run custom binaries. The node service provides SSH access to every node with all of the following clients: OpenSSH: ssh Teleport CLI client: tsh ssh Teleport Proxy UI accessed via a web browser. Each client is authenticated via the Auth Service before being granted access to a Node. Node Identity on a Cluster Node Identity is defined on the Cluster level by the certificate a node possesses. This certificate contains information about the node including: The host ID , a generated UUID unique to a node A nodename , which defaults to hostname of the node, but can be configured. The cluster_name , which defaults to the hostname of the auth server, but can be configured The node role (i.e. node,proxy ) encoded as a certificate extension The cert TTL (time-to-live) A Teleport Cluster is a set of one or more machines whose certificates are signed by the same certificate authority (CA) operating in the Auth Server. A certificate is issued to a node when it joins the cluster for the first time. Learn more about this process in the Auth Guide . Single-Node Clusters are Clusters Once a Node gets a signed certificate from the Node CA, the Node is considered a member of the cluster, even if that cluster has only one node. Connecting to Nodes When a client requests access to a Node, authentication is always performed through a cluster proxy. When the proxy server receives a connection request from a client it validates the client's credentials with the Auth Service. Once the client is authenticated the proxy attempts to connect the client to the requested Node. There is a detailed walk-through of the steps needed to initiate a connection to a node in the Architecture Overview . Cluster State Cluster state is stored in a central storage location configured by the Auth Server. This means that each node is completely stateless and holds no secrets such as keys or passwords. The cluster state information stored includes: Node membership information and online/offline status for each node. List of active sessions. List of locally stored users. RBAC configuration (roles and permissions). Dynamic configuration. Read more about what is stored in the Auth Guide Session Recording By default, nodes submit SSH session traffic to the Auth server for storage. These recorded sessions can be replayed later via tsh play command or in a web browser. Some Teleport users assume that audit and session recording happen by default on the Teleport proxy server. This is not the case in default configuration because a proxy cannot see the encrypted traffic, it is encrypted end-to-end, i.e. from an SSH client to an SSH server/node, see the diagram below: However, starting from Teleport 2.4, it is possible to configure the Teleport proxy to enable \"recording proxy mode\". Trusted Clusters Teleport Auth Service can allow 3rd party users or nodes to connect to cluster nodes if their certificates are signed by a trusted CA. A \"trusted cluster\" is a public key of the trusted CA. It can be configured via teleport.yaml file. More Concepts Architecture Overview Teleport Users Teleport Auth Teleport Proxy","title":"Teleport Nodes"},{"location":"architecture/nodes/#teleport-nodes","text":"Teleport calls any computing device (server, VM, AWS intance, etc) a \"node\".","title":"Teleport Nodes"},{"location":"architecture/nodes/#the-node-service","text":"A node becomes a Teleport Node when the node joins a cluster with a \"join\" token. Read about how nodes are issued certificates in the Auth Guide . A Teleport Node runs the teleport daemon with the node role. This process handles incoming connection requests, authentication, and remote command execution on the node, similar to the function of OpenSSH's sshd . All cluster Nodes keep the Auth Server updated on their status with periodic ping messages. They report their IP addresses and values of their assigned labels. Nodes can access the list of all Nodes in their cluster via the Auth Server API . Tip In most environments we advise replacing the OpenSSH daemon sshd with the Teleport Node Service unless there are existing workflows relying on ssh or in special cases such as embedded devices that can't run custom binaries. The node service provides SSH access to every node with all of the following clients: OpenSSH: ssh Teleport CLI client: tsh ssh Teleport Proxy UI accessed via a web browser. Each client is authenticated via the Auth Service before being granted access to a Node.","title":"The Node Service"},{"location":"architecture/nodes/#node-identity-on-a-cluster","text":"Node Identity is defined on the Cluster level by the certificate a node possesses. This certificate contains information about the node including: The host ID , a generated UUID unique to a node A nodename , which defaults to hostname of the node, but can be configured. The cluster_name , which defaults to the hostname of the auth server, but can be configured The node role (i.e. node,proxy ) encoded as a certificate extension The cert TTL (time-to-live) A Teleport Cluster is a set of one or more machines whose certificates are signed by the same certificate authority (CA) operating in the Auth Server. A certificate is issued to a node when it joins the cluster for the first time. Learn more about this process in the Auth Guide . Single-Node Clusters are Clusters Once a Node gets a signed certificate from the Node CA, the Node is considered a member of the cluster, even if that cluster has only one node.","title":"Node Identity on a Cluster"},{"location":"architecture/nodes/#connecting-to-nodes","text":"When a client requests access to a Node, authentication is always performed through a cluster proxy. When the proxy server receives a connection request from a client it validates the client's credentials with the Auth Service. Once the client is authenticated the proxy attempts to connect the client to the requested Node. There is a detailed walk-through of the steps needed to initiate a connection to a node in the Architecture Overview .","title":"Connecting to Nodes"},{"location":"architecture/nodes/#cluster-state","text":"Cluster state is stored in a central storage location configured by the Auth Server. This means that each node is completely stateless and holds no secrets such as keys or passwords. The cluster state information stored includes: Node membership information and online/offline status for each node. List of active sessions. List of locally stored users. RBAC configuration (roles and permissions). Dynamic configuration. Read more about what is stored in the Auth Guide","title":"Cluster State"},{"location":"architecture/nodes/#session-recording","text":"By default, nodes submit SSH session traffic to the Auth server for storage. These recorded sessions can be replayed later via tsh play command or in a web browser. Some Teleport users assume that audit and session recording happen by default on the Teleport proxy server. This is not the case in default configuration because a proxy cannot see the encrypted traffic, it is encrypted end-to-end, i.e. from an SSH client to an SSH server/node, see the diagram below: However, starting from Teleport 2.4, it is possible to configure the Teleport proxy to enable \"recording proxy mode\".","title":"Session Recording"},{"location":"architecture/nodes/#trusted-clusters","text":"Teleport Auth Service can allow 3rd party users or nodes to connect to cluster nodes if their certificates are signed by a trusted CA. A \"trusted cluster\" is a public key of the trusted CA. It can be configured via teleport.yaml file.","title":"Trusted Clusters"},{"location":"architecture/nodes/#more-concepts","text":"Architecture Overview Teleport Users Teleport Auth Teleport Proxy","title":"More Concepts"},{"location":"architecture/overview/","text":"Architecture Introduction This guide is for those looking for a deeper understanding of Teleport. If you are looking for hands-on instructions on how to set up Teleport for your team, check out the Admin Guide Design Principles Teleport was designed in accordance with the following principles: Off the Shelf Security : Teleport does not re-implement any security primitives and uses well-established, popular implementations of the encryption and network protocols. Open Standards : There is no security through obscurity. Teleport is fully compatible with existing and open standards and other software, including OpenSSH . Cluster-Oriented Design : Teleport is built for managing clusters, not individual servers. In practice this means that hosts and Users have cluster memberships. Identity management and authorization happen on a cluster level. Built for Teams : Teleport was created under the assumption of multiple teams operating on several disconnected clusters. Example use cases might be production-vs-staging environment, or a cluster-per-customer or cluster-per-application basis. This doc introduces the basic concepts of Teleport so you can get started managing access! Definitions Here are definitions of the key concepts you will use in Teleport. Concept Description Node A node is a \"server\", \"host\" or \"computer\". Users can create shell sessions to access nodes remotely. User A user represents someone (a person) or something (a machine) who can perform a set of operations on a node. Cluster A cluster is a group of nodes that work together and can be considered a single system. Cluster nodes can create connections to each other, often over a private network. Cluster nodes often require TLS authentication to ensure that communication between nodes remains secure and comes from a trusted source. Certificate Authority (CA) A Certificate Authority issues SSL certificates in the form of public/private keypairs. Teleport Node A Teleport Node is a regular node that is running the Teleport Node service. Teleport Nodes can be accessed by authorized Teleport Users. A Teleport Node is always considered a member of a Teleport Cluster, even if it's a single-node cluster. Teleport User A Teleport User represents someone who needs access to a Teleport Cluster. Users have stored usernames and passwords, and are mapped to OS users on each node. User data is stored locally or in an external store. Teleport Cluster A Teleport Cluster is comprised of one or more nodes, each of which hold certificates signed by the same Auth Server CA . The CA cryptographically signs the certificate of a node, establishing cluster membership. Teleport CA Teleport operates two internal CAs as a function of the Auth service. One is used to sign User certificates and the other signs Node certificates. Each certificate is used to prove identity, cluster membership and manage access. Teleport Services Teleport uses three services which work together: Nodes , Auth , and Proxy . Teleport Nodes are servers which can be accessed remotely with SSH. The Teleport Node service runs on a machine and is similar to the sshd daemon you may be familiar with. Users can log in to a Teleport Node with all of the following clients: OpenSSH: ssh (works on Linux, MacOS and Windows) Teleport CLI client: tsh ssh (works on Linux and MacOS) Teleport Proxy UI accessed via any modern web browser (including Safari on iOS and Chrome on Android) Teleport Auth authenticates Users and Nodes, authorizes User access to Nodes, and acts as a CA by signing certificates issued to Users and Nodes. Teleport Proxy forwards User credentials to the Auth Service , creates connections to a requested Node after successful authentication, and serves a Web UI . Basic Architecture Overview The numbers correspond to the steps needed to connect a client to a node. These steps are explained below the diagram. Caution The teleport daemon calls services \"roles\" in the CLI client. The --roles flag has no relationship to concept of User Roles or permissions. Initiate Client Connection Authenticate Client Connect to Node Authorize Client Access to Node Tip In the diagram above we show each Teleport service separately for clarity, but Teleport services do not have to run on separate nodes. Teleport can be run as a binary on a single-node cluster with no external storage backend. We demonstrate this minimal setup in the Quickstart Guide . Detailed Architecture Overview Here is a detailed diagram of a Teleport Cluster. The numbers correspond to the steps needed to connect a client to a node. These steps are explained in detail below the diagram. Caution The Teleport Admin tool, tctl , must be physically present on the same machine where Teleport Auth is running. Adding new nodes or inviting new users to the cluster is only possible using this tool. 1: Initiate Client Connection The client tries to establish an SSH connection to a proxy using the CLI interface or a web browser. When establishing a connection, the client offers its certificate. Clients must always connect through a proxy for two reasons: Individual nodes may not always be reachable from outside a secure network. Proxies always record SSH sessions and keep track of active user sessions. This makes it possible for an SSH user to see if someone else is connected to a node she is about to work on. 2: Authenticate Client Certificate The proxy checks if the submitted certificate has been previously signed by the auth server. If there was no certificate previously offered (first time login) or if the certificate has expired, the proxy denies the connection and asks the client to login interactively using a password and a 2nd factor if enabled. Teleport supports Google Authenticator , Authy , or another TOTP generator. The password + 2nd factor are submitted to a proxy via HTTPS, therefore it is critical for a secure configuration of Teleport to install a proper HTTPS certificate on a proxy. Warning Do not use self-signed SSL/HTTPS certificates in production! If the credentials are correct, the auth server generates and signs a new certificate and returns it to the client via the proxy. The client stores this certificate and will use it for subsequent logins. The certificate will automatically expire after 12 hours by default. This TTL can be configured to another value by the cluster administrator. 3: Lookup Node At this step, the proxy tries to locate the requested node in a cluster. There are three lookup mechanisms a proxy uses to find the node's IP address: Uses DNS to resolve the name requested by the client. Asks the Auth Server if there is a Node registered with this nodename . Asks the Auth Server to find a node (or nodes) with a label that matches the requested name. If the node is located, the proxy establishes the connection between the client and the requested node. The destination node then begins recording the session, sending the session history to the auth server to be stored. Note Teleport may also be configured to have the session recording occur on the proxy, see Audit Log for more information. 4: Authenticate Node Certificate When the node receives a connection request, it checks with the Auth Server to validate the node's certificate and validate the Node's cluster membership. If the node certificate is valid, the node is allowed to access the Auth Server API which provides access to information about nodes and users in the cluster. 5: Grant User Node Access The node requests the Auth Server to provide a list of OS users (user mappings) for the connecting client, to make sure the client is authorized to use the requested OS login. Finally, the client is authorized to create an SSH connection to a node. Teleport CLI Tools Teleport offers two command line tools. tsh is a client tool used by the end users, while tctl is used for cluster administration. TSH tsh is similar in nature to OpenSSH ssh or scp . In fact, it has subcommands named after them so you can call: $ tsh --proxy=p ssh -p 1522 user@host $ tsh --proxy=p scp -P example.txt user@host/destination/dir Unlike ssh , tsh is very opinionated about authentication: it always uses auto-expiring certificates and it always connects to Teleport nodes via a proxy. When tsh logs in, the auto-expiring certificate is stored in ~/.tsh and is valid for 12 hours by default, unless you specify another interval via the --ttl flag (capped by the server-side configuration). You can learn more about tsh in the User Manual . TCTL tctl is used to administer a Teleport cluster. It connects to the Auth server listening on 127.0.0.1 and allows a cluster administrator to manage nodes and users in the cluster. tctl is also a tool which can be used to modify the dynamic configuration of the cluster, like creating new user roles or connecting trusted clusters. You can learn more about tctl in the Admin Manual . Next Steps If you haven't already, read the Quickstart Guide to run a minimal setup of Teleport yourself. Set up Teleport for your team with the Admin Guide . Read the rest of the Architecture Guides: Teleport Users Teleport Nodes Teleport Auth Teleport Proxy","title":"Architecture Overview"},{"location":"architecture/overview/#architecture-introduction","text":"This guide is for those looking for a deeper understanding of Teleport. If you are looking for hands-on instructions on how to set up Teleport for your team, check out the Admin Guide","title":"Architecture Introduction"},{"location":"architecture/overview/#design-principles","text":"Teleport was designed in accordance with the following principles: Off the Shelf Security : Teleport does not re-implement any security primitives and uses well-established, popular implementations of the encryption and network protocols. Open Standards : There is no security through obscurity. Teleport is fully compatible with existing and open standards and other software, including OpenSSH . Cluster-Oriented Design : Teleport is built for managing clusters, not individual servers. In practice this means that hosts and Users have cluster memberships. Identity management and authorization happen on a cluster level. Built for Teams : Teleport was created under the assumption of multiple teams operating on several disconnected clusters. Example use cases might be production-vs-staging environment, or a cluster-per-customer or cluster-per-application basis. This doc introduces the basic concepts of Teleport so you can get started managing access!","title":"Design Principles"},{"location":"architecture/overview/#definitions","text":"Here are definitions of the key concepts you will use in Teleport. Concept Description Node A node is a \"server\", \"host\" or \"computer\". Users can create shell sessions to access nodes remotely. User A user represents someone (a person) or something (a machine) who can perform a set of operations on a node. Cluster A cluster is a group of nodes that work together and can be considered a single system. Cluster nodes can create connections to each other, often over a private network. Cluster nodes often require TLS authentication to ensure that communication between nodes remains secure and comes from a trusted source. Certificate Authority (CA) A Certificate Authority issues SSL certificates in the form of public/private keypairs. Teleport Node A Teleport Node is a regular node that is running the Teleport Node service. Teleport Nodes can be accessed by authorized Teleport Users. A Teleport Node is always considered a member of a Teleport Cluster, even if it's a single-node cluster. Teleport User A Teleport User represents someone who needs access to a Teleport Cluster. Users have stored usernames and passwords, and are mapped to OS users on each node. User data is stored locally or in an external store. Teleport Cluster A Teleport Cluster is comprised of one or more nodes, each of which hold certificates signed by the same Auth Server CA . The CA cryptographically signs the certificate of a node, establishing cluster membership. Teleport CA Teleport operates two internal CAs as a function of the Auth service. One is used to sign User certificates and the other signs Node certificates. Each certificate is used to prove identity, cluster membership and manage access.","title":"Definitions"},{"location":"architecture/overview/#teleport-services","text":"Teleport uses three services which work together: Nodes , Auth , and Proxy . Teleport Nodes are servers which can be accessed remotely with SSH. The Teleport Node service runs on a machine and is similar to the sshd daemon you may be familiar with. Users can log in to a Teleport Node with all of the following clients: OpenSSH: ssh (works on Linux, MacOS and Windows) Teleport CLI client: tsh ssh (works on Linux and MacOS) Teleport Proxy UI accessed via any modern web browser (including Safari on iOS and Chrome on Android) Teleport Auth authenticates Users and Nodes, authorizes User access to Nodes, and acts as a CA by signing certificates issued to Users and Nodes. Teleport Proxy forwards User credentials to the Auth Service , creates connections to a requested Node after successful authentication, and serves a Web UI .","title":"Teleport Services"},{"location":"architecture/overview/#basic-architecture-overview","text":"The numbers correspond to the steps needed to connect a client to a node. These steps are explained below the diagram. Caution The teleport daemon calls services \"roles\" in the CLI client. The --roles flag has no relationship to concept of User Roles or permissions. Initiate Client Connection Authenticate Client Connect to Node Authorize Client Access to Node Tip In the diagram above we show each Teleport service separately for clarity, but Teleport services do not have to run on separate nodes. Teleport can be run as a binary on a single-node cluster with no external storage backend. We demonstrate this minimal setup in the Quickstart Guide .","title":"Basic Architecture Overview"},{"location":"architecture/overview/#detailed-architecture-overview","text":"Here is a detailed diagram of a Teleport Cluster. The numbers correspond to the steps needed to connect a client to a node. These steps are explained in detail below the diagram. Caution The Teleport Admin tool, tctl , must be physically present on the same machine where Teleport Auth is running. Adding new nodes or inviting new users to the cluster is only possible using this tool.","title":"Detailed Architecture Overview"},{"location":"architecture/overview/#1-initiate-client-connection","text":"The client tries to establish an SSH connection to a proxy using the CLI interface or a web browser. When establishing a connection, the client offers its certificate. Clients must always connect through a proxy for two reasons: Individual nodes may not always be reachable from outside a secure network. Proxies always record SSH sessions and keep track of active user sessions. This makes it possible for an SSH user to see if someone else is connected to a node she is about to work on.","title":"1: Initiate Client Connection"},{"location":"architecture/overview/#2-authenticate-client-certificate","text":"The proxy checks if the submitted certificate has been previously signed by the auth server. If there was no certificate previously offered (first time login) or if the certificate has expired, the proxy denies the connection and asks the client to login interactively using a password and a 2nd factor if enabled. Teleport supports Google Authenticator , Authy , or another TOTP generator. The password + 2nd factor are submitted to a proxy via HTTPS, therefore it is critical for a secure configuration of Teleport to install a proper HTTPS certificate on a proxy. Warning Do not use self-signed SSL/HTTPS certificates in production! If the credentials are correct, the auth server generates and signs a new certificate and returns it to the client via the proxy. The client stores this certificate and will use it for subsequent logins. The certificate will automatically expire after 12 hours by default. This TTL can be configured to another value by the cluster administrator.","title":"2: Authenticate Client Certificate"},{"location":"architecture/overview/#3-lookup-node","text":"At this step, the proxy tries to locate the requested node in a cluster. There are three lookup mechanisms a proxy uses to find the node's IP address: Uses DNS to resolve the name requested by the client. Asks the Auth Server if there is a Node registered with this nodename . Asks the Auth Server to find a node (or nodes) with a label that matches the requested name. If the node is located, the proxy establishes the connection between the client and the requested node. The destination node then begins recording the session, sending the session history to the auth server to be stored. Note Teleport may also be configured to have the session recording occur on the proxy, see Audit Log for more information.","title":"3: Lookup Node"},{"location":"architecture/overview/#4-authenticate-node-certificate","text":"When the node receives a connection request, it checks with the Auth Server to validate the node's certificate and validate the Node's cluster membership. If the node certificate is valid, the node is allowed to access the Auth Server API which provides access to information about nodes and users in the cluster.","title":"4: Authenticate Node Certificate"},{"location":"architecture/overview/#5-grant-user-node-access","text":"The node requests the Auth Server to provide a list of OS users (user mappings) for the connecting client, to make sure the client is authorized to use the requested OS login. Finally, the client is authorized to create an SSH connection to a node.","title":"5: Grant User Node Access"},{"location":"architecture/overview/#teleport-cli-tools","text":"Teleport offers two command line tools. tsh is a client tool used by the end users, while tctl is used for cluster administration.","title":"Teleport CLI Tools"},{"location":"architecture/overview/#tsh","text":"tsh is similar in nature to OpenSSH ssh or scp . In fact, it has subcommands named after them so you can call: $ tsh --proxy=p ssh -p 1522 user@host $ tsh --proxy=p scp -P example.txt user@host/destination/dir Unlike ssh , tsh is very opinionated about authentication: it always uses auto-expiring certificates and it always connects to Teleport nodes via a proxy. When tsh logs in, the auto-expiring certificate is stored in ~/.tsh and is valid for 12 hours by default, unless you specify another interval via the --ttl flag (capped by the server-side configuration). You can learn more about tsh in the User Manual .","title":"TSH"},{"location":"architecture/overview/#tctl","text":"tctl is used to administer a Teleport cluster. It connects to the Auth server listening on 127.0.0.1 and allows a cluster administrator to manage nodes and users in the cluster. tctl is also a tool which can be used to modify the dynamic configuration of the cluster, like creating new user roles or connecting trusted clusters. You can learn more about tctl in the Admin Manual .","title":"TCTL"},{"location":"architecture/overview/#next-steps","text":"If you haven't already, read the Quickstart Guide to run a minimal setup of Teleport yourself. Set up Teleport for your team with the Admin Guide . Read the rest of the Architecture Guides: Teleport Users Teleport Nodes Teleport Auth Teleport Proxy","title":"Next Steps"},{"location":"architecture/proxy/","text":"The Proxy Service The proxy is a stateless service which performs three main functions in a Teleport cluster: It serves as an authentication gateway. It asks for credentials from connecting clients and forwards them to the Auth server via Auth API . It looks up the IP address for a requested Node and then proxies a connection from client to Node. It serves a Web UI which is used by cluster users to sign up and configure their accounts, explore nodes in a cluster, log into remote nodes, join existing SSH sessions or replay recorded sessions. Connecting to a Node Web to SSH Proxy In this mode, Teleport Proxy implements WSS - secure web sockets - to proxy a client SSH connection: User logs in to Web UI using username and password, and 2nd factor token if configured (2FA Tokens are not used with SSO providers). Proxy passes credentials to the Auth Server's API If Auth Server accepts credentials, it generates a new web session and generates a special ssh keypair associated with this web session. Auth server starts serving OpenSSH ssh-agent protocol to the proxy. The User obtains an SSH session in the Web UI and can interact with the node on a web-based terminal. From the Node's perspective, it's a regular SSH client connection that is authenticated using an OpenSSH certificate, so no special logic is needed. SSL Encryption When using the web UI, the Teleport Proxy terminates SSL traffic and re-encodes data for the SSH client connection. CLI to SSH Proxy Getting Client Certificates Teleport Proxy implements a special method to let clients get short-lived authentication certificates signed by the Certificate Authority (CA) provided by the Auth Service . A tsh client generates an OpenSSH keypair. It forwards the generated public key, username, password and second factor token to the proxy. The Proxy Service forwards request to the Auth Service. If Auth Service accepts credentials, it generates a new certificate signed by its user CA and sends it back to the Proxy Server. The certificate has a TTL which defaults to 12 hours, but can be configured in tctl . The Proxy Server returns the user certificate to the client and client stores it in ~/.tsh/keys . The certificate is also added to the local SSH agent if one is running. Using Client Certificates Once the client has obtained a certificate, it can use it to authenticate with any Node in the cluster. Users can use the certificate using a standard OpenSSH client ssh or using tsh : A client connects to the Proxy Server and provides target node's host and port location. There are three lookup mechanisms a proxy uses to find the node's IP address: Use DNS to resolve the name requested by the client. Asks the Auth Service if there is a Node registered with this nodename . Asks the Auth Service to find a node (or nodes) with a label that matches the requested name. If the node is located, the Proxy establishes an SSH connection to the requested node and starts forwarding traffic from Node to client. The client uses the established SSH tunnel from Proxy to Node to open a new SSH connection. The client authenticates with the target Node using its client certificate. NOTE Teleport's proxy command makes it compatible with SSH jump hosts implemented using OpenSSH's ProxyCommand . It also supports OpenSSH's ProxyJump/ssh -J implementation as of Teleport 4.1. See User Manual Recording Proxy Mode In this mode, the proxy terminates (decrypts) the SSH connection using the certificate supplied by the client via SSH agent forwarding and then establishes its own SSH connection to the final destination server, effectively becoming an authorized \"man in the middle\". This allows the proxy server to forward SSH session data to the auth server to be recorded, as shown below: The recording proxy mode, although less secure , was added to allow Teleport users to enable session recording for OpenSSH's servers running sshd , which is helpful when gradually transitioning large server fleets to Teleport. We consider the \"recording proxy mode\" to be less secure for two reasons: It grants additional privileges to the Teleport proxy. In the default mode, the proxy stores no secrets and cannot \"see\" the decrypted data. This makes a proxy less critical to the security of the overall cluster. But if an attacker gains physical access to a proxy node running in the \"recording\" mode, they will be able to see the decrypted traffic and client keys stored in proxy's process memory. Recording proxy mode requires the SSH agent forwarding. Agent forwarding is required because without it, a proxy will not be able to establish the 2nd connection to the destination node. However, there are advantages of proxy-based session recording too. When sessions are recorded at the nodes, a root user can add iptables rules to prevent sessions logs from reaching the Auth Service. With sessions recorded at the proxy, users with root privileges on nodes have no way of disabling the audit. See the admin guide to learn how to turn on the recording proxy mode. Note that the recording mode is configured on the Auth Service. More Concepts Architecture Overview Teleport Users Teleport Auth Teleport Proxy","title":"Teleport Proxy"},{"location":"architecture/proxy/#the-proxy-service","text":"The proxy is a stateless service which performs three main functions in a Teleport cluster: It serves as an authentication gateway. It asks for credentials from connecting clients and forwards them to the Auth server via Auth API . It looks up the IP address for a requested Node and then proxies a connection from client to Node. It serves a Web UI which is used by cluster users to sign up and configure their accounts, explore nodes in a cluster, log into remote nodes, join existing SSH sessions or replay recorded sessions.","title":"The Proxy Service"},{"location":"architecture/proxy/#connecting-to-a-node","text":"","title":"Connecting to a Node"},{"location":"architecture/proxy/#web-to-ssh-proxy","text":"In this mode, Teleport Proxy implements WSS - secure web sockets - to proxy a client SSH connection: User logs in to Web UI using username and password, and 2nd factor token if configured (2FA Tokens are not used with SSO providers). Proxy passes credentials to the Auth Server's API If Auth Server accepts credentials, it generates a new web session and generates a special ssh keypair associated with this web session. Auth server starts serving OpenSSH ssh-agent protocol to the proxy. The User obtains an SSH session in the Web UI and can interact with the node on a web-based terminal. From the Node's perspective, it's a regular SSH client connection that is authenticated using an OpenSSH certificate, so no special logic is needed. SSL Encryption When using the web UI, the Teleport Proxy terminates SSL traffic and re-encodes data for the SSH client connection.","title":"Web to SSH Proxy"},{"location":"architecture/proxy/#cli-to-ssh-proxy","text":"Getting Client Certificates Teleport Proxy implements a special method to let clients get short-lived authentication certificates signed by the Certificate Authority (CA) provided by the Auth Service . A tsh client generates an OpenSSH keypair. It forwards the generated public key, username, password and second factor token to the proxy. The Proxy Service forwards request to the Auth Service. If Auth Service accepts credentials, it generates a new certificate signed by its user CA and sends it back to the Proxy Server. The certificate has a TTL which defaults to 12 hours, but can be configured in tctl . The Proxy Server returns the user certificate to the client and client stores it in ~/.tsh/keys . The certificate is also added to the local SSH agent if one is running. Using Client Certificates Once the client has obtained a certificate, it can use it to authenticate with any Node in the cluster. Users can use the certificate using a standard OpenSSH client ssh or using tsh : A client connects to the Proxy Server and provides target node's host and port location. There are three lookup mechanisms a proxy uses to find the node's IP address: Use DNS to resolve the name requested by the client. Asks the Auth Service if there is a Node registered with this nodename . Asks the Auth Service to find a node (or nodes) with a label that matches the requested name. If the node is located, the Proxy establishes an SSH connection to the requested node and starts forwarding traffic from Node to client. The client uses the established SSH tunnel from Proxy to Node to open a new SSH connection. The client authenticates with the target Node using its client certificate. NOTE Teleport's proxy command makes it compatible with SSH jump hosts implemented using OpenSSH's ProxyCommand . It also supports OpenSSH's ProxyJump/ssh -J implementation as of Teleport 4.1. See User Manual","title":"CLI to SSH Proxy"},{"location":"architecture/proxy/#recording-proxy-mode","text":"In this mode, the proxy terminates (decrypts) the SSH connection using the certificate supplied by the client via SSH agent forwarding and then establishes its own SSH connection to the final destination server, effectively becoming an authorized \"man in the middle\". This allows the proxy server to forward SSH session data to the auth server to be recorded, as shown below: The recording proxy mode, although less secure , was added to allow Teleport users to enable session recording for OpenSSH's servers running sshd , which is helpful when gradually transitioning large server fleets to Teleport. We consider the \"recording proxy mode\" to be less secure for two reasons: It grants additional privileges to the Teleport proxy. In the default mode, the proxy stores no secrets and cannot \"see\" the decrypted data. This makes a proxy less critical to the security of the overall cluster. But if an attacker gains physical access to a proxy node running in the \"recording\" mode, they will be able to see the decrypted traffic and client keys stored in proxy's process memory. Recording proxy mode requires the SSH agent forwarding. Agent forwarding is required because without it, a proxy will not be able to establish the 2nd connection to the destination node. However, there are advantages of proxy-based session recording too. When sessions are recorded at the nodes, a root user can add iptables rules to prevent sessions logs from reaching the Auth Service. With sessions recorded at the proxy, users with root privileges on nodes have no way of disabling the audit. See the admin guide to learn how to turn on the recording proxy mode. Note that the recording mode is configured on the Auth Service.","title":"Recording Proxy Mode"},{"location":"architecture/proxy/#more-concepts","text":"Architecture Overview Teleport Users Teleport Auth Teleport Proxy","title":"More Concepts"},{"location":"architecture/users/","text":"Teleport Users Types of Users Unlike traditional SSH, Teleport introduces the concept of a User Account. A User Account is not the same as SSH login. Instead each Teleport User is associated with another account which is used to authenticate the user. For community edition users, these will be OS users which are administered outside of Teleport on each cluster node. For example, there can be a Teleport user \"joe\" who can be given permission to login as \"root\" to a specific subset of nodes. Another user \"juliet\" could be given permission to OS users \"root\" and to \"nginx\". Teleport does not have knowledge of the OS Users so it expects both \"root\" and \"nginx\" to exist on the node. For enterprise edition users, these can be stored in an external identity sources such as OKTA, Active Directory, OneLogin, G Suite, or OIDC. Read the Enterprise Guide to learn more. Teleport supports two types of user accounts: Local Users and External Users . Local users Local users are created and stored in Teleport's own identity storage in the Auth Server. Let's look at this table: Teleport User Allowed OS Logins Description joe joe, root Teleport user 'joe' can login into member nodes as OS user 'joe' or 'root' juliet juliet Teleport user 'juliet' can login into member nodes only as OS user 'juliet' ross If no OS login is specified, it defaults to the same name as the Teleport user, here this is \"ross\". To add a new user to Teleport, you have to use the tctl tool on the same node where the auth server is running, i.e. teleport was started with --roles=auth . A cluster administrator must create account entries for every Teleport user with tctl users add . Every Teleport User must be associated with a list of one or more machine-level OS usernames it can authenticate as during a login. This list is called \"user mappings\". The diagram shows the following mappings. A couple of noteworthy things from this example: Teleport User sandra does not have access to grav-02 through Teleport because sandra is not an OS username. Teleport User joe has access to all nodes because the OS user root is present on all nodes. Teleport User logins has access to nodes joe root, joe grav-00, grav-01, grav-02 tara tara grav-01, grav-02 teleport teleport grav-00, grav-02 sandra ops grav-00, grav-01 Teleport supports second factor authentication (2FA) when using a local auth connector and it is enforced by default. 2FA Support 2FA is not supported with SSO providers such as Github or OKTA. To learn more about SSO configuration check out the SSO section of the Enterprise Guide There are two types of 2FA supported: TOTP - e.g. Google Authenticator U2F - e.g. YubiKey TOTP is the default. You can use Google Authenticator or Authy or any other TOTP client. External users External users are users stored elsewhere within an organization. Examples include Github, Active Directory (AD), OIDC, or any identity store with an OpenID/OAuth2 or SAML endpoint. Version Warning External user storage is only supported in Teleport Enterprise. Please take a look at the Teleport Enterprise chapter for more information. Multiple Identity Sources It is possible to have multiple identity sources configured for a Teleport cluster. In this case, an identity source (called a \"connector\") will have to be passed to tsh --auth=connector_name login . The local users connector can be specified via tsh --auth=local login . User Roles Unlike traditional SSH, each Teleport user account is assigned a role . Having roles allows Teleport to implement role-based access control (RBAC), i.e. assign users to groups (roles) and restrict each role to a subset of actions on a subset of nodes in a cluster. More Concepts Architecture Overview Teleport Auth Teleport Nodes Teleport Proxy","title":"Teleport Users"},{"location":"architecture/users/#teleport-users","text":"","title":"Teleport Users"},{"location":"architecture/users/#types-of-users","text":"Unlike traditional SSH, Teleport introduces the concept of a User Account. A User Account is not the same as SSH login. Instead each Teleport User is associated with another account which is used to authenticate the user. For community edition users, these will be OS users which are administered outside of Teleport on each cluster node. For example, there can be a Teleport user \"joe\" who can be given permission to login as \"root\" to a specific subset of nodes. Another user \"juliet\" could be given permission to OS users \"root\" and to \"nginx\". Teleport does not have knowledge of the OS Users so it expects both \"root\" and \"nginx\" to exist on the node. For enterprise edition users, these can be stored in an external identity sources such as OKTA, Active Directory, OneLogin, G Suite, or OIDC. Read the Enterprise Guide to learn more. Teleport supports two types of user accounts: Local Users and External Users .","title":"Types of Users"},{"location":"architecture/users/#local-users","text":"Local users are created and stored in Teleport's own identity storage in the Auth Server. Let's look at this table: Teleport User Allowed OS Logins Description joe joe, root Teleport user 'joe' can login into member nodes as OS user 'joe' or 'root' juliet juliet Teleport user 'juliet' can login into member nodes only as OS user 'juliet' ross If no OS login is specified, it defaults to the same name as the Teleport user, here this is \"ross\". To add a new user to Teleport, you have to use the tctl tool on the same node where the auth server is running, i.e. teleport was started with --roles=auth . A cluster administrator must create account entries for every Teleport user with tctl users add . Every Teleport User must be associated with a list of one or more machine-level OS usernames it can authenticate as during a login. This list is called \"user mappings\". The diagram shows the following mappings. A couple of noteworthy things from this example: Teleport User sandra does not have access to grav-02 through Teleport because sandra is not an OS username. Teleport User joe has access to all nodes because the OS user root is present on all nodes. Teleport User logins has access to nodes joe root, joe grav-00, grav-01, grav-02 tara tara grav-01, grav-02 teleport teleport grav-00, grav-02 sandra ops grav-00, grav-01 Teleport supports second factor authentication (2FA) when using a local auth connector and it is enforced by default. 2FA Support 2FA is not supported with SSO providers such as Github or OKTA. To learn more about SSO configuration check out the SSO section of the Enterprise Guide There are two types of 2FA supported: TOTP - e.g. Google Authenticator U2F - e.g. YubiKey TOTP is the default. You can use Google Authenticator or Authy or any other TOTP client.","title":"Local users"},{"location":"architecture/users/#external-users","text":"External users are users stored elsewhere within an organization. Examples include Github, Active Directory (AD), OIDC, or any identity store with an OpenID/OAuth2 or SAML endpoint. Version Warning External user storage is only supported in Teleport Enterprise. Please take a look at the Teleport Enterprise chapter for more information.","title":"External users"},{"location":"architecture/users/#user-roles","text":"Unlike traditional SSH, each Teleport user account is assigned a role . Having roles allows Teleport to implement role-based access control (RBAC), i.e. assign users to groups (roles) and restrict each role to a subset of actions on a subset of nodes in a cluster.","title":"User Roles"},{"location":"architecture/users/#more-concepts","text":"Architecture Overview Teleport Auth Teleport Nodes Teleport Proxy","title":"More Concepts"},{"location":"enterprise/introduction/","text":"Teleport Enterprise This section will give an overview of Teleport Enterprise, the commercial product built around the open source Teleport Community core. For those that want to jump right in, you can play with the Quick Start Guide for Teleport Enterprise . The table below gives a quick overview of the benefits of Teleport Enterprise. Teleport Enterprise Feature Description Role Based Access Control (RBAC) Allows Teleport administrators to define User Roles and restrict each role to specific actions. RBAC also allows administrators to partition cluster nodes into groups with different access permissions. Single Sign-On (SSO) Allows Teleport to integrate with existing enterprise identity systems. Examples include Active Directory, Github, Google Apps and numerous identity middleware solutions like Auth0, Okta, and so on. Teleport supports SAML and OAuth/OpenID Connect protocols to interact with them. Approval Plugins Plugins to approve or deny escalated RBAC requests. FedRAMP/FIPS With Teleport 4.0, we have built out the foundation to help Teleport Enterprise customers build and meet the requirements in a FedRAMP System Security Plan (SSP). This includes a FIPS 140-2 friendly build of Teleport Enterprise as well as a variety of improvements to aid in complying with security controls even in FedRAMP High environments. Commercial Support In addition to these features, Teleport Enterprise also comes with a premium support SLA with guaranteed response times. Contact Information If you are interested in Teleport Enterprise, please reach out to sales@gravitational.com for more information. RBAC Role Based Access Control (\"RBAC\") allows Teleport administrators to grant granular access permissions to users. An example of an RBAC policy might be: \"admins can do anything, developers must never touch production servers, and interns can only SSH into staging servers as guests\". How does it work? Every user in Teleport is always assigned a set of roles. The open source edition of Teleport automatically assigns every user to the built-in \"admin\" role, but Teleport Enterprise allows administrators to define their own roles with far greater control over user permissions. Let's assume a company is using Active Directory to authenticate users and place them into groups. A typical enterprise deployment of Teleport in this scenario would look like this: Teleport will be configured to use existing user identities stored in Active Directory. Active Directory would have users placed in certain groups or claims, perhaps \"interns\", \"developers\", \"admins\", \"contractors\", etc. The Teleport administrator will have to define Teleport Roles. For example: \"users\", \"developers\" and \"admins\". The last step will be to define mappings from the Active Directory groups (claims) to the Teleport Roles so every Teleport user will be assigned a role based on the group membership. See RBAC for SSH chapter to learn more about configuring RBAC with Teleport. SSO The commercial edition of Teleport allows users to retrieve their SSH credentials via a single sign-on (SSO) system used by the rest of the organization. Examples of supported SSO systems include commercial solutions like Okta , Auth0 , SailPoint , OneLogin or Active Directory , as well as open source products like Keycloak . Other identity management systems are supported as long as they provide an SSO mechanism based on either SAML or OAuth2/OpenID Connect . How does SSO work with SSH? From the user's perspective they need to execute the following command to retrieve their SSH certificate. $ tsh login Teleport can be configured with a certificate TTL to determine how often a user needs to log in. tsh login will print a URL into the console, which will open an SSO login prompt, along with the 2FA, as enforced by the SSO provider. If a user supplies valid credentials, Teleport will issue an SSH certificate. Moreover, SSO can be used in combination with role-based access control (RBAC) to enforce SSH access policies like \"developers must not touch production data\" . See the SSO for SSH chapter for more details. Contact Information For more information about Teleport Enterprise or Gravity please reach out us to sales@gravitational.com or fill out the contact form on our website . FedRAMP/FIPS With Teleport 4.0 we have built the foundation to meet FedRAMP requirements for the purposes of accessing infrastructure. This includes support for FIPS 140-2 , also known as the Federal Information Processing Standard, which is the US government approved standard for cryptographic modules. Enterprise customers can download the custom FIPS package from the Gravitational Dashboard . Look for Linux 64-bit (FedRAMP/FIPS) . Using teleport start --fips Teleport will start in FIPS mode, Teleport will configure the TLS and SSH servers with FIPS compliant cryptographic algorithms. In FIPS mode, if non-compliant algorithms are chosen, Teleport will fail to start. In addition, Teleport checks if the binary was compiled against an approved cryptographic module (BoringCrypto) and fails to start if it was not. See our FedRAMP for SSH and Kubernetes guide for more infromation. Approval Workflows With Teleport 4.2 we've introduced the ability for users to request additional roles. The workflow API makes it easy to dynamically approve or deny these requests. See Approval Workflows Guide for more information","title":"Introduction"},{"location":"enterprise/introduction/#teleport-enterprise","text":"This section will give an overview of Teleport Enterprise, the commercial product built around the open source Teleport Community core. For those that want to jump right in, you can play with the Quick Start Guide for Teleport Enterprise . The table below gives a quick overview of the benefits of Teleport Enterprise. Teleport Enterprise Feature Description Role Based Access Control (RBAC) Allows Teleport administrators to define User Roles and restrict each role to specific actions. RBAC also allows administrators to partition cluster nodes into groups with different access permissions. Single Sign-On (SSO) Allows Teleport to integrate with existing enterprise identity systems. Examples include Active Directory, Github, Google Apps and numerous identity middleware solutions like Auth0, Okta, and so on. Teleport supports SAML and OAuth/OpenID Connect protocols to interact with them. Approval Plugins Plugins to approve or deny escalated RBAC requests. FedRAMP/FIPS With Teleport 4.0, we have built out the foundation to help Teleport Enterprise customers build and meet the requirements in a FedRAMP System Security Plan (SSP). This includes a FIPS 140-2 friendly build of Teleport Enterprise as well as a variety of improvements to aid in complying with security controls even in FedRAMP High environments. Commercial Support In addition to these features, Teleport Enterprise also comes with a premium support SLA with guaranteed response times. Contact Information If you are interested in Teleport Enterprise, please reach out to sales@gravitational.com for more information.","title":"Teleport Enterprise"},{"location":"enterprise/introduction/#rbac","text":"Role Based Access Control (\"RBAC\") allows Teleport administrators to grant granular access permissions to users. An example of an RBAC policy might be: \"admins can do anything, developers must never touch production servers, and interns can only SSH into staging servers as guests\".","title":"RBAC"},{"location":"enterprise/introduction/#how-does-it-work","text":"Every user in Teleport is always assigned a set of roles. The open source edition of Teleport automatically assigns every user to the built-in \"admin\" role, but Teleport Enterprise allows administrators to define their own roles with far greater control over user permissions. Let's assume a company is using Active Directory to authenticate users and place them into groups. A typical enterprise deployment of Teleport in this scenario would look like this: Teleport will be configured to use existing user identities stored in Active Directory. Active Directory would have users placed in certain groups or claims, perhaps \"interns\", \"developers\", \"admins\", \"contractors\", etc. The Teleport administrator will have to define Teleport Roles. For example: \"users\", \"developers\" and \"admins\". The last step will be to define mappings from the Active Directory groups (claims) to the Teleport Roles so every Teleport user will be assigned a role based on the group membership. See RBAC for SSH chapter to learn more about configuring RBAC with Teleport.","title":"How does it work?"},{"location":"enterprise/introduction/#sso","text":"The commercial edition of Teleport allows users to retrieve their SSH credentials via a single sign-on (SSO) system used by the rest of the organization. Examples of supported SSO systems include commercial solutions like Okta , Auth0 , SailPoint , OneLogin or Active Directory , as well as open source products like Keycloak . Other identity management systems are supported as long as they provide an SSO mechanism based on either SAML or OAuth2/OpenID Connect .","title":"SSO"},{"location":"enterprise/introduction/#how-does-sso-work-with-ssh","text":"From the user's perspective they need to execute the following command to retrieve their SSH certificate. $ tsh login Teleport can be configured with a certificate TTL to determine how often a user needs to log in. tsh login will print a URL into the console, which will open an SSO login prompt, along with the 2FA, as enforced by the SSO provider. If a user supplies valid credentials, Teleport will issue an SSH certificate. Moreover, SSO can be used in combination with role-based access control (RBAC) to enforce SSH access policies like \"developers must not touch production data\" . See the SSO for SSH chapter for more details. Contact Information For more information about Teleport Enterprise or Gravity please reach out us to sales@gravitational.com or fill out the contact form on our website .","title":"How does SSO work with SSH?"},{"location":"enterprise/introduction/#fedrampfips","text":"With Teleport 4.0 we have built the foundation to meet FedRAMP requirements for the purposes of accessing infrastructure. This includes support for FIPS 140-2 , also known as the Federal Information Processing Standard, which is the US government approved standard for cryptographic modules. Enterprise customers can download the custom FIPS package from the Gravitational Dashboard . Look for Linux 64-bit (FedRAMP/FIPS) . Using teleport start --fips Teleport will start in FIPS mode, Teleport will configure the TLS and SSH servers with FIPS compliant cryptographic algorithms. In FIPS mode, if non-compliant algorithms are chosen, Teleport will fail to start. In addition, Teleport checks if the binary was compiled against an approved cryptographic module (BoringCrypto) and fails to start if it was not. See our FedRAMP for SSH and Kubernetes guide for more infromation.","title":"FedRAMP/FIPS"},{"location":"enterprise/introduction/#approval-workflows","text":"With Teleport 4.2 we've introduced the ability for users to request additional roles. The workflow API makes it easy to dynamically approve or deny these requests. See Approval Workflows Guide for more information","title":"Approval Workflows"},{"location":"enterprise/quickstart-enterprise/","text":"Teleport Enterprise Quick Start Welcome to the Quick Start Guide for Teleport Enterprise. The goal of this document is to show off the basic capabilities of Teleport. There are three types of services Teleport nodes can run: nodes , proxies and auth servers . Auth servers store user accounts and provide authentication and authorization services for every node and every user in a cluster. Proxy servers route client connection requests to the appropriate node and serve a Web UI which can also be used to log into SSH nodes. Every client-to-node connection in Teleport must be routed via a proxy. Nodes are regular SSH servers, similar to the sshd daemon you may be familiar with. When a node receives a connection request, the request is authenticated through the cluster's auth server. The teleport daemon runs all three of these services by default. This Quick Start Guide will be using this default behavior to create a cluster and interact with it using Teleport's client-side tools: Tool Description tctl Cluster administration tool used to invite nodes to a cluster and manage user accounts. tsh Similar in principle to OpenSSH's ssh . Used to login into remote SSH nodes, list and search for nodes in a cluster, securely upload/download files, etc. browser You can use your web browser to login into any Teleport node by opening https://<proxy-host>:3080 . Prerequisites You will need to have access to the customer portal to download the software. You will also need three computers: two servers and one client (probably a laptop) to complete this tutorial. Let's assume the servers have the following DNS names and IPs: Server Name IP Address Purpose \"auth.example.com\" 10.1.1.10 This server will be used to run all three Teleport services: auth, proxy and node. \"node.example.com\" 10.1.1.11 This server will only run the SSH service. The vast majority of servers in production will be nodes. This Quick Start Guide assumes that both servers are running a systemd-based Linux distribution such as Debian, Ubuntu or a RHEL derivative. Optional: Quickstart using Docker The instructions below describe how to install Teleport Enterprise directly onto your test system. You can also run Teleport Enterprise using Docker if you don't want to install Teleport Enterprise binaries straight away. Installing To start using Teleport Enterprise, you will need to Download the binaries and the license file from the customer portal . After downloading the binary tarball, run: $ tar -xzf teleport-ent-v{{ teleport.version }}-linux-amd64-bin.tar.gz $ cd teleport-ent Copy teleport and tctl binaries to a bin directory (we suggest /usr/local/bin ) on the auth server. Copy teleport binary to a bin directory on the node server. Copy tsh binary to a bin directory on the client computer. License File The Teleport license file contains a X.509 certificate and the corresponding private key in PEM format . Download the license file from the customer portal and save it as /var/lib/teleport/license.pem on the auth server. Configuration File Save the following configuration file as /etc/teleport.yaml on the node.example.com : teleport : auth_token : dogs-are-much-nicer-than-cats # you can also use auth server's IP, i.e. \"10.1.1.10:3025\" auth_servers : [ \"auth.example.com:3025\" ] # enable ssh service and disable auth and proxy: ssh_service : enabled : true auth_service : enabled : false proxy_service : enabled : false Now, save the following configuration file as /etc/teleport.yaml on the auth.example.com : teleport : auth_token : dogs-are-much-nicer-than-cats auth_servers : [ \"localhost:3025\" ] auth_service : # enable the auth service: enabled : true tokens : # this static token is used for other nodes to join this Teleport cluster - proxy,node:dogs-are-much-nicer-than-cats # this token is used to establish trust with other Teleport clusters - trusted_cluster:trains-are-superior-to-cars # by default, local authentication will be used with 2FA authentication : second_factor : otp # SSH is also enabled on this node: ssh_service : enabled : \"yes\" Systemd Unit File Next, download the systemd service unit file from examples directory on Github and save it as /etc/systemd/system/teleport.service on both servers. # run this on both servers: $ sudo systemctl daemon-reload $ sudo systemctl enable teleport Starting # run this on both servers: $ sudo systemctl start teleport Teleport daemon should start and you can use netstat -lptne to make sure that it's listening on TCP/IP ports . On auth.example.com , it should look something like this: $ auth.example.com ~: sudo netstat -lptne Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address State User PID/Program name tcp6 0 0 :::3024 LISTEN 0 337/teleport tcp6 0 0 :::3025 LISTEN 0 337/teleport tcp6 0 0 :::3080 LISTEN 0 337/teleport tcp6 0 0 :::3022 LISTEN 0 337/teleport tcp6 0 0 :::3023 LISTEN 0 337/teleport and node.example.com should look something like this: $ node.example.com ~: sudo netstat -lptne Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address State User PID/Program name tcp6 0 0 :::3022 LISTEN 0 337/teleport See troubleshooting section at the bottom if something is not working. Adding Users This portion of the Quick Start Guide should be performed on the auth server, i.e. on auth.example.com Every user in a Teleport cluster must be assigned at least one role. By default, Teleport comes with one pre-configured role called \"admin\". You can see it's definition by executing sudo tctl get roles/admin > admin-role.yaml . The output will look like this (re-formatted here to use compact YAML representation for brevity): kind : role version : v3 metadata : name : admin spec : options : cert_format : standard forward_agent : true max_session_ttl : 30h0m0s port_forwarding : true # allow rules: allow : logins : - '{% raw %}{{internal.logins}}{% endraw %}' - root node_labels : '*' : '*' rules : - resources : [ role ] verbs : [ list , create , read , update , delete ] - resources : [ auth_connector ] verbs : [ list , create , read , update , delete ] - resources : [ session ] verbs : [ list , read ] - resources : [ trusted_cluster ] verbs : [ list , create , read , update , delete ] # no deny rules are present, the admin role must have access to everything) deny : {} Pay attention to the allow/logins field in the role definition: by default, this role only allows SSH logins as root@host . Note Ignore {% raw %}{{internal.logins}}{% endraw %} \"allowed login\" for now. It exists for compatibility purposes when upgrading existing open source Teleport clusters. You probably want to replace \"root\" with something else. Let's assume there will be a local UNIX account called \"admin\" on all hosts. In this case you can dump the role definition YAML into admin-role.yaml file and update \"allow/logins\" to look like this: allow : logins : [ admin ] Then send it back into Teleport: $ sudo tctl create -f admin-role.yaml Now, lets create a new Teleport user \"joe\" with \"admin\" role: $ sudo tctl users add --roles=admin joe Signup token has been created and is valid for 1 hours. Share this URL with the user: https://auth.example.com:3080/web/newuser/22e3acb6a0c2cde22f13bdc879ff9d2a Share the generated sign-up URL with Joe and let him pick a password and configure the second factor authentication. We recommend Google Authenticator which is available for both Android and iPhone. Assigning Roles To update user's roles, dump the user resource into a file: $ sudo tctl get users/joe > joe.yaml Edit the YAML file and update the \"roles\" array. Then, re-insert it back: $ sudo tctl create -f joe.yaml Logging In Joe now has a local account on a Teleport cluster. The local account is good for administrative purposes but regular users of Teleport Enterprise should be using a Single Sign-On (SSO) mechanism. But first, lets see how Joe can log into the Teleport cluster. He can do this on his client laptop: $ tsh --proxy=auth.example.com --insecure login --user=joe Note that \"--user=joe\" part can be omitted if $USER environment variable is \"joe\". Notice that tsh client always needs --proxy flag because all client connections in Teleport always must to go through an SSH proxy, sometimes called an \"SSH bastion\". Warning For the purposes of this quickstart we are using the --insecure flag which allows us to skip configuring the HTTP/TLS certificate for Teleport proxy. Your browser will throw a warning Your connection is not private . Click Advanced, and Proceed to 0.0.0.0 (unsafe) to preview the Teleport UI. Never use --insecure in production unless you terminate SSL at a load balancer. This will apply to most cloud providers (AWS, GCP and Azure). You must configure a HTTP/TLS certificate for the Proxy. This process has been made easier with Let's Encrypt. We've instructions here . If successful, tsh login command will receive Joe's user certificate and will store it in ~/.tsh/keys/<proxy> directory. With a certificate in place, Joe can now interact with the Teleport cluster: # SSH into any host behind the proxy: $ tsh ssh node.example.com # See what hosts are available behind the proxy: $ tsh ls # Log out (this will remove the user certificate from ~/.tsh) $ tsh logout Configuring SSO The local account is good for administrative purposes but regular users of Teleport Enterprise should be using a Single Sign-On (SSO) mechanism that use SAML or OIDC protocols. Take a look at the SSH via Single Sign-on chapter to learn the basics of integrating Teleport with SSO providers. We have the following detailed guides for configuring SSO providers: Okta Active Directory One Login Github Any SAML-compliant provider can be configured with Teleport by following the same steps. There are Teleport Enterprise customers who are using Oracle IDM, SailPoint and others. Run Teleport Enterprise using Docker We provide pre-built Docker images for every version of Teleport Enterprise. These images are hosted on quay.io. All tags under quay.io/gravitational/teleport-ent are Teleport Enterprise images We currently only offer Docker images for x86_64 architectures. Note You will need a recent version of Docker installed to follow this section of the quick start guide. Warning This setup will not let you 'SSH into' the node that is running Teleport without additional configuration. Pick your image This table gives an idea of how our image naming scheme works. We offer images which point to a static version of Teleport Enterprise, as well as images which are automatically rebuilt every night. These nightly images point to the latest version of Teleport Enterprise from the three most recent release branches. They are stable, and we recommend their use to easily keep your Teleport Enterprise installation up to date. Image name Community or Enterprise? Teleport version Image automatically updated? Image base quay.io/gravitational/teleport-ent:{{ version }} Enterprise The latest version of Teleport Enterprise {{ version }} Yes Ubuntu 20.04 quay.io/gravitational/teleport-ent:{{ version }}-fips Enterprise FIPS The latest version of Teleport Enterprise {{version }} FIPS Yes Ubuntu 20.04 quay.io/gravitational/teleport-ent:{{ teleport.version }} Enterprise The version specified in the image's tag (i.e. {{ teleport.version }}) No Ubuntu 20.04 quay.io/gravitational/teleport-ent:{{ teleport.version }}-fips Enterprise FIPS The version specified in the image's tag (i.e. {{ teleport.version }}) No Ubuntu 20.04 For testing, we always recommend that you use the latest release version of Teleport Enterprise, which is currently {{teleport.latest_ent_docker_image}} . Quickstart using docker-compose Note You will need a recent version of docker-compose installed to follow this section of the quick start guide. The easiest way to start Teleport Enterprise quickly is to use docker-compose with our teleport-ent-quickstart.yml file: # download the quickstart file from our Github repo curl -Lso teleport-ent-quickstart.yml https://raw.githubusercontent.com/gravitational/teleport/master/docker/teleport-ent-quickstart.yml # start teleport quickstart using docker-compose docker-compose -f teleport-ent-quickstart.yml up The docker-compose quickstart will automatically create a config file for you at ./docker/teleport/config/teleport.yaml This config is mounted into the container under /etc/teleport/teleport.yaml It will also start teleport using this config file, with Teleport's data directory set to ./docker/teleport/data and mounted under /var/lib/teleport It will mount your license file (named license.pem ) from the current directory into the Docker container By default, docker-compose will output Teleport's logs to the console for you to observe. If you would rather run the Teleport container in the background, use docker-compose -f teleport-ent-quickstart.yml up -d You can stop the Teleport container using docker-compose -f teleport-ent-quickstart.yml down Quickstart using docker run If you'd prefer to complete these steps manually, here's some sample docker run commands: # create local config and data directories for teleport, which will be mounted into the container mkdir -p ~/teleport/config ~/teleport/data # download your license file from the Gravitational dashboard and put it in the correct directory # the file needs to be named license.pem cp ~/downloads/downloaded-license.pem ~/teleport/data/license.pem # generate a sample teleport config and write it to the local config directory # this container will write the config and immediately exit - this is expected docker run --hostname localhost --rm \\ --entrypoint = /bin/sh \\ -v ~/teleport/config:/etc/teleport \\ {{ teleport.latest_ent_docker_image }} -c \"teleport configure > /etc/teleport/teleport.yaml\" # change the path to the license file in the sample config sed -i 's_/path/to/license-if-using-teleport-enterprise.pem_/var/lib/teleport/license.pem_g' ~/teleport/config/teleport.yaml # start teleport with mounted license, config and data directories, plus all ports docker run --hostname localhost --name teleport \\ -v ~/teleport/config:/etc/teleport \\ -v ~/teleport/data:/var/lib/teleport \\ -p 3023 :3023 -p 3025 :3025 -p 3080 :3080 \\ {{ teleport.latest_ent_docker_image }} Creating a Teleport user when using Docker quickstart To create a user inside your Teleport Enterprise container, use docker exec . This example command will create a Teleport user called testuser which has the admin role. Feel free to change these to suit your needs. docker exec teleport tctl users add testuser --roles = admin When you run this command, Teleport will output a URL which you must open to complete the user signup process: User testuser has been created but requires a password. Share this URL with the user to complete user setup, link is valid for 1h0m0s: https://localhost:3080/web/invite/4f2718a52ce107568b191f222ba069f7 NOTE: Make sure localhost:3080 points at a Teleport proxy which users can access. You can now follow this guide from \"Adding Users\" onwards to create your user and log into Teleport Enterprise. Troubleshooting If Teleport services do not start, take a look at the syslog: $ sudo journalctl -fu teleport Usually the error will be reported there. Common reasons for failure are: Mismatched tokens, i.e. \"auth_token\" on the node does not match \"tokens/node\" value on the auth server. Network issues: port 3025 is closed via iptables. Network issues: ports 3025 or 3022 are occupied by another process. Disk issues: Teleport fails to create /var/lib/teleport because the volume is read-only or not accessible. Getting Help If something is not working, please reach out to us by creating a ticket in your customer portal . Customers who have purchased the premium support package can also ping us through your Slack channel.","title":"Quick Start Guide"},{"location":"enterprise/quickstart-enterprise/#teleport-enterprise-quick-start","text":"Welcome to the Quick Start Guide for Teleport Enterprise. The goal of this document is to show off the basic capabilities of Teleport. There are three types of services Teleport nodes can run: nodes , proxies and auth servers . Auth servers store user accounts and provide authentication and authorization services for every node and every user in a cluster. Proxy servers route client connection requests to the appropriate node and serve a Web UI which can also be used to log into SSH nodes. Every client-to-node connection in Teleport must be routed via a proxy. Nodes are regular SSH servers, similar to the sshd daemon you may be familiar with. When a node receives a connection request, the request is authenticated through the cluster's auth server. The teleport daemon runs all three of these services by default. This Quick Start Guide will be using this default behavior to create a cluster and interact with it using Teleport's client-side tools: Tool Description tctl Cluster administration tool used to invite nodes to a cluster and manage user accounts. tsh Similar in principle to OpenSSH's ssh . Used to login into remote SSH nodes, list and search for nodes in a cluster, securely upload/download files, etc. browser You can use your web browser to login into any Teleport node by opening https://<proxy-host>:3080 .","title":"Teleport Enterprise Quick Start"},{"location":"enterprise/quickstart-enterprise/#prerequisites","text":"You will need to have access to the customer portal to download the software. You will also need three computers: two servers and one client (probably a laptop) to complete this tutorial. Let's assume the servers have the following DNS names and IPs: Server Name IP Address Purpose \"auth.example.com\" 10.1.1.10 This server will be used to run all three Teleport services: auth, proxy and node. \"node.example.com\" 10.1.1.11 This server will only run the SSH service. The vast majority of servers in production will be nodes. This Quick Start Guide assumes that both servers are running a systemd-based Linux distribution such as Debian, Ubuntu or a RHEL derivative.","title":"Prerequisites"},{"location":"enterprise/quickstart-enterprise/#optional-quickstart-using-docker","text":"The instructions below describe how to install Teleport Enterprise directly onto your test system. You can also run Teleport Enterprise using Docker if you don't want to install Teleport Enterprise binaries straight away.","title":"Optional: Quickstart using Docker"},{"location":"enterprise/quickstart-enterprise/#installing","text":"To start using Teleport Enterprise, you will need to Download the binaries and the license file from the customer portal . After downloading the binary tarball, run: $ tar -xzf teleport-ent-v{{ teleport.version }}-linux-amd64-bin.tar.gz $ cd teleport-ent Copy teleport and tctl binaries to a bin directory (we suggest /usr/local/bin ) on the auth server. Copy teleport binary to a bin directory on the node server. Copy tsh binary to a bin directory on the client computer.","title":"Installing"},{"location":"enterprise/quickstart-enterprise/#license-file","text":"The Teleport license file contains a X.509 certificate and the corresponding private key in PEM format . Download the license file from the customer portal and save it as /var/lib/teleport/license.pem on the auth server.","title":"License File"},{"location":"enterprise/quickstart-enterprise/#configuration-file","text":"Save the following configuration file as /etc/teleport.yaml on the node.example.com : teleport : auth_token : dogs-are-much-nicer-than-cats # you can also use auth server's IP, i.e. \"10.1.1.10:3025\" auth_servers : [ \"auth.example.com:3025\" ] # enable ssh service and disable auth and proxy: ssh_service : enabled : true auth_service : enabled : false proxy_service : enabled : false Now, save the following configuration file as /etc/teleport.yaml on the auth.example.com : teleport : auth_token : dogs-are-much-nicer-than-cats auth_servers : [ \"localhost:3025\" ] auth_service : # enable the auth service: enabled : true tokens : # this static token is used for other nodes to join this Teleport cluster - proxy,node:dogs-are-much-nicer-than-cats # this token is used to establish trust with other Teleport clusters - trusted_cluster:trains-are-superior-to-cars # by default, local authentication will be used with 2FA authentication : second_factor : otp # SSH is also enabled on this node: ssh_service : enabled : \"yes\"","title":"Configuration File"},{"location":"enterprise/quickstart-enterprise/#systemd-unit-file","text":"Next, download the systemd service unit file from examples directory on Github and save it as /etc/systemd/system/teleport.service on both servers. # run this on both servers: $ sudo systemctl daemon-reload $ sudo systemctl enable teleport","title":"Systemd Unit File"},{"location":"enterprise/quickstart-enterprise/#starting","text":"# run this on both servers: $ sudo systemctl start teleport Teleport daemon should start and you can use netstat -lptne to make sure that it's listening on TCP/IP ports . On auth.example.com , it should look something like this: $ auth.example.com ~: sudo netstat -lptne Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address State User PID/Program name tcp6 0 0 :::3024 LISTEN 0 337/teleport tcp6 0 0 :::3025 LISTEN 0 337/teleport tcp6 0 0 :::3080 LISTEN 0 337/teleport tcp6 0 0 :::3022 LISTEN 0 337/teleport tcp6 0 0 :::3023 LISTEN 0 337/teleport and node.example.com should look something like this: $ node.example.com ~: sudo netstat -lptne Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address State User PID/Program name tcp6 0 0 :::3022 LISTEN 0 337/teleport See troubleshooting section at the bottom if something is not working.","title":"Starting"},{"location":"enterprise/quickstart-enterprise/#adding-users","text":"This portion of the Quick Start Guide should be performed on the auth server, i.e. on auth.example.com Every user in a Teleport cluster must be assigned at least one role. By default, Teleport comes with one pre-configured role called \"admin\". You can see it's definition by executing sudo tctl get roles/admin > admin-role.yaml . The output will look like this (re-formatted here to use compact YAML representation for brevity): kind : role version : v3 metadata : name : admin spec : options : cert_format : standard forward_agent : true max_session_ttl : 30h0m0s port_forwarding : true # allow rules: allow : logins : - '{% raw %}{{internal.logins}}{% endraw %}' - root node_labels : '*' : '*' rules : - resources : [ role ] verbs : [ list , create , read , update , delete ] - resources : [ auth_connector ] verbs : [ list , create , read , update , delete ] - resources : [ session ] verbs : [ list , read ] - resources : [ trusted_cluster ] verbs : [ list , create , read , update , delete ] # no deny rules are present, the admin role must have access to everything) deny : {} Pay attention to the allow/logins field in the role definition: by default, this role only allows SSH logins as root@host . Note Ignore {% raw %}{{internal.logins}}{% endraw %} \"allowed login\" for now. It exists for compatibility purposes when upgrading existing open source Teleport clusters. You probably want to replace \"root\" with something else. Let's assume there will be a local UNIX account called \"admin\" on all hosts. In this case you can dump the role definition YAML into admin-role.yaml file and update \"allow/logins\" to look like this: allow : logins : [ admin ] Then send it back into Teleport: $ sudo tctl create -f admin-role.yaml Now, lets create a new Teleport user \"joe\" with \"admin\" role: $ sudo tctl users add --roles=admin joe Signup token has been created and is valid for 1 hours. Share this URL with the user: https://auth.example.com:3080/web/newuser/22e3acb6a0c2cde22f13bdc879ff9d2a Share the generated sign-up URL with Joe and let him pick a password and configure the second factor authentication. We recommend Google Authenticator which is available for both Android and iPhone.","title":"Adding Users"},{"location":"enterprise/quickstart-enterprise/#assigning-roles","text":"To update user's roles, dump the user resource into a file: $ sudo tctl get users/joe > joe.yaml Edit the YAML file and update the \"roles\" array. Then, re-insert it back: $ sudo tctl create -f joe.yaml","title":"Assigning Roles"},{"location":"enterprise/quickstart-enterprise/#logging-in","text":"Joe now has a local account on a Teleport cluster. The local account is good for administrative purposes but regular users of Teleport Enterprise should be using a Single Sign-On (SSO) mechanism. But first, lets see how Joe can log into the Teleport cluster. He can do this on his client laptop: $ tsh --proxy=auth.example.com --insecure login --user=joe Note that \"--user=joe\" part can be omitted if $USER environment variable is \"joe\". Notice that tsh client always needs --proxy flag because all client connections in Teleport always must to go through an SSH proxy, sometimes called an \"SSH bastion\". Warning For the purposes of this quickstart we are using the --insecure flag which allows us to skip configuring the HTTP/TLS certificate for Teleport proxy. Your browser will throw a warning Your connection is not private . Click Advanced, and Proceed to 0.0.0.0 (unsafe) to preview the Teleport UI. Never use --insecure in production unless you terminate SSL at a load balancer. This will apply to most cloud providers (AWS, GCP and Azure). You must configure a HTTP/TLS certificate for the Proxy. This process has been made easier with Let's Encrypt. We've instructions here . If successful, tsh login command will receive Joe's user certificate and will store it in ~/.tsh/keys/<proxy> directory. With a certificate in place, Joe can now interact with the Teleport cluster: # SSH into any host behind the proxy: $ tsh ssh node.example.com # See what hosts are available behind the proxy: $ tsh ls # Log out (this will remove the user certificate from ~/.tsh) $ tsh logout","title":"Logging In"},{"location":"enterprise/quickstart-enterprise/#configuring-sso","text":"The local account is good for administrative purposes but regular users of Teleport Enterprise should be using a Single Sign-On (SSO) mechanism that use SAML or OIDC protocols. Take a look at the SSH via Single Sign-on chapter to learn the basics of integrating Teleport with SSO providers. We have the following detailed guides for configuring SSO providers: Okta Active Directory One Login Github Any SAML-compliant provider can be configured with Teleport by following the same steps. There are Teleport Enterprise customers who are using Oracle IDM, SailPoint and others.","title":"Configuring SSO"},{"location":"enterprise/quickstart-enterprise/#run-teleport-enterprise-using-docker","text":"We provide pre-built Docker images for every version of Teleport Enterprise. These images are hosted on quay.io. All tags under quay.io/gravitational/teleport-ent are Teleport Enterprise images We currently only offer Docker images for x86_64 architectures. Note You will need a recent version of Docker installed to follow this section of the quick start guide. Warning This setup will not let you 'SSH into' the node that is running Teleport without additional configuration.","title":"Run Teleport Enterprise using Docker"},{"location":"enterprise/quickstart-enterprise/#pick-your-image","text":"This table gives an idea of how our image naming scheme works. We offer images which point to a static version of Teleport Enterprise, as well as images which are automatically rebuilt every night. These nightly images point to the latest version of Teleport Enterprise from the three most recent release branches. They are stable, and we recommend their use to easily keep your Teleport Enterprise installation up to date. Image name Community or Enterprise? Teleport version Image automatically updated? Image base quay.io/gravitational/teleport-ent:{{ version }} Enterprise The latest version of Teleport Enterprise {{ version }} Yes Ubuntu 20.04 quay.io/gravitational/teleport-ent:{{ version }}-fips Enterprise FIPS The latest version of Teleport Enterprise {{version }} FIPS Yes Ubuntu 20.04 quay.io/gravitational/teleport-ent:{{ teleport.version }} Enterprise The version specified in the image's tag (i.e. {{ teleport.version }}) No Ubuntu 20.04 quay.io/gravitational/teleport-ent:{{ teleport.version }}-fips Enterprise FIPS The version specified in the image's tag (i.e. {{ teleport.version }}) No Ubuntu 20.04 For testing, we always recommend that you use the latest release version of Teleport Enterprise, which is currently {{teleport.latest_ent_docker_image}} .","title":"Pick your image"},{"location":"enterprise/quickstart-enterprise/#quickstart-using-docker-compose","text":"Note You will need a recent version of docker-compose installed to follow this section of the quick start guide. The easiest way to start Teleport Enterprise quickly is to use docker-compose with our teleport-ent-quickstart.yml file: # download the quickstart file from our Github repo curl -Lso teleport-ent-quickstart.yml https://raw.githubusercontent.com/gravitational/teleport/master/docker/teleport-ent-quickstart.yml # start teleport quickstart using docker-compose docker-compose -f teleport-ent-quickstart.yml up The docker-compose quickstart will automatically create a config file for you at ./docker/teleport/config/teleport.yaml This config is mounted into the container under /etc/teleport/teleport.yaml It will also start teleport using this config file, with Teleport's data directory set to ./docker/teleport/data and mounted under /var/lib/teleport It will mount your license file (named license.pem ) from the current directory into the Docker container By default, docker-compose will output Teleport's logs to the console for you to observe. If you would rather run the Teleport container in the background, use docker-compose -f teleport-ent-quickstart.yml up -d You can stop the Teleport container using docker-compose -f teleport-ent-quickstart.yml down","title":"Quickstart using docker-compose"},{"location":"enterprise/quickstart-enterprise/#quickstart-using-docker-run","text":"If you'd prefer to complete these steps manually, here's some sample docker run commands: # create local config and data directories for teleport, which will be mounted into the container mkdir -p ~/teleport/config ~/teleport/data # download your license file from the Gravitational dashboard and put it in the correct directory # the file needs to be named license.pem cp ~/downloads/downloaded-license.pem ~/teleport/data/license.pem # generate a sample teleport config and write it to the local config directory # this container will write the config and immediately exit - this is expected docker run --hostname localhost --rm \\ --entrypoint = /bin/sh \\ -v ~/teleport/config:/etc/teleport \\ {{ teleport.latest_ent_docker_image }} -c \"teleport configure > /etc/teleport/teleport.yaml\" # change the path to the license file in the sample config sed -i 's_/path/to/license-if-using-teleport-enterprise.pem_/var/lib/teleport/license.pem_g' ~/teleport/config/teleport.yaml # start teleport with mounted license, config and data directories, plus all ports docker run --hostname localhost --name teleport \\ -v ~/teleport/config:/etc/teleport \\ -v ~/teleport/data:/var/lib/teleport \\ -p 3023 :3023 -p 3025 :3025 -p 3080 :3080 \\ {{ teleport.latest_ent_docker_image }}","title":"Quickstart using docker run"},{"location":"enterprise/quickstart-enterprise/#creating-a-teleport-user-when-using-docker-quickstart","text":"To create a user inside your Teleport Enterprise container, use docker exec . This example command will create a Teleport user called testuser which has the admin role. Feel free to change these to suit your needs. docker exec teleport tctl users add testuser --roles = admin When you run this command, Teleport will output a URL which you must open to complete the user signup process: User testuser has been created but requires a password. Share this URL with the user to complete user setup, link is valid for 1h0m0s: https://localhost:3080/web/invite/4f2718a52ce107568b191f222ba069f7 NOTE: Make sure localhost:3080 points at a Teleport proxy which users can access. You can now follow this guide from \"Adding Users\" onwards to create your user and log into Teleport Enterprise.","title":"Creating a Teleport user when using Docker quickstart"},{"location":"enterprise/quickstart-enterprise/#troubleshooting","text":"If Teleport services do not start, take a look at the syslog: $ sudo journalctl -fu teleport Usually the error will be reported there. Common reasons for failure are: Mismatched tokens, i.e. \"auth_token\" on the node does not match \"tokens/node\" value on the auth server. Network issues: port 3025 is closed via iptables. Network issues: ports 3025 or 3022 are occupied by another process. Disk issues: Teleport fails to create /var/lib/teleport because the volume is read-only or not accessible.","title":"Troubleshooting"},{"location":"enterprise/quickstart-enterprise/#getting-help","text":"If something is not working, please reach out to us by creating a ticket in your customer portal . Customers who have purchased the premium support package can also ping us through your Slack channel.","title":"Getting Help"},{"location":"enterprise/ssh-kubernetes-fedramp/","text":"FedRAMP for SSH and Kubernetes Access With Teleport 4.0 we have built the foundation to meet FedRAMP requirements for the purposes of accessing infrastructure. This includes support for FIPS 140-2 , also known as the Federal Information Processing Standard, which is the US government approved standard for cryptographic modules. This document outlines a high level overview of how Teleport FIPS mode works and how it can help your company to become FedRAMP certified. Obtain FedRAMP certification with Teleport Teleport includes new FedRAMP and FIPS 140-2 features to support companies that sell into government agencies. Control Teleport Features AC-03 Access Enforcement Teleport Enterprise supports robust Role-based Access Controls (RBAC) to: \u2022 Control which SSH nodes a user can or cannot access. \u2022 Control cluster level configuration (session recording, configuration, etc.) \u2022 Control which UNIX logins a user is allowed to use when logging into a server. AC-10 Concurrent Session Control Teleport administrators can define concurrent session limits using Teleport\u2019s RBAC. AC-17 Remote Access Teleport administrators create users with configurable roles that can be used to allow or deny access to system resources. AC-20 Use of External Information Systems Teleport supports connecting multiple independent clusters using a feature called Trusted Clusters . When allowing access from one cluster to another, roles are mapped according to a pre-defined relationship of the scope of access. AU-03 Audit and Accountability \u2013 Content of Audit Records and AU-12 Audit Generation Teleport contains an Audit Log that records cluster-wide events such as: \u2022 Failed login attempts. \u2022 Commands that were executed (SSH \u201cexec\u201d commands). \u2022 Ports that were forwarded. \u2022 File transfers that were initiated. AU-10 Non-Repudiation Teleport audit logging supports both events as well as audit of an entire SSH session. For non-repudiation purposes a full session can be replayed back and viewed. CM-08 Information System Component Inventory Teleport maintains a live list of all nodes within a cluster. This node list can be queried by users (who see a subset they have access to) and administrators any time. IA-03 Device Identification and Authentication Teleport requires valid x509 or SSH certificates issued by a Teleport Certificate Authority (CA) to establish a network connection for device-to-device network connection between Teleport components. SC-12 Cryptographic Key Establish and Management Teleport initializes cryptographic keys that act as a Certificate Authority (CA) to further issue x509 and SSH certificates. SSH and x509 user certificates that are issued are signed by the CA and are (by default) short-lived. SSH host certificates are also signed by the CA and rotated automatically (a manual force rotation can also be performed). Teleport Enterprise builds against a FIPS 140-2 compliant library (BoringCrypto) is available. In addition, when Teleport Enterprise is in FedRAMP/FIPS 140-2 mode, Teleport will only start and use FIPS 140-2 compliant cryptography. AC-2 Account Management Audit events are emitted in the auth server when a user is created, updated, deleted, locked or unlocked. AC-2 (12) Account Management At the close of a connection the total data transmitted and received is emitted to the Audit Log. Enterprise customers can download the custom FIPS package from the Gravitational Dashboard . Look for Linux 64-bit (FedRAMP/FIPS) . RPM and DEB packages are also available. Setup Customers can follow our Enterprise Quickstart for basic instructions on how to setup Teleport Enterprise. You'll need to start with the Teleport Enterprise FIPS Binary. After downloading the binary tarball, run: $ tar -xzf teleport-ent-v{{ teleport.version }}-linux-amd64-fips-bin.tar.gz $ cd teleport-ent $ sudo ./install # This will copy Teleport Enterprise to /usr/local/bin. Configuration Teleport Auth Server Now, save the following configuration file as /etc/teleport.yaml on the auth server. teleport : auth_token : zw6C82kq7VEUSJeSDzuldWsxakql6jrTYmphxRQOlrATTGbLQoaIwEBo48o9 # Pre-defined tokens for adding new nodes to a cluster. Each token specifies # the role a new node will be allowed to assume. The more secure way to # add nodes is to use `ttl node add --ttl` command to generate auto-expiring # tokens. # # We recommend to use tools like `pwgen` to generate sufficiently random # tokens of 32+ byte length. # you can also use auth server's IP, i.e. \"10.1.1.10:3025\" auth_servers : [ \"10.1.1.10:3025\" ] auth_service : # enable the auth service: enabled : true tokens : # this static token is used for other nodes to join this Teleport cluster - proxy,node:zw6C82kq7VEUSJeSDzuldWsxakql6jrTYmphxRQOlrATTGbLQoaIwEBo48o9 # this token is used to establish trust with other Teleport clusters - trusted_cluster:TaZff3DLbpsMZmIMhvEr7kulOgegjg7yyQNTS0q6UFWfsJ9N6rxVBjg6t7nw # To Support FIPS local_auth needs to be turned off and a SSO connector is # required to log into Teleport. authentication : # local_auth needs to be set to false in FIPS mode. local_auth : false type : saml # If using Proxy Mode, Teleport requires host key checks. # This setting needs is required to start in Teleport in FIPS mode proxy_checks_host_keys : true # SSH is also enabled on this node: ssh_service : enabled : false Teleport Node Save the following configuration file as /etc/teleport.yaml on the node server. teleport : auth_token : zw6C82kq7VEUSJeSDzuldWsxakql6jrTYmphxRQOlrATTGbLQoaIwEBo48o9 auth_servers : [ \"10.1.1.10:3025\" ] # enable ssh service and disable auth and proxy: ssh_service : enabled : true auth_service : enabled : false proxy_service : enabled : false Systemd Unit File Next, download the systemd service unit file from the examples directory on Github and save it as /etc/systemd/system/teleport.service on both servers. # run this on both servers: $ sudo systemctl daemon-reload $ sudo systemctl enable teleport Starting Teleport in FIPS mode. When using teleport start --fips , Teleport will start in FIPS mode. Teleport will configure the TLS and SSH servers with FIPS compliant cryptographic algorithms. In FIPS mode, if non-compliant algorithms are chosen, Teleport will fail to start. In addition, Teleport checks if the binary was compiled against an approved cryptographic module (BoringCrypto) and fails to start if it was not. For OSS and Enterprise binaries not compiled with BoringCrypto, this flag will report that this version of Teleport is not compiled with the appropriate cryptographic module. Running commands like ps aux can be useful to note that Teleport is running in FedRAMP enforcing mode. If no ciphersuites are provided, Teleport will set the default ciphersuites to be FIPS 140-2 compliant. If ciphersuites, key exchange and MAC algorithms are provided in the Teleport configuration, Teleport will validate that they are FIPS 140-2 compliant.. Teleport will always enable at-rest encryption for both DynamoDB and S3. If recording proxy mode is selected, validation of host certificates should always happen. FedRAMP Audit Log At the close of a connection (close of a *srv.ServerContext) the total data transmitted and received is emitted to the Audit Log. What else does the Teleport FIPS binary enforce? Supporting configurable TLS versions. This is to ensure that only TLS 1.2 is supported in FedRAMP mode. Removes all uses of non-compliant algorithms like NaCl and replace with compliant algorithms like AES-GCM. Teleport is compiled with BoringCrypto User, host and CA certificates (and host keys for recording proxy mode) should only use 2048-bit RSA private keys.","title":"FedRAMP for SSH & K8s"},{"location":"enterprise/ssh-kubernetes-fedramp/#fedramp-for-ssh-and-kubernetes-access","text":"With Teleport 4.0 we have built the foundation to meet FedRAMP requirements for the purposes of accessing infrastructure. This includes support for FIPS 140-2 , also known as the Federal Information Processing Standard, which is the US government approved standard for cryptographic modules. This document outlines a high level overview of how Teleport FIPS mode works and how it can help your company to become FedRAMP certified.","title":"FedRAMP for SSH and Kubernetes Access"},{"location":"enterprise/ssh-kubernetes-fedramp/#obtain-fedramp-certification-with-teleport","text":"Teleport includes new FedRAMP and FIPS 140-2 features to support companies that sell into government agencies. Control Teleport Features AC-03 Access Enforcement Teleport Enterprise supports robust Role-based Access Controls (RBAC) to: \u2022 Control which SSH nodes a user can or cannot access. \u2022 Control cluster level configuration (session recording, configuration, etc.) \u2022 Control which UNIX logins a user is allowed to use when logging into a server. AC-10 Concurrent Session Control Teleport administrators can define concurrent session limits using Teleport\u2019s RBAC. AC-17 Remote Access Teleport administrators create users with configurable roles that can be used to allow or deny access to system resources. AC-20 Use of External Information Systems Teleport supports connecting multiple independent clusters using a feature called Trusted Clusters . When allowing access from one cluster to another, roles are mapped according to a pre-defined relationship of the scope of access. AU-03 Audit and Accountability \u2013 Content of Audit Records and AU-12 Audit Generation Teleport contains an Audit Log that records cluster-wide events such as: \u2022 Failed login attempts. \u2022 Commands that were executed (SSH \u201cexec\u201d commands). \u2022 Ports that were forwarded. \u2022 File transfers that were initiated. AU-10 Non-Repudiation Teleport audit logging supports both events as well as audit of an entire SSH session. For non-repudiation purposes a full session can be replayed back and viewed. CM-08 Information System Component Inventory Teleport maintains a live list of all nodes within a cluster. This node list can be queried by users (who see a subset they have access to) and administrators any time. IA-03 Device Identification and Authentication Teleport requires valid x509 or SSH certificates issued by a Teleport Certificate Authority (CA) to establish a network connection for device-to-device network connection between Teleport components. SC-12 Cryptographic Key Establish and Management Teleport initializes cryptographic keys that act as a Certificate Authority (CA) to further issue x509 and SSH certificates. SSH and x509 user certificates that are issued are signed by the CA and are (by default) short-lived. SSH host certificates are also signed by the CA and rotated automatically (a manual force rotation can also be performed). Teleport Enterprise builds against a FIPS 140-2 compliant library (BoringCrypto) is available. In addition, when Teleport Enterprise is in FedRAMP/FIPS 140-2 mode, Teleport will only start and use FIPS 140-2 compliant cryptography. AC-2 Account Management Audit events are emitted in the auth server when a user is created, updated, deleted, locked or unlocked. AC-2 (12) Account Management At the close of a connection the total data transmitted and received is emitted to the Audit Log. Enterprise customers can download the custom FIPS package from the Gravitational Dashboard . Look for Linux 64-bit (FedRAMP/FIPS) . RPM and DEB packages are also available.","title":"Obtain FedRAMP certification with Teleport"},{"location":"enterprise/ssh-kubernetes-fedramp/#setup","text":"Customers can follow our Enterprise Quickstart for basic instructions on how to setup Teleport Enterprise. You'll need to start with the Teleport Enterprise FIPS Binary. After downloading the binary tarball, run: $ tar -xzf teleport-ent-v{{ teleport.version }}-linux-amd64-fips-bin.tar.gz $ cd teleport-ent $ sudo ./install # This will copy Teleport Enterprise to /usr/local/bin.","title":"Setup"},{"location":"enterprise/ssh-kubernetes-fedramp/#configuration","text":"","title":"Configuration"},{"location":"enterprise/ssh-kubernetes-fedramp/#teleport-auth-server","text":"Now, save the following configuration file as /etc/teleport.yaml on the auth server. teleport : auth_token : zw6C82kq7VEUSJeSDzuldWsxakql6jrTYmphxRQOlrATTGbLQoaIwEBo48o9 # Pre-defined tokens for adding new nodes to a cluster. Each token specifies # the role a new node will be allowed to assume. The more secure way to # add nodes is to use `ttl node add --ttl` command to generate auto-expiring # tokens. # # We recommend to use tools like `pwgen` to generate sufficiently random # tokens of 32+ byte length. # you can also use auth server's IP, i.e. \"10.1.1.10:3025\" auth_servers : [ \"10.1.1.10:3025\" ] auth_service : # enable the auth service: enabled : true tokens : # this static token is used for other nodes to join this Teleport cluster - proxy,node:zw6C82kq7VEUSJeSDzuldWsxakql6jrTYmphxRQOlrATTGbLQoaIwEBo48o9 # this token is used to establish trust with other Teleport clusters - trusted_cluster:TaZff3DLbpsMZmIMhvEr7kulOgegjg7yyQNTS0q6UFWfsJ9N6rxVBjg6t7nw # To Support FIPS local_auth needs to be turned off and a SSO connector is # required to log into Teleport. authentication : # local_auth needs to be set to false in FIPS mode. local_auth : false type : saml # If using Proxy Mode, Teleport requires host key checks. # This setting needs is required to start in Teleport in FIPS mode proxy_checks_host_keys : true # SSH is also enabled on this node: ssh_service : enabled : false","title":"Teleport Auth Server"},{"location":"enterprise/ssh-kubernetes-fedramp/#teleport-node","text":"Save the following configuration file as /etc/teleport.yaml on the node server. teleport : auth_token : zw6C82kq7VEUSJeSDzuldWsxakql6jrTYmphxRQOlrATTGbLQoaIwEBo48o9 auth_servers : [ \"10.1.1.10:3025\" ] # enable ssh service and disable auth and proxy: ssh_service : enabled : true auth_service : enabled : false proxy_service : enabled : false","title":"Teleport Node"},{"location":"enterprise/ssh-kubernetes-fedramp/#systemd-unit-file","text":"Next, download the systemd service unit file from the examples directory on Github and save it as /etc/systemd/system/teleport.service on both servers. # run this on both servers: $ sudo systemctl daemon-reload $ sudo systemctl enable teleport","title":"Systemd Unit File"},{"location":"enterprise/ssh-kubernetes-fedramp/#starting-teleport-in-fips-mode","text":"When using teleport start --fips , Teleport will start in FIPS mode. Teleport will configure the TLS and SSH servers with FIPS compliant cryptographic algorithms. In FIPS mode, if non-compliant algorithms are chosen, Teleport will fail to start. In addition, Teleport checks if the binary was compiled against an approved cryptographic module (BoringCrypto) and fails to start if it was not. For OSS and Enterprise binaries not compiled with BoringCrypto, this flag will report that this version of Teleport is not compiled with the appropriate cryptographic module. Running commands like ps aux can be useful to note that Teleport is running in FedRAMP enforcing mode. If no ciphersuites are provided, Teleport will set the default ciphersuites to be FIPS 140-2 compliant. If ciphersuites, key exchange and MAC algorithms are provided in the Teleport configuration, Teleport will validate that they are FIPS 140-2 compliant.. Teleport will always enable at-rest encryption for both DynamoDB and S3. If recording proxy mode is selected, validation of host certificates should always happen.","title":"Starting Teleport in FIPS mode."},{"location":"enterprise/ssh-kubernetes-fedramp/#fedramp-audit-log","text":"At the close of a connection (close of a *srv.ServerContext) the total data transmitted and received is emitted to the Audit Log.","title":"FedRAMP Audit Log"},{"location":"enterprise/ssh-kubernetes-fedramp/#what-else-does-the-teleport-fips-binary-enforce","text":"Supporting configurable TLS versions. This is to ensure that only TLS 1.2 is supported in FedRAMP mode. Removes all uses of non-compliant algorithms like NaCl and replace with compliant algorithms like AES-GCM. Teleport is compiled with BoringCrypto User, host and CA certificates (and host keys for recording proxy mode) should only use 2048-bit RSA private keys.","title":"What else does the Teleport FIPS binary enforce?"},{"location":"enterprise/ssh-rbac/","text":"Role Based Access Control for SSH & K8s Introduction Role Based Access Control (RBAC) gives Teleport administrators more granular access controls. An example of an RBAC policy could be: \"admins can do anything, developers must never touch production servers and interns can only SSH into staging servers as guests\" RBAC is almost always used in conjunction with Single Sign-On ( SSO ) but it also works with users stored in Teleport's internal database. How does it work? Let's assume a company is using Okta to authenticate users and place them into groups. A typical deployment of Teleport in this scenario would look like this: Configure Teleport to use existing user identities stored in Okta. Okta would have users placed in certain groups, perhaps \"developers\", \"admins\", \"contractors\", etc. Teleport would have certain Teleport roles defined. For example: \"developers\" and \"admins\". Mappings would connect the Okta groups (SAML assertions) to the Teleport roles. Every Teleport user will be assigned a Teleport role based on their Okta group membership. Roles Every user in Teleport is always assigned a set of roles. One can think of them as \"SSH Roles\". The open source edition of Teleport automatically assigns every user to the built-in admin role but the Teleport Enterprise allows administrators to define their own roles with far greater control over the user permissions. Some of the permissions a role could define include: Which SSH nodes a user can or cannot access. Teleport uses node labels to do this, i.e. some nodes can be labeled \"production\" while others can be labeled \"staging\". Ability to replay recorded sessions. Ability to update cluster configuration. Which UNIX logins a user is allowed to use when logging into servers. A Teleport role works by having two lists of rules: allow rules and deny rules. When declaring access rules, keep in mind the following: Everything is denied by default. Deny rules get evaluated first and take priority. A rule consists of two parts: the resources and verbs. Here's an example of an allow rule describing a list verb applied to the SSH sessions resource. It means \"allow users of this role to see a list of active SSH sessions\". allow : - resources : [ session ] verbs : [ list ] If this rule was declared in deny section of a role definition, it effectively prohibits users from getting a list of trusted clusters and sessions. You can see all of the available resources and verbs under the allow section in the admin role configuration below. To manage cluster roles, a Teleport administrator can use the Web UI or the command line using tctl resource commands . To see the list of roles in a Teleport cluster, an administrator can execute: $ tctl get roles By default there is always one role called admin which looks like this: kind : role version : v3 metadata : name : admin spec : # SSH options used for user sessions with default values: options : # max_session_ttl defines the TTL (time to live) of SSH certificates # issued to the users with this role. max_session_ttl : 8h # forward_agent controls whether SSH agent forwarding is allowed forward_agent : true # port_forwarding controls whether TCP port forwarding is allowed port_forwarding : true # client_idle_timeout determines if SSH sessions to cluster nodes are forcefully # terminated after no activity from a client (idle client). it overrides the # global cluster setting. examples: \"30m\", \"1h\" or \"1h30m\" client_idle_timeout : never # determines if the clients will be forcefully disconnected when their # certificates expire in the middle of an active SSH session. # it overrides the global cluster setting. disconnect_expired_cert : no # Optional: max_connections Per user limit of concurrent sessions within a # cluster. max_connections : 2 # Optional: max_sessions total number of session channels which can be established # across a single connection. 10 will match OpenSSH default behavior. max_sessions : 10 # permit_x11_forwarding allows users to use X11 forwarding with openssh clients and servers through the proxy permit_x11_forwarding : true # allow section declares a list of resource/verb combinations that are # allowed for the users of this role. by default nothing is allowed. allow : # logins array defines the OS/UNIX logins a user is allowed to use. # a few special variables are supported here (see below) logins : [ root , '{% raw %}{{internal.logins}}{% endraw %}' ] # if kubernetes integration is enabled, this setting configures which # kubernetes groups the users of this role will be assigned to. # note that you can refer to a SAML/OIDC trait via the \"external\" property bag, # this allows you to specify Kubernetes group membership in an identity manager: kubernetes_groups : [ \"system:masters\" , \"{% raw %}{{external.trait_name}}{% endraw %}\" ] ] # list of node labels a user will be allowed to connect to: node_labels : # a user can only connect to a node marked with 'test' label: 'environment' : 'test' # the wildcard ('*') means \"any node\" '*' : '*' # labels can be specified as a list: 'environment' : [ 'test' , 'staging' ] # regular expressions are also supported, for example the equivalent # of the list example above can be expressed as: 'environment' : '^test|staging$' # defines roles that this user can can request. # needed for teleport's request workflow # https://gravitational.com/teleport/docs/enterprise/workflow/ request : roles : - dba # list of allow-rules. see below for more information. rules : - resources : [ role ] verbs : [ list , create , read , update , delete ] - resources : [ auth_connector ] verbs : [ list , create , read , update , delete ] - resources : [ session ] verbs : [ list , read ] - resources : [ trusted_cluster ] verbs : [ list , create , read , update , delete ] # list and read audit log, including audit events and recorded sessions - resources : [ event ] verbs : [ list , read ] # the deny section uses the identical format as the 'allow' section. # the deny rules always override allow rules. deny : {} The following variables can be used with logins field: Variable Description {% raw %}{{internal.logins}}{% endraw %} Substituted with \"allowed logins\" parameter used in tctl users add [user] <allowed logins> command. This applies only to users stored in Teleport's own local database. {% raw %}{{external.xyz}}{% endraw %} Substituted with a value from an external SSO provider . If using SAML, this will be expanded with \"xyz\" assertion value. For OIDC, this will be expanded a value of \"xyz\" claim. Both variables above are there to deliver the same benefit: they allow Teleport administrators to define allowed OS logins via the user database, be it the local DB, or an identity manager behind a SAML or OIDC endpoint. An example of a SAML assertion: Assuming you have the following SAML assertion attribute in your response: <Attribute Name=\"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\"> <AttributeValue>firstname.lastname</AttributeValue> </Attribute> ... you can use the following format in your role: logins: - '{% raw %}{{external[\"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\"]}}{% endraw %}' Role Options As shown above, a role can define certain restrictions on SSH sessions initiated by users. The table below documents the behavior of each option if multiple roles are assigned to a user. Option Description Multi-role behavior max_session_ttl Max. time to live (TTL) of a user's SSH certificates The shortest TTL wins forward_agent Allow SSH agent forwarding Logical \"OR\" i.e. if any role allows agent forwarding, it's allowed port_forwarding Allow TCP port forwarding Logical \"OR\" i.e. if any role allows port forwarding, it's allowed client_idle_timeout Forcefully terminate active SSH sessions after an idle interval The shortest timeout value wins, i.e. the most restrictive value is selected disconnect_expired_cert Forcefully terminate active SSH sessions when a client certificate expires Logical \"OR\" i.e. evaluates to \"yes\" if at least one role requires session termination max_connections Limit on how many active SSH sessions can be started via Teleport max_sessions Total number of session channels which can be established across a single SSH connection via Teleport RBAC for Hosts A Teleport role can also define which hosts (nodes) a user can have access to. This works by labeling nodes and listing allow/deny labels in a role definition. Consider the following use case: The infrastructure is split into staging/production environments using labels like environment=production and environment=staging . You can create roles that only have access to one environment. Let's say you create an intern role with allow rule for label environment=staging . Example The role below allows access to all nodes labeled \"env=stage\" except those that also have \"workload=database\" (these will always be denied). Access to any other nodes will be denied: kind : role version : v3 metadata : name : example-role spec : allow : node_labels : 'env' : 'stage' deny : node_labels : # multiple labels are interpreted as an \"or\" operation. in this case # Teleport will deny access to any node labeled as 'database' or 'backup' 'workload' : [ 'database' , 'backup' ] Dynamic RBAC Node labels can be dynamic, i.e. determined at runtime by an output of an executable. In this case, you can implement \"permissions follow workload\" policies (eg., any server where PostgreSQL is running becomes automatically accessible only by the members of the \"DBA\" group and nobody else). Extended Node Labels Syntax Below are a few examples for more complex filtering using various regexes. kind : role version : v3 metadata : name : example-role spec : allow : node_labels : # literal strings: 'environment' : 'test' # the wildcard ('*') means \"any node\" '*' : '*' # a list of alternative options: 'environment' : [ 'test' , 'staging' ] # regular expressions are also supported, for example the equivalent # of the list example above can be expressed as: 'environment' : '^test|staging$' RBAC for Sessions As shown in the role example above, a Teleport administrator can restrict access to user sessions using the following rule: rules : - resources : [ session ] verbs : [ list , read ] \"list\" determines if a user is allowed to see the list of past sessions. \"read\" determines if a user is allowed to replay a session. It is possible to restrict \"list\" but to allow \"read\" (in this case a user will be able to replay a session using tsh play if they know the session ID) FAQ Q: What if a node has multiple labels? A: In this case, the access will be granted only if all of the labels defined in the role are present. This effectively means Teleport uses an \"AND\" operator when evaluating node-level access using labels. Q: Can I use node-level RBAC with OpenSSH servers? A: No. OpenSSH servers running sshd do not have the ability to label themselves. This is one of the reasons to run Teleport node service instead.","title":"RBAC"},{"location":"enterprise/ssh-rbac/#role-based-access-control-for-ssh-k8s","text":"","title":"Role Based Access Control for SSH &amp; K8s"},{"location":"enterprise/ssh-rbac/#introduction","text":"Role Based Access Control (RBAC) gives Teleport administrators more granular access controls. An example of an RBAC policy could be: \"admins can do anything, developers must never touch production servers and interns can only SSH into staging servers as guests\" RBAC is almost always used in conjunction with Single Sign-On ( SSO ) but it also works with users stored in Teleport's internal database.","title":"Introduction"},{"location":"enterprise/ssh-rbac/#how-does-it-work","text":"Let's assume a company is using Okta to authenticate users and place them into groups. A typical deployment of Teleport in this scenario would look like this: Configure Teleport to use existing user identities stored in Okta. Okta would have users placed in certain groups, perhaps \"developers\", \"admins\", \"contractors\", etc. Teleport would have certain Teleport roles defined. For example: \"developers\" and \"admins\". Mappings would connect the Okta groups (SAML assertions) to the Teleport roles. Every Teleport user will be assigned a Teleport role based on their Okta group membership.","title":"How does it work?"},{"location":"enterprise/ssh-rbac/#roles","text":"Every user in Teleport is always assigned a set of roles. One can think of them as \"SSH Roles\". The open source edition of Teleport automatically assigns every user to the built-in admin role but the Teleport Enterprise allows administrators to define their own roles with far greater control over the user permissions. Some of the permissions a role could define include: Which SSH nodes a user can or cannot access. Teleport uses node labels to do this, i.e. some nodes can be labeled \"production\" while others can be labeled \"staging\". Ability to replay recorded sessions. Ability to update cluster configuration. Which UNIX logins a user is allowed to use when logging into servers. A Teleport role works by having two lists of rules: allow rules and deny rules. When declaring access rules, keep in mind the following: Everything is denied by default. Deny rules get evaluated first and take priority. A rule consists of two parts: the resources and verbs. Here's an example of an allow rule describing a list verb applied to the SSH sessions resource. It means \"allow users of this role to see a list of active SSH sessions\". allow : - resources : [ session ] verbs : [ list ] If this rule was declared in deny section of a role definition, it effectively prohibits users from getting a list of trusted clusters and sessions. You can see all of the available resources and verbs under the allow section in the admin role configuration below. To manage cluster roles, a Teleport administrator can use the Web UI or the command line using tctl resource commands . To see the list of roles in a Teleport cluster, an administrator can execute: $ tctl get roles By default there is always one role called admin which looks like this: kind : role version : v3 metadata : name : admin spec : # SSH options used for user sessions with default values: options : # max_session_ttl defines the TTL (time to live) of SSH certificates # issued to the users with this role. max_session_ttl : 8h # forward_agent controls whether SSH agent forwarding is allowed forward_agent : true # port_forwarding controls whether TCP port forwarding is allowed port_forwarding : true # client_idle_timeout determines if SSH sessions to cluster nodes are forcefully # terminated after no activity from a client (idle client). it overrides the # global cluster setting. examples: \"30m\", \"1h\" or \"1h30m\" client_idle_timeout : never # determines if the clients will be forcefully disconnected when their # certificates expire in the middle of an active SSH session. # it overrides the global cluster setting. disconnect_expired_cert : no # Optional: max_connections Per user limit of concurrent sessions within a # cluster. max_connections : 2 # Optional: max_sessions total number of session channels which can be established # across a single connection. 10 will match OpenSSH default behavior. max_sessions : 10 # permit_x11_forwarding allows users to use X11 forwarding with openssh clients and servers through the proxy permit_x11_forwarding : true # allow section declares a list of resource/verb combinations that are # allowed for the users of this role. by default nothing is allowed. allow : # logins array defines the OS/UNIX logins a user is allowed to use. # a few special variables are supported here (see below) logins : [ root , '{% raw %}{{internal.logins}}{% endraw %}' ] # if kubernetes integration is enabled, this setting configures which # kubernetes groups the users of this role will be assigned to. # note that you can refer to a SAML/OIDC trait via the \"external\" property bag, # this allows you to specify Kubernetes group membership in an identity manager: kubernetes_groups : [ \"system:masters\" , \"{% raw %}{{external.trait_name}}{% endraw %}\" ] ] # list of node labels a user will be allowed to connect to: node_labels : # a user can only connect to a node marked with 'test' label: 'environment' : 'test' # the wildcard ('*') means \"any node\" '*' : '*' # labels can be specified as a list: 'environment' : [ 'test' , 'staging' ] # regular expressions are also supported, for example the equivalent # of the list example above can be expressed as: 'environment' : '^test|staging$' # defines roles that this user can can request. # needed for teleport's request workflow # https://gravitational.com/teleport/docs/enterprise/workflow/ request : roles : - dba # list of allow-rules. see below for more information. rules : - resources : [ role ] verbs : [ list , create , read , update , delete ] - resources : [ auth_connector ] verbs : [ list , create , read , update , delete ] - resources : [ session ] verbs : [ list , read ] - resources : [ trusted_cluster ] verbs : [ list , create , read , update , delete ] # list and read audit log, including audit events and recorded sessions - resources : [ event ] verbs : [ list , read ] # the deny section uses the identical format as the 'allow' section. # the deny rules always override allow rules. deny : {} The following variables can be used with logins field: Variable Description {% raw %}{{internal.logins}}{% endraw %} Substituted with \"allowed logins\" parameter used in tctl users add [user] <allowed logins> command. This applies only to users stored in Teleport's own local database. {% raw %}{{external.xyz}}{% endraw %} Substituted with a value from an external SSO provider . If using SAML, this will be expanded with \"xyz\" assertion value. For OIDC, this will be expanded a value of \"xyz\" claim. Both variables above are there to deliver the same benefit: they allow Teleport administrators to define allowed OS logins via the user database, be it the local DB, or an identity manager behind a SAML or OIDC endpoint.","title":"Roles"},{"location":"enterprise/ssh-rbac/#role-options","text":"As shown above, a role can define certain restrictions on SSH sessions initiated by users. The table below documents the behavior of each option if multiple roles are assigned to a user. Option Description Multi-role behavior max_session_ttl Max. time to live (TTL) of a user's SSH certificates The shortest TTL wins forward_agent Allow SSH agent forwarding Logical \"OR\" i.e. if any role allows agent forwarding, it's allowed port_forwarding Allow TCP port forwarding Logical \"OR\" i.e. if any role allows port forwarding, it's allowed client_idle_timeout Forcefully terminate active SSH sessions after an idle interval The shortest timeout value wins, i.e. the most restrictive value is selected disconnect_expired_cert Forcefully terminate active SSH sessions when a client certificate expires Logical \"OR\" i.e. evaluates to \"yes\" if at least one role requires session termination max_connections Limit on how many active SSH sessions can be started via Teleport max_sessions Total number of session channels which can be established across a single SSH connection via Teleport","title":"Role Options"},{"location":"enterprise/ssh-rbac/#rbac-for-hosts","text":"A Teleport role can also define which hosts (nodes) a user can have access to. This works by labeling nodes and listing allow/deny labels in a role definition. Consider the following use case: The infrastructure is split into staging/production environments using labels like environment=production and environment=staging . You can create roles that only have access to one environment. Let's say you create an intern role with allow rule for label environment=staging .","title":"RBAC for Hosts"},{"location":"enterprise/ssh-rbac/#example","text":"The role below allows access to all nodes labeled \"env=stage\" except those that also have \"workload=database\" (these will always be denied). Access to any other nodes will be denied: kind : role version : v3 metadata : name : example-role spec : allow : node_labels : 'env' : 'stage' deny : node_labels : # multiple labels are interpreted as an \"or\" operation. in this case # Teleport will deny access to any node labeled as 'database' or 'backup' 'workload' : [ 'database' , 'backup' ] Dynamic RBAC Node labels can be dynamic, i.e. determined at runtime by an output of an executable. In this case, you can implement \"permissions follow workload\" policies (eg., any server where PostgreSQL is running becomes automatically accessible only by the members of the \"DBA\" group and nobody else).","title":"Example"},{"location":"enterprise/ssh-rbac/#extended-node-labels-syntax","text":"Below are a few examples for more complex filtering using various regexes. kind : role version : v3 metadata : name : example-role spec : allow : node_labels : # literal strings: 'environment' : 'test' # the wildcard ('*') means \"any node\" '*' : '*' # a list of alternative options: 'environment' : [ 'test' , 'staging' ] # regular expressions are also supported, for example the equivalent # of the list example above can be expressed as: 'environment' : '^test|staging$'","title":"Extended Node Labels Syntax"},{"location":"enterprise/ssh-rbac/#rbac-for-sessions","text":"As shown in the role example above, a Teleport administrator can restrict access to user sessions using the following rule: rules : - resources : [ session ] verbs : [ list , read ] \"list\" determines if a user is allowed to see the list of past sessions. \"read\" determines if a user is allowed to replay a session. It is possible to restrict \"list\" but to allow \"read\" (in this case a user will be able to replay a session using tsh play if they know the session ID)","title":"RBAC for Sessions"},{"location":"enterprise/ssh-rbac/#faq","text":"Q: What if a node has multiple labels? A: In this case, the access will be granted only if all of the labels defined in the role are present. This effectively means Teleport uses an \"AND\" operator when evaluating node-level access using labels. Q: Can I use node-level RBAC with OpenSSH servers? A: No. OpenSSH servers running sshd do not have the ability to label themselves. This is one of the reasons to run Teleport node service instead.","title":"FAQ"},{"location":"enterprise/sso/oidc/","text":"OAuth2 / OIDC Authentication for SSH This guide will cover how to configure an SSO provider using OpenID Connect (also known as OIDC) to issue SSH credentials to a specific groups of users. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. Version Warning This guide requires an Enterprise edition of Teleport. The Community edition of Teleport only supports Github as an SSO provider. Enable OIDC Authentication First, configure Teleport auth server to use OIDC authentication instead of the local user database. Update /etc/teleport.yaml as show below and restart the teleport daemon. auth_service : authentication : type : oidc Identity Providers Register Teleport with the external identity provider you will be using and obtain your client_id and client_secret . This information should be documented on the identity providers website. Here are a few links: Auth0 Client Configuration Google Identity Platform Keycloak Client Registration Add your OIDC connector information to teleport.yaml . A few examples are provided below. Tip For Google / G Suite please follow our dedicated Guide OIDC Redirect URL OIDC relies on HTTP re-directs to return control back to Teleport after authentication is complete. The redirect URL must be selected by a Teleport administrator in advance. If the Teleport web proxy is running on proxy.example.com host, the redirect URL should be https://proxy.example.com:3080/v1/webapi/oidc/callback OIDC connector configuration The next step is to add an OIDC connector to Teleport. The connectors are manipulated via tctl resource commands . To create a new connector, create a connector resource file in YAML format, for example oidc-connector.yaml . The file contents are shown below. This connector requests the scope group from the identity provider then mapping the value to either to admin role or the user role depending on the value returned for group within the claims. { !examples/resources/oidc-connector.yaml! } Create the connector: $ tctl create oidc-connector.yaml Create Teleport Roles The next step is to define Teleport roles. They are created using the same tctl resource commands as we used for the auth connector. Below are two example roles that are mentioned above, the first is an admin with full access to the system while the second is a developer with limited access. # role-admin.yaml kind : \"role\" version : \"v3\" metadata : name : \"admin\" spec : options : max_session_ttl : \"90h0m0s\" allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] Users are only allowed to login to nodes labelled with access: relaxed teleport label. Developers can log in as either ubuntu to a username that arrives in their assertions. Developers also do not have any rules needed to obtain admin access. # role-dev.yaml kind : \"role\" version : \"v3\" metadata : name : \"dev\" spec : options : max_session_ttl : \"90h0m0s\" allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Create both roles: $ tctl create role-admin.yaml $ tctl create role-dev.yaml Optional: ACR Values Teleport supports sending Authentication Context Class Reference (ACR) values when obtaining an authorization code from an OIDC provider. By default ACR values are not set. However, if the acr_values field is set, Teleport expects to receive the same value in the acr claim, otherwise it will consider the callback invalid. In addition, Teleport supports OIDC provider specific ACR value processing which can be enabled by setting the provider field in OIDC configuration. At the moment, the only build-in support is for NetIQ. A example of using ACR values and provider specific processing is below: # example connector which uses ACR values kind : oidc version : v2 metadata : name : \"oidc-connector\" spec : issuer_url : \"https://oidc.example.com\" client_id : \"xxxxxxxxxxxxxxxxxxxxxxx.example.com\" client_secret : \"zzzzzzzzzzzzzzzzzzzzzzzz\" redirect_url : \"https://<cluster-url>:3080/v1/webapi/oidc/callback\" display : \"Login with Example\" acr_values : \"foo/bar\" provider : netiq scope : [ \"group\" ] claims_to_roles : - claim : \"group\" value : \"admin\" roles : [ \"admin\" ] - claim : \"group\" value : \"user\" roles : [ \"user\" ] Optional: Redirect URL and Timeout The redirect URL must be accessible by all user, optional redirect timeout. # Extra parts of OIDC yaml have been removed. spec : redirect_url : https://<cluster-url>.example.com:3080/v1/webapi/oidc/callback # Optional Redirect Timeout. # redirect_timeout: 90s Optional: Prompt By default, Teleport will prompt end users to select an account each time they log in even if the user only has one account. Teleport 4.2 now lets Teleport Admins configure this option. Since prompt is optional, by setting the variable to an empty string Teleport will override the default select_account . kind : oidc version : v2 metadata : name : connector spec : prompt : '' The below example will prompt the end-user for reauthentication and will require consent from the client. kind : oidc version : v2 metadata : name : connector spec : prompt : 'login consent' A list of available optional prompt parameters are available from the OpenID website . Testing For the Web UI, if the above configuration were real, you would see a button that says Login with Example . Simply click on that and you will be re-directed to a login page for your identity provider and if successful, redirected back to Teleport. For console login, you simple type tsh --proxy <proxy-addr> ssh <server-addr> and a browser window should automatically open taking you to the login page for your identity provider. tsh will also output a link the login page of the identity provider if you are not automatically redirected. Troubleshooting If you get \"access denied errors\" the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass --debug flag to teleport start command.","title":"OIDC"},{"location":"enterprise/sso/oidc/#oauth2-oidc-authentication-for-ssh","text":"This guide will cover how to configure an SSO provider using OpenID Connect (also known as OIDC) to issue SSH credentials to a specific groups of users. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. Version Warning This guide requires an Enterprise edition of Teleport. The Community edition of Teleport only supports Github as an SSO provider.","title":"OAuth2 / OIDC Authentication for SSH"},{"location":"enterprise/sso/oidc/#enable-oidc-authentication","text":"First, configure Teleport auth server to use OIDC authentication instead of the local user database. Update /etc/teleport.yaml as show below and restart the teleport daemon. auth_service : authentication : type : oidc","title":"Enable OIDC Authentication"},{"location":"enterprise/sso/oidc/#identity-providers","text":"Register Teleport with the external identity provider you will be using and obtain your client_id and client_secret . This information should be documented on the identity providers website. Here are a few links: Auth0 Client Configuration Google Identity Platform Keycloak Client Registration Add your OIDC connector information to teleport.yaml . A few examples are provided below. Tip For Google / G Suite please follow our dedicated Guide","title":"Identity Providers"},{"location":"enterprise/sso/oidc/#oidc-redirect-url","text":"OIDC relies on HTTP re-directs to return control back to Teleport after authentication is complete. The redirect URL must be selected by a Teleport administrator in advance. If the Teleport web proxy is running on proxy.example.com host, the redirect URL should be https://proxy.example.com:3080/v1/webapi/oidc/callback","title":"OIDC Redirect URL"},{"location":"enterprise/sso/oidc/#oidc-connector-configuration","text":"The next step is to add an OIDC connector to Teleport. The connectors are manipulated via tctl resource commands . To create a new connector, create a connector resource file in YAML format, for example oidc-connector.yaml . The file contents are shown below. This connector requests the scope group from the identity provider then mapping the value to either to admin role or the user role depending on the value returned for group within the claims. { !examples/resources/oidc-connector.yaml! } Create the connector: $ tctl create oidc-connector.yaml","title":"OIDC connector configuration"},{"location":"enterprise/sso/oidc/#create-teleport-roles","text":"The next step is to define Teleport roles. They are created using the same tctl resource commands as we used for the auth connector. Below are two example roles that are mentioned above, the first is an admin with full access to the system while the second is a developer with limited access. # role-admin.yaml kind : \"role\" version : \"v3\" metadata : name : \"admin\" spec : options : max_session_ttl : \"90h0m0s\" allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] Users are only allowed to login to nodes labelled with access: relaxed teleport label. Developers can log in as either ubuntu to a username that arrives in their assertions. Developers also do not have any rules needed to obtain admin access. # role-dev.yaml kind : \"role\" version : \"v3\" metadata : name : \"dev\" spec : options : max_session_ttl : \"90h0m0s\" allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Create both roles: $ tctl create role-admin.yaml $ tctl create role-dev.yaml","title":"Create Teleport Roles"},{"location":"enterprise/sso/oidc/#optional-acr-values","text":"Teleport supports sending Authentication Context Class Reference (ACR) values when obtaining an authorization code from an OIDC provider. By default ACR values are not set. However, if the acr_values field is set, Teleport expects to receive the same value in the acr claim, otherwise it will consider the callback invalid. In addition, Teleport supports OIDC provider specific ACR value processing which can be enabled by setting the provider field in OIDC configuration. At the moment, the only build-in support is for NetIQ. A example of using ACR values and provider specific processing is below: # example connector which uses ACR values kind : oidc version : v2 metadata : name : \"oidc-connector\" spec : issuer_url : \"https://oidc.example.com\" client_id : \"xxxxxxxxxxxxxxxxxxxxxxx.example.com\" client_secret : \"zzzzzzzzzzzzzzzzzzzzzzzz\" redirect_url : \"https://<cluster-url>:3080/v1/webapi/oidc/callback\" display : \"Login with Example\" acr_values : \"foo/bar\" provider : netiq scope : [ \"group\" ] claims_to_roles : - claim : \"group\" value : \"admin\" roles : [ \"admin\" ] - claim : \"group\" value : \"user\" roles : [ \"user\" ]","title":"Optional: ACR Values"},{"location":"enterprise/sso/oidc/#optional-redirect-url-and-timeout","text":"The redirect URL must be accessible by all user, optional redirect timeout. # Extra parts of OIDC yaml have been removed. spec : redirect_url : https://<cluster-url>.example.com:3080/v1/webapi/oidc/callback # Optional Redirect Timeout. # redirect_timeout: 90s","title":"Optional: Redirect URL and Timeout"},{"location":"enterprise/sso/oidc/#optional-prompt","text":"By default, Teleport will prompt end users to select an account each time they log in even if the user only has one account. Teleport 4.2 now lets Teleport Admins configure this option. Since prompt is optional, by setting the variable to an empty string Teleport will override the default select_account . kind : oidc version : v2 metadata : name : connector spec : prompt : '' The below example will prompt the end-user for reauthentication and will require consent from the client. kind : oidc version : v2 metadata : name : connector spec : prompt : 'login consent' A list of available optional prompt parameters are available from the OpenID website .","title":"Optional: Prompt"},{"location":"enterprise/sso/oidc/#testing","text":"For the Web UI, if the above configuration were real, you would see a button that says Login with Example . Simply click on that and you will be re-directed to a login page for your identity provider and if successful, redirected back to Teleport. For console login, you simple type tsh --proxy <proxy-addr> ssh <server-addr> and a browser window should automatically open taking you to the login page for your identity provider. tsh will also output a link the login page of the identity provider if you are not automatically redirected.","title":"Testing"},{"location":"enterprise/sso/oidc/#troubleshooting","text":"If you get \"access denied errors\" the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass --debug flag to teleport start command.","title":"Troubleshooting"},{"location":"enterprise/sso/ssh-adfs/","text":"SSH Authentication with ADFS Active Directory as an SSO provider for SSH authentication This guide will cover how to configure Active Directory Federation Services ADFS to be a single sign-on (SSO) provider to issue SSH credentials to specific groups of users. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. Version Warning This guide requires a commercial edition of Teleport. The open source edition of Teleport only supports Github as an SSO provider. Enable ADFS Authentication First, configure Teleport auth server to use ADFS authentication instead of the local user database. Update /etc/teleport.yaml as shown below and restart the teleport daemon. auth_service : authentication : type : saml Configure ADFS You'll need to configure ADFS to export claims about a user (Claims Provider Trust in ADFS terminology) and you'll need to configure AD FS to trust Teleport (a Relying Party Trust in ADFS terminology). For Claims Provider Trust configuration you'll need to specify at least the following two incoming claims: Name ID and Group . Name ID should be a mapping of the LDAP Attribute E-Mail-Addresses to Name ID . A group membership claim should be used to map users to roles (for example to separate normal users and admins). In addition if you are using dynamic roles (see below), it may be useful to map the LDAP Attribute SAM-Account-Name to Windows account name and create another mapping of E-Mail-Addresses to UPN . You'll also need to create a Relying Party Trust, use the below information to help guide you through the Wizard. Note, for development purposes we recommend using https://localhost:3080/v1/webapi/saml/acs as the Assertion Consumer Service (ACS) URL, but for production you'll want to change this to a domain that can be accessed by other users as well. Create a claims aware trust. Enter data about the relying party manually. Set the display name to something along the lines of \"Teleport\". Skip the token encryption certificate. Select \"Enable support for SAML 2.0 Web SSO protocol\" and set the URL to https://localhost:3080/v1/webapi/saml/acs . Set the relying party trust identifier to https://localhost:3080/v1/webapi/saml/acs as well. For access control policy select \"Permit everyone\" . Once the Relying Party Trust has been created, update the Claim Issuance Policy for it. Like before make sure you send at least Name ID and Group claims to the relying party (Teleport). If you are using dynamic roles, it may be useful to map the LDAP Attribute SAM-Account-Name to \"Windows account name\" and create another mapping of E-Mail-Addresses to \"UPN\" . Lastly, ensure the user you create in Active Directory has an email address associated with it. To check this open Server Manager then \"Tools -> Active Directory Users and Computers\" and select the user and right click and open properties. Make sure the email address field is filled out. Create Teleport Roles Lets create two Teleport roles: one for administrators and the other is for normal users. You can create them using tctl create {file name} CLI command or via the Web UI. # admin-role.yaml kind : \"role\" version : \"v3\" metadata : name : \"admin\" spec : options : max_session_ttl : \"8h0m0s\" allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] # user-role.yaml kind : \"role\" version : \"v3\" metadata : name : \"dev\" spec : options : # regular users can only be guests and their certificates will have a TTL of 1 hour: max_session_ttl : \"1h\" allow : # only allow login as either ubuntu or the 'windowsaccountname' claim logins : [ '{% raw %}{{external[\"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\"]}}{% endraw %}' , ubuntu ] node_labels : \"access\" : \"relaxed\" This role declares: Devs are only allowed to login to nodes labelled with access: relaxed label. Developers can log in as ubuntu user Notice {% raw %}{{external[\"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\"]}}{% endraw %} login. It configures Teleport to look at \"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\" ADFS claim and use that field as an allowed login for each user. Also note the double quotes ( \" ) and square brackets ( [] ) around the claim name - these are important. Developers also do not have any \"allow rules\" i.e. they will not be able to see/replay past sessions or re-configure the Teleport cluster. Next, create a SAML connector resource : { !examples/resources/adfs-connector.yaml! } The acs field should match the value you set in ADFS earlier and you can obtain the entity_descriptor_url from ADFS under \"ADFS -> Service -> Endpoints -> Metadata\" . The attributes_to_roles is used to map attributes to the Teleport roles you just created. In our situation, we are mapping the \"Group\" attribute whose full name is http://schemas.xmlsoap.org/claims/Group with a value of \"admins\" to the \"admin\" role. Groups with the value \"users\" is being mapped to the \"users\" role. Export the Signing Key For the last step, you'll need to export the signing key: $ tctl saml export adfs Save the output to a file named saml.crt , return back to ADFS, open the \"Relying Party Trust\" and add this file as one of the signature verification certificates. Testing The Web UI will now contain a new button: \"Login with MS Active Directory\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple SAML connectors. In this case a connector name can be passed via tsh login --auth=connector_name IMPORTANT Teleport only supports sending party initiated flows for SAML 2.0. This means you can not initiate login from your identity provider, you have to initiate login from either the Teleport Web UI or CLI. Troubleshooting If you get \"access denied errors\" the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass --debug flag to teleport start command.","title":"Active Directory (ADFS)"},{"location":"enterprise/sso/ssh-adfs/#ssh-authentication-with-adfs","text":"","title":"SSH Authentication with ADFS"},{"location":"enterprise/sso/ssh-adfs/#active-directory-as-an-sso-provider-for-ssh-authentication","text":"This guide will cover how to configure Active Directory Federation Services ADFS to be a single sign-on (SSO) provider to issue SSH credentials to specific groups of users. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. Version Warning This guide requires a commercial edition of Teleport. The open source edition of Teleport only supports Github as an SSO provider.","title":"Active Directory as an SSO provider for SSH authentication"},{"location":"enterprise/sso/ssh-adfs/#enable-adfs-authentication","text":"First, configure Teleport auth server to use ADFS authentication instead of the local user database. Update /etc/teleport.yaml as shown below and restart the teleport daemon. auth_service : authentication : type : saml","title":"Enable ADFS Authentication"},{"location":"enterprise/sso/ssh-adfs/#configure-adfs","text":"You'll need to configure ADFS to export claims about a user (Claims Provider Trust in ADFS terminology) and you'll need to configure AD FS to trust Teleport (a Relying Party Trust in ADFS terminology). For Claims Provider Trust configuration you'll need to specify at least the following two incoming claims: Name ID and Group . Name ID should be a mapping of the LDAP Attribute E-Mail-Addresses to Name ID . A group membership claim should be used to map users to roles (for example to separate normal users and admins). In addition if you are using dynamic roles (see below), it may be useful to map the LDAP Attribute SAM-Account-Name to Windows account name and create another mapping of E-Mail-Addresses to UPN . You'll also need to create a Relying Party Trust, use the below information to help guide you through the Wizard. Note, for development purposes we recommend using https://localhost:3080/v1/webapi/saml/acs as the Assertion Consumer Service (ACS) URL, but for production you'll want to change this to a domain that can be accessed by other users as well. Create a claims aware trust. Enter data about the relying party manually. Set the display name to something along the lines of \"Teleport\". Skip the token encryption certificate. Select \"Enable support for SAML 2.0 Web SSO protocol\" and set the URL to https://localhost:3080/v1/webapi/saml/acs . Set the relying party trust identifier to https://localhost:3080/v1/webapi/saml/acs as well. For access control policy select \"Permit everyone\" . Once the Relying Party Trust has been created, update the Claim Issuance Policy for it. Like before make sure you send at least Name ID and Group claims to the relying party (Teleport). If you are using dynamic roles, it may be useful to map the LDAP Attribute SAM-Account-Name to \"Windows account name\" and create another mapping of E-Mail-Addresses to \"UPN\" . Lastly, ensure the user you create in Active Directory has an email address associated with it. To check this open Server Manager then \"Tools -> Active Directory Users and Computers\" and select the user and right click and open properties. Make sure the email address field is filled out.","title":"Configure ADFS"},{"location":"enterprise/sso/ssh-adfs/#create-teleport-roles","text":"Lets create two Teleport roles: one for administrators and the other is for normal users. You can create them using tctl create {file name} CLI command or via the Web UI. # admin-role.yaml kind : \"role\" version : \"v3\" metadata : name : \"admin\" spec : options : max_session_ttl : \"8h0m0s\" allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] # user-role.yaml kind : \"role\" version : \"v3\" metadata : name : \"dev\" spec : options : # regular users can only be guests and their certificates will have a TTL of 1 hour: max_session_ttl : \"1h\" allow : # only allow login as either ubuntu or the 'windowsaccountname' claim logins : [ '{% raw %}{{external[\"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\"]}}{% endraw %}' , ubuntu ] node_labels : \"access\" : \"relaxed\" This role declares: Devs are only allowed to login to nodes labelled with access: relaxed label. Developers can log in as ubuntu user Notice {% raw %}{{external[\"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\"]}}{% endraw %} login. It configures Teleport to look at \"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\" ADFS claim and use that field as an allowed login for each user. Also note the double quotes ( \" ) and square brackets ( [] ) around the claim name - these are important. Developers also do not have any \"allow rules\" i.e. they will not be able to see/replay past sessions or re-configure the Teleport cluster. Next, create a SAML connector resource : { !examples/resources/adfs-connector.yaml! } The acs field should match the value you set in ADFS earlier and you can obtain the entity_descriptor_url from ADFS under \"ADFS -> Service -> Endpoints -> Metadata\" . The attributes_to_roles is used to map attributes to the Teleport roles you just created. In our situation, we are mapping the \"Group\" attribute whose full name is http://schemas.xmlsoap.org/claims/Group with a value of \"admins\" to the \"admin\" role. Groups with the value \"users\" is being mapped to the \"users\" role.","title":"Create Teleport Roles"},{"location":"enterprise/sso/ssh-adfs/#export-the-signing-key","text":"For the last step, you'll need to export the signing key: $ tctl saml export adfs Save the output to a file named saml.crt , return back to ADFS, open the \"Relying Party Trust\" and add this file as one of the signature verification certificates.","title":"Export the Signing Key"},{"location":"enterprise/sso/ssh-adfs/#testing","text":"The Web UI will now contain a new button: \"Login with MS Active Directory\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple SAML connectors. In this case a connector name can be passed via tsh login --auth=connector_name IMPORTANT Teleport only supports sending party initiated flows for SAML 2.0. This means you can not initiate login from your identity provider, you have to initiate login from either the Teleport Web UI or CLI.","title":"Testing"},{"location":"enterprise/sso/ssh-adfs/#troubleshooting","text":"If you get \"access denied errors\" the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass --debug flag to teleport start command.","title":"Troubleshooting"},{"location":"enterprise/sso/ssh-azuread/","text":"SSH Authentication with Azure Active Directory (AD) This guide will cover how to configure Microsoft Azure Active Directory to issue SSH credentials to specific groups of users with a SAML Authentication Connector. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" Azure AD group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. The following steps configure an example SAML authentication connector matching AzureAD groups with security roles. You can choose to configure other options. Version Warning This guide requires an Enterprise version of Teleport. The open source edition of Teleport only supports Github as an SSO provider. Prerequisites: Before you get started you\u2019ll need: An Enterprise version of Teleport v4.2 or greater, downloaded from https://dashboard.gravitational.com/ . An Azure AD admin account with access to creating non-gallery applications (P2 License) To register one or more users in the directory To create at least two security groups in AzureAD and assign one or more users to each group Configure Azure AD Select Enterprise Applications from the AzureAD Directory Home Select New application Select a Non-gallery application Enter the display name (Ex: Teleport) 5.Select properties under Manage and turn off User assignment required Select Single Sign-on under Manage and choose SAML Select to edit Basic SAML Configuration Put in the Entity ID and Reply URL the same proxy url https://teleport.example.com:3080/v1/webapi/saml/acs Edit User Attributes & Claims i. Edit the Claim Name. Change the name identifier format to Default. Make sure the source attribute is user.userprincipalname. ii. Add a group Claim to have user security groups available to the connector iii. Add a Claim to pass the username from transforming the AzureAD User name. On the SAML Signing Certificate select to download SAML Download the Federation Metadata XML. Important This is a important document. Treat the Federation Metadata XML file as you would a password. Create a SAML Connector Now, create a SAML connector resource . Replace the acs element with your Teleport address, update the group IDs with the actual AzureAD group ID values, and insert the downloaded Federation Metadata XML into the entity_descriptor resource. Write down this template as azure-connector.yaml : kind : saml version : v2 metadata : # the name of the connector name : azure-saml spec : display : \"Microsoft\" # acs is the Assertion Consumer Service URL. This should be the address of # the Teleport proxy that your identity provider will communicate with. acs : https://teleport.example.com:3080/v1/webapi/saml/acs attributes_to_roles : - { name : \"http://schemas.microsoft.com/ws/2008/06/identity/claims/groups\" , value : \"<group id 930210...>\" , roles : [ \"admin\" ]} - { name : \"http://schemas.microsoft.com/ws/2008/06/identity/claims/groups\" , value : \"<group id 93b110...>\" , roles : [ \"dev\" ]} entity_descriptor : | <federationmedata.xml contents> Create the connector using tctl tool: FYI Teleport will automatically transform the contents of the connector when viewed from the web UI. Create Teleport Roles We are going to create 2 roles: Privileged role admin who is able to login as root and is capable of administrating the cluster Non-privileged role dev kind : role version : v3 metadata : name : admin spec : options : max_session_ttl : 24h allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] Devs are only allowed to login to nodes labeled with access: relaxed Teleport label. Developers can log in as either ubuntu or a username that arrives in their assertions. Developers also do not have any rules needed to obtain admin access to Teleport. kind : role version : v3 metadata : name : dev spec : options : max_session_ttl : 24h allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Notice: Replace ubuntu with linux login available on your servers! $ tctl create admin.yaml $ tctl create dev.yaml Testing Update the Teleport settings to use the SAML settings to make this the default. auth_service : authentication : type : saml The Web UI will now contain a new button: \"Login with Microsoft\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple SAML connectors. In this case a connector name can be passed via tsh login --auth=connector_name Troubleshooting If you get \"access denied\" errors the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Example of a user being denied due as the role clusteradmin wasn't setup. { \"code\" : \"T1001W\" , \"error\" : \"role clusteradmin is not found\" , \"event\" : \"user.login\" , \"method\" : \"saml\" , \"success\" : false , \"time\" : \"2019-06-15T19:38:07Z\" , \"uid\" : \"cd9e45d0-b68c-43c3-87cf-73c4e0ec37e9\" } Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass the --debug flag to teleport start command.","title":"Azure Active Directory (AD)"},{"location":"enterprise/sso/ssh-azuread/#ssh-authentication-with-azure-active-directory-ad","text":"This guide will cover how to configure Microsoft Azure Active Directory to issue SSH credentials to specific groups of users with a SAML Authentication Connector. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" Azure AD group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. The following steps configure an example SAML authentication connector matching AzureAD groups with security roles. You can choose to configure other options. Version Warning This guide requires an Enterprise version of Teleport. The open source edition of Teleport only supports Github as an SSO provider.","title":"SSH Authentication with Azure Active Directory (AD)"},{"location":"enterprise/sso/ssh-azuread/#prerequisites","text":"Before you get started you\u2019ll need: An Enterprise version of Teleport v4.2 or greater, downloaded from https://dashboard.gravitational.com/ . An Azure AD admin account with access to creating non-gallery applications (P2 License) To register one or more users in the directory To create at least two security groups in AzureAD and assign one or more users to each group","title":"Prerequisites:"},{"location":"enterprise/sso/ssh-azuread/#configure-azure-ad","text":"Select Enterprise Applications from the AzureAD Directory Home Select New application Select a Non-gallery application Enter the display name (Ex: Teleport) 5.Select properties under Manage and turn off User assignment required Select Single Sign-on under Manage and choose SAML Select to edit Basic SAML Configuration Put in the Entity ID and Reply URL the same proxy url https://teleport.example.com:3080/v1/webapi/saml/acs Edit User Attributes & Claims i. Edit the Claim Name. Change the name identifier format to Default. Make sure the source attribute is user.userprincipalname. ii. Add a group Claim to have user security groups available to the connector iii. Add a Claim to pass the username from transforming the AzureAD User name. On the SAML Signing Certificate select to download SAML Download the Federation Metadata XML. Important This is a important document. Treat the Federation Metadata XML file as you would a password.","title":"Configure Azure AD"},{"location":"enterprise/sso/ssh-azuread/#create-a-saml-connector","text":"Now, create a SAML connector resource . Replace the acs element with your Teleport address, update the group IDs with the actual AzureAD group ID values, and insert the downloaded Federation Metadata XML into the entity_descriptor resource. Write down this template as azure-connector.yaml : kind : saml version : v2 metadata : # the name of the connector name : azure-saml spec : display : \"Microsoft\" # acs is the Assertion Consumer Service URL. This should be the address of # the Teleport proxy that your identity provider will communicate with. acs : https://teleport.example.com:3080/v1/webapi/saml/acs attributes_to_roles : - { name : \"http://schemas.microsoft.com/ws/2008/06/identity/claims/groups\" , value : \"<group id 930210...>\" , roles : [ \"admin\" ]} - { name : \"http://schemas.microsoft.com/ws/2008/06/identity/claims/groups\" , value : \"<group id 93b110...>\" , roles : [ \"dev\" ]} entity_descriptor : | <federationmedata.xml contents> Create the connector using tctl tool: FYI Teleport will automatically transform the contents of the connector when viewed from the web UI.","title":"Create a SAML Connector"},{"location":"enterprise/sso/ssh-azuread/#create-teleport-roles","text":"We are going to create 2 roles: Privileged role admin who is able to login as root and is capable of administrating the cluster Non-privileged role dev kind : role version : v3 metadata : name : admin spec : options : max_session_ttl : 24h allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] Devs are only allowed to login to nodes labeled with access: relaxed Teleport label. Developers can log in as either ubuntu or a username that arrives in their assertions. Developers also do not have any rules needed to obtain admin access to Teleport. kind : role version : v3 metadata : name : dev spec : options : max_session_ttl : 24h allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Notice: Replace ubuntu with linux login available on your servers! $ tctl create admin.yaml $ tctl create dev.yaml","title":"Create Teleport Roles"},{"location":"enterprise/sso/ssh-azuread/#testing","text":"Update the Teleport settings to use the SAML settings to make this the default. auth_service : authentication : type : saml The Web UI will now contain a new button: \"Login with Microsoft\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple SAML connectors. In this case a connector name can be passed via tsh login --auth=connector_name","title":"Testing"},{"location":"enterprise/sso/ssh-azuread/#troubleshooting","text":"If you get \"access denied\" errors the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Example of a user being denied due as the role clusteradmin wasn't setup. { \"code\" : \"T1001W\" , \"error\" : \"role clusteradmin is not found\" , \"event\" : \"user.login\" , \"method\" : \"saml\" , \"success\" : false , \"time\" : \"2019-06-15T19:38:07Z\" , \"uid\" : \"cd9e45d0-b68c-43c3-87cf-73c4e0ec37e9\" } Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass the --debug flag to teleport start command.","title":"Troubleshooting"},{"location":"enterprise/sso/ssh-gsuite/","text":"SSH Authentication with G Suite (Google Apps) Google Apps as SSO for SSH This guide will cover how to configure G Suite to be a single sign-on (SSO) provider to issue SSH credentials to specific groups of users. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" Google group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. Version Warning This guide requires an enterprise version of Teleport 4.1.4 or greater. The open source edition of Teleport only supports Github as an SSO provider. Prerequisites: Before you get started you\u2019ll need: An Enterprise version of Teleport v4.1.4 or greater, downloaded from https://dashboard.gravitational.com/ . Be a G Suite Super Admin. As Google Best Practices, we would recommend setting up a seperate super admin with 2FA vs using your user. e.g. A dedicated account ben-ops@practice.io vs my daily ben@practice.io Ability to create GCP Project. This might require signing up to GCP, but for this project it won\u2019t require using any paid services. It\u2019s just a side effect of G Suite and GCP being closely related. Have a verified Domain . Ability to Setup G Suite Groups Configure G Suite Obtain OAuth 2.0 credentials https://developers.google.com/identity/protocols/OpenIDConnect Create a new Project. Select OAuth client ID. Make Application Type Public & Setup Domain Verification Copy OAuth Client ID and Client Secret for YAML Below. Note: The redirect_url: https://teleport.example.com:3080/v1/webapi/oidc/callback Create a Service Account Leave Service account users roles, and admin roles as blank. Leave Service account permissions as blank. Enable Account Delegation: Download Service Account JSON This JSON file will need to be uploaded to the Authentication server, and will be later referenced by the OIDC Connector, under google_service_account_uri . Note Teleport requires the service account JSON to be uploaded to all Teleport authentication servers when setting up in a HA config. Manage API Scopes: Before setting the Manage API client access capture the client ID of the service account. Within GSuite to access the Manage API client access go to Security -> Settings. Navigate to Advanced Settings and open Manage API client access. Put the client ID in the Client Name field and the below permissions in the API scopes as a single comma separated line. Press Authorize. Warning Do not use the email of the service account. The configuration display will look the same but the service account will not have the domain-wide delegation required. The client_id field must be the unique ID number captured from the admin UI. An indicator that this is misconfigured is if you see \"Client is unauthorized to retrieve access tokens using this method, or client not authorized for any of the scopes requested.\" in your log. Note The email that you set for google_admin_email must be the email address of a user that has permission to list all groups, users, and group membership in your G Suite account. This user will generally need super admin privileges. Client Name: For Client Name: Use the Unique ID for the service account. See Video for instructions . API Scopes: Copy these three API Scopes. https://www.googleapis.com/auth/admin.directory.group.member.readonly, https://www.googleapis.com/auth/admin.directory.group.readonly, https://www.googleapis.com/auth/admin.directory.user.readonly Create a OIDC Connector Now, create a OIDC connector resource . Write down this template as gsuite-connector.yaml : { !examples/resources/gsuite-connector.yaml! } Create the connector using tctl tool: $ tctl create gsuite-connector.yaml Create Teleport Roles We are going to create 2 roles: Privileged role admin who is able to login as root and is capable of administrating the cluster Non-privileged dev kind : role version : v3 metadata : name : admin spec : options : max_session_ttl : 24h allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] Devs are only allowed to login to nodes labelled with access: relaxed Teleport label. Developers can log in as either ubuntu or a username that arrives in their assertions. Developers also do not have any rules needed to obtain admin access to Teleport. kind : role version : v3 metadata : name : dev spec : options : max_session_ttl : 24h allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Notice: Replace ubuntu with linux login available on your servers! $ tctl create admin.yaml $ tctl create dev.yaml Testing The Web UI will now contain a new button: \"Login with GSuite\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple OIDC connectors. In this case a connector name can be passed via tsh login --auth=gsuite Troubleshooting If you get \"access denied\" errors the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Example of a user being denied due as the role clusteradmin wasn't setup. { \"code\" : \"T1001W\" , \"error\" : \"role clusteradmin is not found\" , \"event\" : \"user.login\" , \"method\" : \"oidc\" , \"success\" : false , \"time\" : \"2019-06-15T19:38:07Z\" , \"uid\" : \"cd9e45d0-b68c-43c3-87cf-73c4e0ec37e9\" } Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass the --debug flag to teleport start command.","title":"G Suite"},{"location":"enterprise/sso/ssh-gsuite/#ssh-authentication-with-g-suite-google-apps","text":"","title":"SSH Authentication with G Suite (Google Apps)"},{"location":"enterprise/sso/ssh-gsuite/#google-apps-as-sso-for-ssh","text":"This guide will cover how to configure G Suite to be a single sign-on (SSO) provider to issue SSH credentials to specific groups of users. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" Google group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. Version Warning This guide requires an enterprise version of Teleport 4.1.4 or greater. The open source edition of Teleport only supports Github as an SSO provider.","title":"Google Apps as SSO for SSH"},{"location":"enterprise/sso/ssh-gsuite/#prerequisites","text":"Before you get started you\u2019ll need: An Enterprise version of Teleport v4.1.4 or greater, downloaded from https://dashboard.gravitational.com/ . Be a G Suite Super Admin. As Google Best Practices, we would recommend setting up a seperate super admin with 2FA vs using your user. e.g. A dedicated account ben-ops@practice.io vs my daily ben@practice.io Ability to create GCP Project. This might require signing up to GCP, but for this project it won\u2019t require using any paid services. It\u2019s just a side effect of G Suite and GCP being closely related. Have a verified Domain . Ability to Setup G Suite Groups","title":"Prerequisites:"},{"location":"enterprise/sso/ssh-gsuite/#configure-g-suite","text":"Obtain OAuth 2.0 credentials https://developers.google.com/identity/protocols/OpenIDConnect Create a new Project. Select OAuth client ID. Make Application Type Public & Setup Domain Verification Copy OAuth Client ID and Client Secret for YAML Below. Note: The redirect_url: https://teleport.example.com:3080/v1/webapi/oidc/callback","title":"Configure G Suite"},{"location":"enterprise/sso/ssh-gsuite/#create-a-service-account","text":"Leave Service account users roles, and admin roles as blank. Leave Service account permissions as blank.","title":"Create a Service Account"},{"location":"enterprise/sso/ssh-gsuite/#enable-account-delegation","text":"","title":"Enable Account Delegation:"},{"location":"enterprise/sso/ssh-gsuite/#download-service-account-json","text":"This JSON file will need to be uploaded to the Authentication server, and will be later referenced by the OIDC Connector, under google_service_account_uri . Note Teleport requires the service account JSON to be uploaded to all Teleport authentication servers when setting up in a HA config.","title":"Download Service Account JSON"},{"location":"enterprise/sso/ssh-gsuite/#manage-api-scopes","text":"Before setting the Manage API client access capture the client ID of the service account. Within GSuite to access the Manage API client access go to Security -> Settings. Navigate to Advanced Settings and open Manage API client access. Put the client ID in the Client Name field and the below permissions in the API scopes as a single comma separated line. Press Authorize. Warning Do not use the email of the service account. The configuration display will look the same but the service account will not have the domain-wide delegation required. The client_id field must be the unique ID number captured from the admin UI. An indicator that this is misconfigured is if you see \"Client is unauthorized to retrieve access tokens using this method, or client not authorized for any of the scopes requested.\" in your log. Note The email that you set for google_admin_email must be the email address of a user that has permission to list all groups, users, and group membership in your G Suite account. This user will generally need super admin privileges. Client Name: For Client Name: Use the Unique ID for the service account. See Video for instructions . API Scopes: Copy these three API Scopes. https://www.googleapis.com/auth/admin.directory.group.member.readonly, https://www.googleapis.com/auth/admin.directory.group.readonly, https://www.googleapis.com/auth/admin.directory.user.readonly","title":"Manage API Scopes:"},{"location":"enterprise/sso/ssh-gsuite/#create-a-oidc-connector","text":"Now, create a OIDC connector resource . Write down this template as gsuite-connector.yaml : { !examples/resources/gsuite-connector.yaml! } Create the connector using tctl tool: $ tctl create gsuite-connector.yaml","title":"Create a OIDC Connector"},{"location":"enterprise/sso/ssh-gsuite/#create-teleport-roles","text":"We are going to create 2 roles: Privileged role admin who is able to login as root and is capable of administrating the cluster Non-privileged dev kind : role version : v3 metadata : name : admin spec : options : max_session_ttl : 24h allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] Devs are only allowed to login to nodes labelled with access: relaxed Teleport label. Developers can log in as either ubuntu or a username that arrives in their assertions. Developers also do not have any rules needed to obtain admin access to Teleport. kind : role version : v3 metadata : name : dev spec : options : max_session_ttl : 24h allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Notice: Replace ubuntu with linux login available on your servers! $ tctl create admin.yaml $ tctl create dev.yaml","title":"Create Teleport Roles"},{"location":"enterprise/sso/ssh-gsuite/#testing","text":"The Web UI will now contain a new button: \"Login with GSuite\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple OIDC connectors. In this case a connector name can be passed via tsh login --auth=gsuite","title":"Testing"},{"location":"enterprise/sso/ssh-gsuite/#troubleshooting","text":"If you get \"access denied\" errors the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Example of a user being denied due as the role clusteradmin wasn't setup. { \"code\" : \"T1001W\" , \"error\" : \"role clusteradmin is not found\" , \"event\" : \"user.login\" , \"method\" : \"oidc\" , \"success\" : false , \"time\" : \"2019-06-15T19:38:07Z\" , \"uid\" : \"cd9e45d0-b68c-43c3-87cf-73c4e0ec37e9\" } Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass the --debug flag to teleport start command.","title":"Troubleshooting"},{"location":"enterprise/sso/ssh-okta/","text":"SSH Authentication with Okta How to use Okta as a single sign-on (SSO) provider for SSH This guide will cover how to configure Okta to issue SSH credentials to specific groups of users. When used in combination with role based access control (RBAC), it allows SSH administrators to define policies like: Only members of \"DBA\" group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. Version Warning This guide requires a commercial edition of Teleport. The open source edition of Teleport only supports Github as an SSO provider. Enable SAML Authentication First, configure Teleport auth server to use SAML authentication instead of the local user database. Update /etc/teleport.yaml as shown below and restart the teleport daemon. auth_service : authentication : type : saml Configure Okta First, create a SAML 2.0 Web App in Okta configuration section 1. Switch to Classic UI 2. Select Create New App Select Web Platform and SAML 2.0 Sign On Method. Configure the App We are going to map the Okta groups we've created above to the SAML Attribute statements (special signed metadata exposed via a SAML XML response). GENERAL Single sign on URL https://teleport-proxy.example.com:3080/v1/webapi/saml/acs Audience URI (SP Entity ID) https://teleport-proxy.example.com:3080/v1/webapi/saml/acs Name ID format EmailAddress Application username Okta username GROUP ATTRIBUTE STATEMENTS Name: groups | Name format: Unspecified Filter: Matches regex | .* Note: RegEx requires .* tip Notice that we have set \"NameID\" to the email format and mapped the groups with a wildcard regex in the Group Attribute statements. We have also set the \"Audience\" and SSO URL to the same value. Create & Assign Groups Create Groups We are going to create two groups: \"okta-dev\" and \"okta-admin\": ...and the admin: Assign groups and people to your SAML app: Make sure to download the metadata in the form of an XML document. It will be used it to configure a Teleport connector: Create a SAML Connector Now, create a SAML connector resource : { !examples/resources/saml-connector.yaml! } Create the connector using tctl tool: $ tctl create okta-connector.yaml Create Teleport Roles We are going to create 2 roles, privileged role admin who is able to login as root and is capable of administrating the cluster and non-privileged dev. kind : role version : v3 metadata : name : admin spec : options : max_session_ttl : 24h allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] The developer role: kind : role version : v3 metadata : name : dev spec : options : max_session_ttl : 24h allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Devs are only allowed to login to nodes labelled with access: relaxed label. Developers can log in as ubuntu user Notice {% raw %}{{external.username}}{% endraw %} login. It configures Teleport to look at \"username\" Okta claim and use that field as an allowed login for each user. Developers also do not have any \"allow rules\" i.e. they will not be able to see/replay past sessions or re-configure the Teleport cluster. Now, create both roles on the auth server: $ tctl create admin.yaml $ tctl create dev.yaml Testing The Web UI will now contain a new button: \"Login with Okta\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple SAML connectors. In this case a connector name can be passed via tsh login --auth=connector_name IMPORTANT Teleport only supports sending party initiated flows for SAML 2.0. This means you can not initiate login from your identity provider, you have to initiate login from either the Teleport Web UI or CLI. Troubleshooting If you get \"access denied errors\" the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass --debug flag to teleport start command.","title":"Okta"},{"location":"enterprise/sso/ssh-okta/#ssh-authentication-with-okta","text":"","title":"SSH Authentication with Okta"},{"location":"enterprise/sso/ssh-okta/#how-to-use-okta-as-a-single-sign-on-sso-provider-for-ssh","text":"This guide will cover how to configure Okta to issue SSH credentials to specific groups of users. When used in combination with role based access control (RBAC), it allows SSH administrators to define policies like: Only members of \"DBA\" group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. Version Warning This guide requires a commercial edition of Teleport. The open source edition of Teleport only supports Github as an SSO provider.","title":"How to use Okta as a single sign-on (SSO) provider for SSH"},{"location":"enterprise/sso/ssh-okta/#enable-saml-authentication","text":"First, configure Teleport auth server to use SAML authentication instead of the local user database. Update /etc/teleport.yaml as shown below and restart the teleport daemon. auth_service : authentication : type : saml","title":"Enable SAML Authentication"},{"location":"enterprise/sso/ssh-okta/#configure-okta","text":"First, create a SAML 2.0 Web App in Okta configuration section","title":"Configure Okta"},{"location":"enterprise/sso/ssh-okta/#configure-the-app","text":"We are going to map the Okta groups we've created above to the SAML Attribute statements (special signed metadata exposed via a SAML XML response). GENERAL Single sign on URL https://teleport-proxy.example.com:3080/v1/webapi/saml/acs Audience URI (SP Entity ID) https://teleport-proxy.example.com:3080/v1/webapi/saml/acs Name ID format EmailAddress Application username Okta username GROUP ATTRIBUTE STATEMENTS Name: groups | Name format: Unspecified Filter: Matches regex | .*","title":"Configure the App"},{"location":"enterprise/sso/ssh-okta/#create-assign-groups","text":"Create Groups We are going to create two groups: \"okta-dev\" and \"okta-admin\": ...and the admin: Assign groups and people to your SAML app: Make sure to download the metadata in the form of an XML document. It will be used it to configure a Teleport connector:","title":"Create &amp; Assign Groups"},{"location":"enterprise/sso/ssh-okta/#create-a-saml-connector","text":"Now, create a SAML connector resource : { !examples/resources/saml-connector.yaml! } Create the connector using tctl tool: $ tctl create okta-connector.yaml","title":"Create a SAML Connector"},{"location":"enterprise/sso/ssh-okta/#create-teleport-roles","text":"We are going to create 2 roles, privileged role admin who is able to login as root and is capable of administrating the cluster and non-privileged dev. kind : role version : v3 metadata : name : admin spec : options : max_session_ttl : 24h allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] The developer role: kind : role version : v3 metadata : name : dev spec : options : max_session_ttl : 24h allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Devs are only allowed to login to nodes labelled with access: relaxed label. Developers can log in as ubuntu user Notice {% raw %}{{external.username}}{% endraw %} login. It configures Teleport to look at \"username\" Okta claim and use that field as an allowed login for each user. Developers also do not have any \"allow rules\" i.e. they will not be able to see/replay past sessions or re-configure the Teleport cluster. Now, create both roles on the auth server: $ tctl create admin.yaml $ tctl create dev.yaml","title":"Create Teleport Roles"},{"location":"enterprise/sso/ssh-okta/#testing","text":"The Web UI will now contain a new button: \"Login with Okta\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple SAML connectors. In this case a connector name can be passed via tsh login --auth=connector_name IMPORTANT Teleport only supports sending party initiated flows for SAML 2.0. This means you can not initiate login from your identity provider, you have to initiate login from either the Teleport Web UI or CLI.","title":"Testing"},{"location":"enterprise/sso/ssh-okta/#troubleshooting","text":"If you get \"access denied errors\" the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass --debug flag to teleport start command.","title":"Troubleshooting"},{"location":"enterprise/sso/ssh-one-login/","text":"SSH Authentication with OneLogin Using OneLogin as a single sign-on (SSO) provider for SSH This guide will cover how to configure OneLogin to issue SSH credentials to specific groups of users. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. Version Warning This guide requires an Enterprise edition of Teleport. The community edition of Teleport only supports Github as an SSO provider. Enable SAML Authentication Configure Teleport auth server to use SAML authentication instead of the local user database. Update /etc/teleport.yaml as shown below and restart the teleport daemon. auth_service : authentication : type : saml Configure Application Using OneLogin control panel, create a SAML 2.0 Web App in SAML configuration section: Download Icons Square Icon Rectangular Icon Important Make sure to pick SAML Test Connector (SP) and not SAML Test Connector (IdP) , because teleport only supports SP - service provider initiated SAML flows. Set Audience , Recipient and ACS (Consumer) URL Validator to the same value: https://teleport.example.com/v1/webapi/saml/acs where teleport.example.com is the public name of the teleport web proxy service: Teleport needs to assign groups to users. Configure the application with some parameters exposed as SAML attribute statements: Important Make sure to check Include in SAML assertion checkbox. Add users to the application: Download SAML XML Metadata Once the application is set up, download SAML Metadata . Create a SAML Connector Now, create a SAML connector resource . Write down this template as onelogin-connector.yaml : { !examples/resources/onelogin-connector.yaml! } To fill in the fields, open SSO tab: acs - is the name of the teleport web proxy, e.g. https://teleport.example.com/v1/webapi/saml/acs issuer - use value from Issuer URL field , e.g. https://app.onelogin.com/saml/metadata/123456 sso - use the value from the value from field SAML 2.0 Endpoint (HTTP) but replace http-post with http-redirect , e.g. https://mycompany.onelogin.com/trust/saml2/http-redirect/sso/123456 Important Make sure to replace http-post with http-redirect . cert - download certificate, by clicking \"view details link\" and add to cert section Create the connector using tctl tool: $ tctl create onelogin-connector.yaml Create Teleport Roles We are going to create 2 roles, privileged role admin who is able to login as root and is capable of administrating the cluster and non-privileged dev. kind : role version : v3 metadata : name : admin spec : options : max_session_ttl : 24h allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] Devs are only allowed to login to nodes labelled with access: relaxed Teleport label. Developers can log in as either ubuntu to a username that arrives in their assertions. Developers also do not have any rules needed to obtain admin access to Teleport. kind : role version : v3 metadata : name : dev spec : options : max_session_ttl : \"24h\" allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Notice: Replace ubuntu with linux login available on your servers! $ tctl create admin.yaml $ tctl create dev.yaml Testing The Web UI will now contain a new button: \"Login with OneLogin\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple SAML connectors. In this case a connector name can be passed via tsh login --auth=connector_name IMPORTANT Teleport only supports sending party initiated flows for SAML 2.0. This means you can not initiate login from your identity provider, you have to initiate login from either the Teleport Web UI or CLI. Troubleshooting If you get \"access denied errors\" the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass --debug flag to teleport start command.","title":"OneLogin"},{"location":"enterprise/sso/ssh-one-login/#ssh-authentication-with-onelogin","text":"","title":"SSH Authentication with OneLogin"},{"location":"enterprise/sso/ssh-one-login/#using-onelogin-as-a-single-sign-on-sso-provider-for-ssh","text":"This guide will cover how to configure OneLogin to issue SSH credentials to specific groups of users. When used in combination with role based access control (RBAC) it allows SSH administrators to define policies like: Only members of \"DBA\" group can SSH into machines running PostgreSQL. Developers must never SSH into production servers. ... and many others. Version Warning This guide requires an Enterprise edition of Teleport. The community edition of Teleport only supports Github as an SSO provider.","title":"Using OneLogin as a single sign-on (SSO) provider for SSH"},{"location":"enterprise/sso/ssh-one-login/#enable-saml-authentication","text":"Configure Teleport auth server to use SAML authentication instead of the local user database. Update /etc/teleport.yaml as shown below and restart the teleport daemon. auth_service : authentication : type : saml","title":"Enable SAML Authentication"},{"location":"enterprise/sso/ssh-one-login/#configure-application","text":"Using OneLogin control panel, create a SAML 2.0 Web App in SAML configuration section:","title":"Configure Application"},{"location":"enterprise/sso/ssh-one-login/#download-icons","text":"Square Icon Rectangular Icon Important Make sure to pick SAML Test Connector (SP) and not SAML Test Connector (IdP) , because teleport only supports SP - service provider initiated SAML flows. Set Audience , Recipient and ACS (Consumer) URL Validator to the same value: https://teleport.example.com/v1/webapi/saml/acs where teleport.example.com is the public name of the teleport web proxy service: Teleport needs to assign groups to users. Configure the application with some parameters exposed as SAML attribute statements: Important Make sure to check Include in SAML assertion checkbox. Add users to the application:","title":"Download Icons"},{"location":"enterprise/sso/ssh-one-login/#download-saml-xml-metadata","text":"Once the application is set up, download SAML Metadata .","title":"Download SAML XML Metadata"},{"location":"enterprise/sso/ssh-one-login/#create-a-saml-connector","text":"Now, create a SAML connector resource . Write down this template as onelogin-connector.yaml : { !examples/resources/onelogin-connector.yaml! } To fill in the fields, open SSO tab: acs - is the name of the teleport web proxy, e.g. https://teleport.example.com/v1/webapi/saml/acs issuer - use value from Issuer URL field , e.g. https://app.onelogin.com/saml/metadata/123456 sso - use the value from the value from field SAML 2.0 Endpoint (HTTP) but replace http-post with http-redirect , e.g. https://mycompany.onelogin.com/trust/saml2/http-redirect/sso/123456 Important Make sure to replace http-post with http-redirect . cert - download certificate, by clicking \"view details link\" and add to cert section Create the connector using tctl tool: $ tctl create onelogin-connector.yaml","title":"Create a SAML Connector"},{"location":"enterprise/sso/ssh-one-login/#create-teleport-roles","text":"We are going to create 2 roles, privileged role admin who is able to login as root and is capable of administrating the cluster and non-privileged dev. kind : role version : v3 metadata : name : admin spec : options : max_session_ttl : 24h allow : logins : [ root ] node_labels : \"*\" : \"*\" rules : - resources : [ \"*\" ] verbs : [ \"*\" ] Devs are only allowed to login to nodes labelled with access: relaxed Teleport label. Developers can log in as either ubuntu to a username that arrives in their assertions. Developers also do not have any rules needed to obtain admin access to Teleport. kind : role version : v3 metadata : name : dev spec : options : max_session_ttl : \"24h\" allow : logins : [ \"{% raw %}{{external.username}}{% endraw %}\" , ubuntu ] node_labels : access : relaxed Notice: Replace ubuntu with linux login available on your servers! $ tctl create admin.yaml $ tctl create dev.yaml","title":"Create Teleport Roles"},{"location":"enterprise/sso/ssh-one-login/#testing","text":"The Web UI will now contain a new button: \"Login with OneLogin\". The CLI is the same as before: $ tsh --proxy=proxy.example.com login This command will print the SSO login URL (and will try to open it automatically in a browser). Tip Teleport can use multiple SAML connectors. In this case a connector name can be passed via tsh login --auth=connector_name IMPORTANT Teleport only supports sending party initiated flows for SAML 2.0. This means you can not initiate login from your identity provider, you have to initiate login from either the Teleport Web UI or CLI.","title":"Testing"},{"location":"enterprise/sso/ssh-one-login/#troubleshooting","text":"If you get \"access denied errors\" the number one place to check is the audit log on the Teleport auth server. It is located in /var/lib/teleport/log by default and it will contain the detailed reason why a user's login was denied. Some errors (like filesystem permissions or misconfigured network) can be diagnosed using Teleport's stderr log, which is usually available via: $ sudo journalctl -fu teleport If you wish to increase the verbosity of Teleport's syslog, you can pass --debug flag to teleport start command.","title":"Troubleshooting"},{"location":"enterprise/sso/ssh-sso/","text":"Single Sign-On (SSO) for SSH The commercial edition of Teleport allows users to login and retrieve their SSH credentials through a Single Sign-On (SSO) system used by the rest of the organization. Examples of supported SSO systems include commercial solutions like Okta , Auth0 , SailPoint , OneLogin or Active Directory , as well as open source products like Keycloak . Other identity management systems are supported as long as they provide an SSO mechanism based on either SAML or OAuth2/OpenID Connect . SSO Setup Guides Azure Active Directory (AD) Active Directory (ADFS) G Suite OneLogin OIDC Okta How does SSO work with SSH? From the user's perspective they need to execute the following command login: # this command will automatically open the default web browser and take a user # through the login process with an SSO provider: $ tsh login --proxy=proxy.example.com # output: If browser window does not open automatically, open it by clicking on the link: http://127.0.0.1:45235/055a310a-1099-43ea-8cf6-ffc41d88ad1f Teleport will wait for up to 3 minutes for a user to authenticate. If authentication succeeds, Teleport will retrieve an SSH certificate and will store it in ~/.tsh/keys/proxy.example.com directory and also will add it to an SSH agent if there's one running. Configuring SSO Teleport works with SSO providers by relying on a concept called \"authentication connector\" . An auth connector is a plugin which controls how a user logs in and which group he or she belongs to. The following connectors are supported: local connector type uses the built-in user database. This database can be manipulated by the tctl users command. saml connector type uses the SAML protocol to authenticate users and query their group membership. oidc connector type uses the OpenID Connect protocol to authenticate users and query their group membership. To configure SSO , a Teleport administrator must: Update /etc/teleport.yaml on the auth server to set the default authentication connector. Define the connector resource and save it into a YAML file (like connector.yaml ) Create the connector using tctl create connector.yaml . # snippet from /etc/teleport.yaml on the auth server: auth_service : # defines the default authentication connector type: authentication : type : saml An example of a connector: # connector.yaml kind : saml version : v2 metadata : name : corporate spec : # display allows to set the caption of the \"login\" button # in the Web interface display : \"Okta\" acs : https://teleport-proxy.example.com:3080/v1/webapi/saml/acs attributes_to_roles : - { name : \"groups\" , value : \"okta-admin\" , roles : [ \"admin\" ]} - { name : \"groups\" , value : \"okta-dev\" , roles : [ \"dev\" ]} # note that wildcards can also be used. the next line instructs Teleport # to assign \"admin\" role to any user who has the SAML attribute that begins with \"admin\": - { name : \"group\" , value : \"admin*\" , roles : [ \"admin\" ] } # regular expressions with capture are also supported. the next line instructs Teleport # to assign users to roles `admin-1` if his SAML \"group\" attribute equals 'ssh_admin_1': - { name : \"group\" , value : \"^ssh_admin_(.*)$\" , roles : [ \"admin-$1\" ] } entity_descriptor : | <paste SAML XML contents here> See examples/resources directory in the Teleport Github repository for examples of possible connectors. You may use entity_descriptor_url , in lieu of entity_descriptor , to fetch the entity descriptor from your IDP. Though, we recommend \"pinning\" the entity descriptor by including the XML rather than fetching from a URL. User Logins Often it is required to restrict SSO users to their unique UNIX logins when they connect to Teleport nodes. To support this: Use the SSO provider to create a field called \"unix_login\" (you can use another name). Make sure it's exposed as a claim via SAML/OIDC. Update a Teleport SSH role to include {% raw %}{{external.unix_login}}{% endraw %} variable into the list of allowed logins: kind : role version : v3 metadata : name : sso_user spec : allow : logins : - '{% raw %}{{external.unix_login}}{% endraw %}' node_labels : '*' : '*' Working with External Email Identity Along with sending groups, an SSO provider will also provide a user's email address. In many organizations, the username that a person uses to log into a system is the same as the first part of their email address - the 'local' part. For example, dave.smith@acme.com might log in with the username dave.smith . Teleport 4.2.6+ adds an easy way to extract the first part of an email address so it can be used as a username - this is the {% raw %}{{email.local}}{% endraw %} function. If the email claim from the identity provider (which can be accessed via {% raw %}{{external.email}}{% endraw %} ) is sent and contains an email address, you can extract the 'local' part of the email address before the @ sign like this: {% raw %}{{email.local(external.email)}}{% endraw %} Here's how this looks in a Teleport role: kind : role version : v3 metadata : name : sso_user spec : allow : logins : # Extracts the local part of dave.smith@acme.com, so the login will # now support dave.smith. - '{% raw %}{{email.local(external.email)}}{% endraw %}' node_labels : '*' : '*' Multiple SSO Providers Teleport can also support multiple connectors, i.e. a Teleport administrator can define and create multiple connector resources using tctl create as shown above. To see all configured connectors, execute this on the auth server: $ tctl get connectors To delete/update connectors, use the usual tctl rm and tctl create commands as described in the Resources section in the Admin Manual. If multiple authentication connectors exist, the clients must supply a connector name to tsh login via --auth argument: # use \"okta\" SAML connector: $ tsh --proxy=proxy.example.com login --auth=okta # use local Teleport user DB: $ tsh --proxy=proxy.example.com login --auth=local --user=admin Refer to the following guides to configure authentication connectors of both SAML and OIDC types: SSH Authentication with Okta SSH Authentication with OneLogin SSH Authentication with ADFS SSH Authentication with OAuth2 / OpenID Connect SSO Customization Provider YAML Example Github display: Github Microsoft display: Microsoft Google display: Google BitBucket display: Bitbucket OpenID display: Okta Troubleshooting Troubleshooting SSO configuration can be challenging. Usually a Teleport administrator must be able to: Ensure that HTTP/TLS certificates are configured properly for both Teleport proxy and the SSO provider. Be able to see what SAML/OIDC claims and values are getting exported and passed by the SSO provider to Teleport. Be able to see how Teleport maps the received claims to role mappings as defined in the connector. If something is not working, we recommend to: Double-check the host names, tokens and TCP ports in a connector definition. Look into Teleport's audit log for claim mapping problems. It is usually stored on the auth server in the /var/lib/teleport/log directory.ad","title":"SSO"},{"location":"enterprise/sso/ssh-sso/#single-sign-on-sso-for-ssh","text":"The commercial edition of Teleport allows users to login and retrieve their SSH credentials through a Single Sign-On (SSO) system used by the rest of the organization. Examples of supported SSO systems include commercial solutions like Okta , Auth0 , SailPoint , OneLogin or Active Directory , as well as open source products like Keycloak . Other identity management systems are supported as long as they provide an SSO mechanism based on either SAML or OAuth2/OpenID Connect .","title":"Single Sign-On (SSO) for SSH"},{"location":"enterprise/sso/ssh-sso/#sso-setup-guides","text":"Azure Active Directory (AD) Active Directory (ADFS) G Suite OneLogin OIDC Okta","title":"SSO Setup Guides"},{"location":"enterprise/sso/ssh-sso/#how-does-sso-work-with-ssh","text":"From the user's perspective they need to execute the following command login: # this command will automatically open the default web browser and take a user # through the login process with an SSO provider: $ tsh login --proxy=proxy.example.com # output: If browser window does not open automatically, open it by clicking on the link: http://127.0.0.1:45235/055a310a-1099-43ea-8cf6-ffc41d88ad1f Teleport will wait for up to 3 minutes for a user to authenticate. If authentication succeeds, Teleport will retrieve an SSH certificate and will store it in ~/.tsh/keys/proxy.example.com directory and also will add it to an SSH agent if there's one running.","title":"How does SSO work with SSH?"},{"location":"enterprise/sso/ssh-sso/#configuring-sso","text":"Teleport works with SSO providers by relying on a concept called \"authentication connector\" . An auth connector is a plugin which controls how a user logs in and which group he or she belongs to. The following connectors are supported: local connector type uses the built-in user database. This database can be manipulated by the tctl users command. saml connector type uses the SAML protocol to authenticate users and query their group membership. oidc connector type uses the OpenID Connect protocol to authenticate users and query their group membership. To configure SSO , a Teleport administrator must: Update /etc/teleport.yaml on the auth server to set the default authentication connector. Define the connector resource and save it into a YAML file (like connector.yaml ) Create the connector using tctl create connector.yaml . # snippet from /etc/teleport.yaml on the auth server: auth_service : # defines the default authentication connector type: authentication : type : saml An example of a connector: # connector.yaml kind : saml version : v2 metadata : name : corporate spec : # display allows to set the caption of the \"login\" button # in the Web interface display : \"Okta\" acs : https://teleport-proxy.example.com:3080/v1/webapi/saml/acs attributes_to_roles : - { name : \"groups\" , value : \"okta-admin\" , roles : [ \"admin\" ]} - { name : \"groups\" , value : \"okta-dev\" , roles : [ \"dev\" ]} # note that wildcards can also be used. the next line instructs Teleport # to assign \"admin\" role to any user who has the SAML attribute that begins with \"admin\": - { name : \"group\" , value : \"admin*\" , roles : [ \"admin\" ] } # regular expressions with capture are also supported. the next line instructs Teleport # to assign users to roles `admin-1` if his SAML \"group\" attribute equals 'ssh_admin_1': - { name : \"group\" , value : \"^ssh_admin_(.*)$\" , roles : [ \"admin-$1\" ] } entity_descriptor : | <paste SAML XML contents here> See examples/resources directory in the Teleport Github repository for examples of possible connectors. You may use entity_descriptor_url , in lieu of entity_descriptor , to fetch the entity descriptor from your IDP. Though, we recommend \"pinning\" the entity descriptor by including the XML rather than fetching from a URL.","title":"Configuring SSO"},{"location":"enterprise/sso/ssh-sso/#user-logins","text":"Often it is required to restrict SSO users to their unique UNIX logins when they connect to Teleport nodes. To support this: Use the SSO provider to create a field called \"unix_login\" (you can use another name). Make sure it's exposed as a claim via SAML/OIDC. Update a Teleport SSH role to include {% raw %}{{external.unix_login}}{% endraw %} variable into the list of allowed logins: kind : role version : v3 metadata : name : sso_user spec : allow : logins : - '{% raw %}{{external.unix_login}}{% endraw %}' node_labels : '*' : '*'","title":"User Logins"},{"location":"enterprise/sso/ssh-sso/#working-with-external-email-identity","text":"Along with sending groups, an SSO provider will also provide a user's email address. In many organizations, the username that a person uses to log into a system is the same as the first part of their email address - the 'local' part. For example, dave.smith@acme.com might log in with the username dave.smith . Teleport 4.2.6+ adds an easy way to extract the first part of an email address so it can be used as a username - this is the {% raw %}{{email.local}}{% endraw %} function. If the email claim from the identity provider (which can be accessed via {% raw %}{{external.email}}{% endraw %} ) is sent and contains an email address, you can extract the 'local' part of the email address before the @ sign like this: {% raw %}{{email.local(external.email)}}{% endraw %} Here's how this looks in a Teleport role: kind : role version : v3 metadata : name : sso_user spec : allow : logins : # Extracts the local part of dave.smith@acme.com, so the login will # now support dave.smith. - '{% raw %}{{email.local(external.email)}}{% endraw %}' node_labels : '*' : '*'","title":"Working with External Email Identity"},{"location":"enterprise/sso/ssh-sso/#multiple-sso-providers","text":"Teleport can also support multiple connectors, i.e. a Teleport administrator can define and create multiple connector resources using tctl create as shown above. To see all configured connectors, execute this on the auth server: $ tctl get connectors To delete/update connectors, use the usual tctl rm and tctl create commands as described in the Resources section in the Admin Manual. If multiple authentication connectors exist, the clients must supply a connector name to tsh login via --auth argument: # use \"okta\" SAML connector: $ tsh --proxy=proxy.example.com login --auth=okta # use local Teleport user DB: $ tsh --proxy=proxy.example.com login --auth=local --user=admin Refer to the following guides to configure authentication connectors of both SAML and OIDC types: SSH Authentication with Okta SSH Authentication with OneLogin SSH Authentication with ADFS SSH Authentication with OAuth2 / OpenID Connect","title":"Multiple SSO Providers"},{"location":"enterprise/sso/ssh-sso/#sso-customization","text":"Provider YAML Example Github display: Github Microsoft display: Microsoft Google display: Google BitBucket display: Bitbucket OpenID display: Okta","title":"SSO Customization"},{"location":"enterprise/sso/ssh-sso/#troubleshooting","text":"Troubleshooting SSO configuration can be challenging. Usually a Teleport administrator must be able to: Ensure that HTTP/TLS certificates are configured properly for both Teleport proxy and the SSO provider. Be able to see what SAML/OIDC claims and values are getting exported and passed by the SSO provider to Teleport. Be able to see how Teleport maps the received claims to role mappings as defined in the connector. If something is not working, we recommend to: Double-check the host names, tokens and TCP ports in a connector definition. Look into Teleport's audit log for claim mapping problems. It is usually stored on the auth server in the /var/lib/teleport/log directory.ad","title":"Troubleshooting"},{"location":"enterprise/workflow/","text":"Teleport Approval Workflows Approving Workflow using an External Integration Integrating Teleport with Slack Integrating Teleport with Mattermost Integrating Teleport with Jira Cloud Integrating Teleport with Jira Server Integrating Teleport with PagerDuty Approval Workflows Setup Teleport 4.2 introduced the ability for users to request additional roles. The workflow API makes it easy to dynamically approve or deny these requests. Setup Contractor Role This role allows the contractor to request the role DBA. kind : role metadata : name : contractor spec : options : # ... allow : request : roles : [ 'dba' ] # ... deny : # ... DBA Role This role allows the contractor to request the role DBA. kind : role metadata : name : dba spec : options : # ... # Only allows the contractor to use this role for 1 hour from time of request. max_session_ttl : 1h allow : # ... deny : # ... Admin Role This role allows the admin to approve the contractor's request. kind : role metadata : name : admin spec : options : # ... allow : # ... deny : # ... # list of allow-rules, see # https://gravitational.com/teleport/docs/enterprise/ssh-rbac/ rules : # Access Request is part of Approval Workflows introduced in 4.2 # `access_request` should only be given to Teleport Admins. - resources : [ access_request ] verbs : [ list , read , update , delete ] $ tsh login teleport-cluster --request-roles = dba Seeking request approval... ( id: bc8ca931-fec9-4b15-9a6f-20c13c5641a9 ) As a Teleport Administrator: $ tctl request ls Token Requestor Metadata Created At ( UTC ) Status ------------------------------------ --------- -------------- ------------------- ------- bc8ca931-fec9-4b15-9a6f-20c13c5641a9 alice roles = dba 07 Nov 19 19 :38 UTC PENDING $ tctl request approve bc8ca931-fec9-4b15-9a6f-20c13c5641a9 Assuming approval, tsh will automatically manage a certificate re-issued with the newly requested roles applied. In this case contractor will now have have the permission of the dba . Warning Granting a role with administrative abilities could allow a user to permanently upgrade their privileges (e.g. if contractor was granted admin for some reason). We recommend only escalating to the next role of least privilege vs jumping directly to \"Super Admin\" role. The deny.request block can help mitigate the risk of doing this by accident. See Example Below. # Example role that explicitly denies a contractor from requesting the admin # role. kind : role metadata : name : contractor spec : options : # ... allow : # ... deny : request : roles : [ 'admin' ] Other features of Approval Workflows. Users can request multiple roles at one time. e.g roles: ['dba','netsec','cluster-x'] Approved requests have no effect on Teleport's behavior outside of allowing additional roles on re-issue. This has the nice effect of making requests \"compatible\" with older versions of Teleport, since only the issuing Auth Server needs any particular knowledge of the feature. Integrating with an External Tool Integration Feature Type Setup Instructions Slack Chatbot Setup Slack Mattermost Chatbot Setup Mattermost Jira Server Project Board Setup Jira Server Jira Cloud Project Board Setup Jira Cloud PagerDuty Schedule Setup PagerDuty","title":"Approval Workflow"},{"location":"enterprise/workflow/#teleport-approval-workflows","text":"","title":"Teleport Approval Workflows"},{"location":"enterprise/workflow/#approval-workflows-setup","text":"Teleport 4.2 introduced the ability for users to request additional roles. The workflow API makes it easy to dynamically approve or deny these requests.","title":"Approval Workflows Setup"},{"location":"enterprise/workflow/#setup","text":"Contractor Role This role allows the contractor to request the role DBA. kind : role metadata : name : contractor spec : options : # ... allow : request : roles : [ 'dba' ] # ... deny : # ... DBA Role This role allows the contractor to request the role DBA. kind : role metadata : name : dba spec : options : # ... # Only allows the contractor to use this role for 1 hour from time of request. max_session_ttl : 1h allow : # ... deny : # ... Admin Role This role allows the admin to approve the contractor's request. kind : role metadata : name : admin spec : options : # ... allow : # ... deny : # ... # list of allow-rules, see # https://gravitational.com/teleport/docs/enterprise/ssh-rbac/ rules : # Access Request is part of Approval Workflows introduced in 4.2 # `access_request` should only be given to Teleport Admins. - resources : [ access_request ] verbs : [ list , read , update , delete ] $ tsh login teleport-cluster --request-roles = dba Seeking request approval... ( id: bc8ca931-fec9-4b15-9a6f-20c13c5641a9 ) As a Teleport Administrator: $ tctl request ls Token Requestor Metadata Created At ( UTC ) Status ------------------------------------ --------- -------------- ------------------- ------- bc8ca931-fec9-4b15-9a6f-20c13c5641a9 alice roles = dba 07 Nov 19 19 :38 UTC PENDING $ tctl request approve bc8ca931-fec9-4b15-9a6f-20c13c5641a9 Assuming approval, tsh will automatically manage a certificate re-issued with the newly requested roles applied. In this case contractor will now have have the permission of the dba . Warning Granting a role with administrative abilities could allow a user to permanently upgrade their privileges (e.g. if contractor was granted admin for some reason). We recommend only escalating to the next role of least privilege vs jumping directly to \"Super Admin\" role. The deny.request block can help mitigate the risk of doing this by accident. See Example Below. # Example role that explicitly denies a contractor from requesting the admin # role. kind : role metadata : name : contractor spec : options : # ... allow : # ... deny : request : roles : [ 'admin' ]","title":"Setup"},{"location":"enterprise/workflow/#other-features-of-approval-workflows","text":"Users can request multiple roles at one time. e.g roles: ['dba','netsec','cluster-x'] Approved requests have no effect on Teleport's behavior outside of allowing additional roles on re-issue. This has the nice effect of making requests \"compatible\" with older versions of Teleport, since only the issuing Auth Server needs any particular knowledge of the feature.","title":"Other features of Approval Workflows."},{"location":"enterprise/workflow/#integrating-with-an-external-tool","text":"Integration Feature Type Setup Instructions Slack Chatbot Setup Slack Mattermost Chatbot Setup Mattermost Jira Server Project Board Setup Jira Server Jira Cloud Project Board Setup Jira Cloud PagerDuty Schedule Setup PagerDuty","title":"Integrating with an External Tool"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/","text":"SSH login approvals using Jira Teleport Jira Plugin Setup This guide will talk through how to setup Teleport with Jira. Teleport to Jira integration allows you to treat Teleport access and permission requests using Jira tickets. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles. Setup This guide assumes that you have: Teleport Enterprise 4.2.8 or newer Admin privileges with access to tctl Jira Server or Jira Cloud installation with an owner privileges, specifically to setup webhooks, issue types, and workflows. Create an access-plugin role and user within Teleport First off, using an existing Teleport Cluster, we are going to create a new Teleport User and Role to access Teleport. Create User and Role for access. Log into Teleport Authentication Server, this is where you normally run tctl . Create a new user and role that only has API access to the access_request API. The below script will create a yaml resource file for a new user and role. $ cat > rscs.yaml <<EOF kind: user metadata: name: access-plugin-jira spec: roles: ['access-plugin-jira'] version: v2 --- kind: role metadata: name: access-plugin-jira spec: allow: rules: - resources: ['access_request'] verbs: ['list','read','update'] # teleport currently refuses to issue certs for a user with 0 logins, # this restriction may be lifted in future versions. logins: ['access-plugin-jira'] version: v3 EOF # ... $ tctl create -f rscs.yaml Export access-plugin Certificate Teleport Plugin use the access-plugin-jira role and user to perform the approval. We export the identity files, using tctl auth sign . $ tctl auth sign --format = tls --user = access-plugin --out = auth --ttl = 8760h # ... The above sequence should result in three PEM encoded files being generated: auth.crt, auth.key, and auth.cas (certificate, private key, and CA certs respectively). We'll reference the auth.crt, auth.key, and auth.cas files later when configuring the plugins . Certificate Lifetime By default, tctl auth sign produces certificates with a relatively short lifetime. For production deployments, the --ttl flag can be used to ensure a more practical certificate lifetime. --ttl=8760h exports a 1 year token Setting up your Jira Project Creating the permission management project All new permission requests are going to show up in a project you choose. We recommend that you create a separate project for permissions management, and a new board in said project. You'll need the project Jira key to configure the plugin. Setting up the status board Create a new board for tasks in the permission management project. The board has to have at least these three columns: Pending Approved Denied Teleport Jira Plugin will create a new issue for each new permission request in the first available column on the board. When you drag the request task to Approved column on Jira, the request will be approved. If you drag the request task to the Denied column in Jira, the request will be denied. Setting up Request ID field on Jira Teleport Jira Plugin requires a custom issue field to be created. Go to your Jira Project settings \u2192 Issue Types \u2192 Select type Task \u2192 add a new Short Text field named TeleportAccessRequestId . Teleport uses this field to reference its internal request ID. If anyone changes this field on Jira, or tries to forge the permission request, Teleport will validate it and ignore it. Getting your Jira API token If you're using Jira Cloud, navigate to Account Settings \u2192 Security \u2192 API Tokens and create a new app specific API token in your Jira installation. You'll need this token later to configure the plugin. For Jira Server, the URL of the API tokens page will be different depending on your installation. Setting up Jira Webhooks Go to Settings \u2192 General \u2192 System \u2192 Webhooks and create a new Webhook for Jira to tell the Teleport Plugin about updates. For the webhook URL, use the URL that you'll run the plugin on. It needs to be a publicly accessible URL that we'll set up later. Jira requires the webhook listener to run over HTTPS. The Teleport Jira plugin webhook needs to be notified only about new issues being created, issues being updated, or deleted. You can leave all the other boxes empty. Plugin Defaults Jira Webhook will send updates about any issues in any projects in your Jira installation to the webhook. We suggest that you use JQL filters to limit which issues are being sent to the plugin. The Plugin's web server will run with TLS, but you can disable it with --insecure-no-tls to test things out in a dev environment. In the webhook settings page, make sure that the webhook will only send Issue Updated updates. It's not critical if anything else gets sent, the plugin will just ignore everything else. Installing We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-jira-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-jira-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-jira/ $ ./install $ which teleport-jira /usr/local/bin/teleport-jira Run ./install in from 'teleport-jira' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation. Configuration file Teleport Jira Plugin uses a config file in TOML format. Generate a boilerplate config by running the following command: $ teleport-jira configure > teleport-jira.toml $ sudo mv teleport-jira.toml /etc By default, Jira Teleport Plugin will use a config in /etc/teleport-jira.toml , and you can override it with -c config/file/path.toml flag. { ! examples / resources / plugins / teleport-jira . toml ! } The [teleport] section describes where the teleport service running, and what keys should the plugin use to authenticate itself. Use the keys that you've generated. The [jira] section requires a few things: Your Jira Cloud or Jira Server URL. For Jira Cloud, it looks something like yourcompany.atlassian.net . Your username on Jira, i.e. ben@gravitational.com Your Jira API token that you've created above. A Jira Project key, available in Project settings. [http] setting block describes how the Plugin's HTTP server works. The HTTP server is responsible for listening for updates from Jira, and processing updates, like when someone drags a task from Inbox to Approved column. You must provide an address the server should listen on, and a certificate to use. It's possible to setup on the same server as the Teleport Proxy, so you can use the same TLS certificate. Testing You should be able to run the Teleport plugin now! teleport-jira start The log output should look familiar to what Teleport service logs. You should see that it connected to Teleport, and is listening for new Teleport requests and Jira webhooks. Go ahead and test it: tsh login --request-roles = admin That should create a new permission request on Teleport (you can test if it did with tctl request ls ), and you should see a new task on your Jira project board. Setup with SystemD In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-jira.service! } Save this as teleport-jira.service . Audit Log The plugin will let anyone with access to the Jira board approve/deny requests so it's important to review Teleport's audit log. Feedback If you have any issues with this plugin please create an issue here .","title":"Teleport  Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#ssh-login-approvals-using-jira","text":"","title":"SSH login approvals using Jira"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#teleport-jira-plugin-setup","text":"This guide will talk through how to setup Teleport with Jira. Teleport to Jira integration allows you to treat Teleport access and permission requests using Jira tickets. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles.","title":"Teleport Jira Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#setup","text":"This guide assumes that you have: Teleport Enterprise 4.2.8 or newer Admin privileges with access to tctl Jira Server or Jira Cloud installation with an owner privileges, specifically to setup webhooks, issue types, and workflows.","title":"Setup"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#create-an-access-plugin-role-and-user-within-teleport","text":"First off, using an existing Teleport Cluster, we are going to create a new Teleport User and Role to access Teleport.","title":"Create an access-plugin role and user within Teleport"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#setting-up-your-jira-project","text":"","title":"Setting up your Jira Project"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#creating-the-permission-management-project","text":"All new permission requests are going to show up in a project you choose. We recommend that you create a separate project for permissions management, and a new board in said project. You'll need the project Jira key to configure the plugin.","title":"Creating the permission management project"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#setting-up-the-status-board","text":"Create a new board for tasks in the permission management project. The board has to have at least these three columns: Pending Approved Denied Teleport Jira Plugin will create a new issue for each new permission request in the first available column on the board. When you drag the request task to Approved column on Jira, the request will be approved. If you drag the request task to the Denied column in Jira, the request will be denied.","title":"Setting up the status board"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#setting-up-request-id-field-on-jira","text":"Teleport Jira Plugin requires a custom issue field to be created. Go to your Jira Project settings \u2192 Issue Types \u2192 Select type Task \u2192 add a new Short Text field named TeleportAccessRequestId . Teleport uses this field to reference its internal request ID. If anyone changes this field on Jira, or tries to forge the permission request, Teleport will validate it and ignore it.","title":"Setting up Request ID field on Jira"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#getting-your-jira-api-token","text":"If you're using Jira Cloud, navigate to Account Settings \u2192 Security \u2192 API Tokens and create a new app specific API token in your Jira installation. You'll need this token later to configure the plugin. For Jira Server, the URL of the API tokens page will be different depending on your installation.","title":"Getting your Jira API token"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#setting-up-jira-webhooks","text":"Go to Settings \u2192 General \u2192 System \u2192 Webhooks and create a new Webhook for Jira to tell the Teleport Plugin about updates. For the webhook URL, use the URL that you'll run the plugin on. It needs to be a publicly accessible URL that we'll set up later. Jira requires the webhook listener to run over HTTPS. The Teleport Jira plugin webhook needs to be notified only about new issues being created, issues being updated, or deleted. You can leave all the other boxes empty. Plugin Defaults Jira Webhook will send updates about any issues in any projects in your Jira installation to the webhook. We suggest that you use JQL filters to limit which issues are being sent to the plugin. The Plugin's web server will run with TLS, but you can disable it with --insecure-no-tls to test things out in a dev environment. In the webhook settings page, make sure that the webhook will only send Issue Updated updates. It's not critical if anything else gets sent, the plugin will just ignore everything else.","title":"Setting up Jira Webhooks"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#installing","text":"We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-jira-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-jira-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-jira/ $ ./install $ which teleport-jira /usr/local/bin/teleport-jira Run ./install in from 'teleport-jira' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation.","title":"Installing"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#configuration-file","text":"Teleport Jira Plugin uses a config file in TOML format. Generate a boilerplate config by running the following command: $ teleport-jira configure > teleport-jira.toml $ sudo mv teleport-jira.toml /etc By default, Jira Teleport Plugin will use a config in /etc/teleport-jira.toml , and you can override it with -c config/file/path.toml flag. { ! examples / resources / plugins / teleport-jira . toml ! } The [teleport] section describes where the teleport service running, and what keys should the plugin use to authenticate itself. Use the keys that you've generated. The [jira] section requires a few things: Your Jira Cloud or Jira Server URL. For Jira Cloud, it looks something like yourcompany.atlassian.net . Your username on Jira, i.e. ben@gravitational.com Your Jira API token that you've created above. A Jira Project key, available in Project settings. [http] setting block describes how the Plugin's HTTP server works. The HTTP server is responsible for listening for updates from Jira, and processing updates, like when someone drags a task from Inbox to Approved column. You must provide an address the server should listen on, and a certificate to use. It's possible to setup on the same server as the Teleport Proxy, so you can use the same TLS certificate.","title":"Configuration file"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#testing","text":"You should be able to run the Teleport plugin now! teleport-jira start The log output should look familiar to what Teleport service logs. You should see that it connected to Teleport, and is listening for new Teleport requests and Jira webhooks. Go ahead and test it: tsh login --request-roles = admin That should create a new permission request on Teleport (you can test if it did with tctl request ls ), and you should see a new task on your Jira project board.","title":"Testing"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#setup-with-systemd","text":"In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-jira.service! } Save this as teleport-jira.service .","title":"Setup with SystemD"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#audit-log","text":"The plugin will let anyone with access to the Jira board approve/deny requests so it's important to review Teleport's audit log.","title":"Audit Log"},{"location":"enterprise/workflow/ssh-approval-jira-cloud/#feedback","text":"If you have any issues with this plugin please create an issue here .","title":"Feedback"},{"location":"enterprise/workflow/ssh-approval-jira-server/","text":"SSH login approvals using Jira Server Teleport Jira Server Plugin Setup This guide will talk through how to setup Teleport with Jira Server. Teleport to Jira Server integration allows you to treat Teleport access and permission requests as Jira Tasks. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles. Note Teleport's tsh request workflow is synchronous and needs to be approved within 1 hour of the request. Your browser does not support the video tag. Setup Prerequisites An Enterprise or Pro Teleport Cluster Admin Privileges with access and control of tctl Jira Server installation with owner privileges, specifically to setup webhooks, issue types, and workflows. This plugin has been tested with Jira Software 8.8.0 Create an access-plugin role and user within Teleport First off, using an existing Teleport Cluster, we are going to create a new Teleport User and Role to access Teleport. Create User and Role for access. Log into Teleport Authentication Server, this is where you normally run tctl . Create a new user and role that only has API access to the access_request API. The below script will create a yaml resource file for a new user and role. $ cat > rscs.yaml <<EOF kind: user metadata: name: access-plugin-jira spec: roles: ['access-plugin-jira'] version: v2 --- kind: role metadata: name: access-plugin-jira spec: allow: rules: - resources: ['access_request'] verbs: ['list','read','update'] # teleport currently refuses to issue certs for a user with 0 logins, # this restriction may be lifted in future versions. logins: ['access-plugin-jira'] version: v3 EOF # ... $ tctl create -f rscs.yaml Export access-plugin Certificate Teleport Plugin uses the access-plugin-jira role and user to perform the approval. We export the identity files, using tctl auth sign . $ tctl auth sign --format = tls --user = access-plugin-jira --out = auth --ttl = 8760h # ... The above sequence should result in three PEM encoded files being generated: auth.crt, auth.key, and auth.cas (certificate, private key, and CA certs respectively). We'll reference the auth.crt, auth.key, and auth.cas files later when configuring the plugins . Certificate Lifetime By default, tctl auth sign produces certificates with a relatively short lifetime. For production deployments, the --ttl flag can be used to ensure a more practical certificate lifetime. --ttl=8760h exports a 1 year token Setting up your Jira Server instance Creating a Project Teleport Jira Plugin relies on your Jira project having a board with at least three statuses (columns): Pending, Approved, and Denied. It's therefore the easiest scenario to create a new Jira project for Teleport to use. The specific type of project you choose when you create it doesn't matter, as long as you can setup a Kanban Board for it, but we recommend that you go with Kanban Software Development \u2014 this will reduce the amount of setup work you'll have to do and provide the board out of the box. You'll need the project key for the Teleport plugin settings later on. It's usually a 3 character code for the project. Setting up Request ID field on Jira Teleport stores the request metadata in a special Jira custom field that must be named teleportAccessRequestId. To create that field, go to Administration -> Issues -> Custom Fields -> Add Custom Field. Name the field teleportAccessRequestId , and choose Text Field (single line) as the field type. Assign the field to your project, or make it global. Teleport Access Request ID is an internal field and it's not supposed to be edited by users, so you can leave the Screens section blank. That means that the field won't show up in Jira UI. Go to Project Settings -> Fields and make sure that the teleportAccessRequestId field shows up on the list of fields available in this project. Setting up the status board The default Jira Software workflow has a different board setup from what Teleport needs, so we'll setup another workflow and assign that workflow to the project board. Go to Administration -> Workflows. You can choose to add a new workflow (recommended), or edit the existing workflow, it'll be called Software Simplified Workflow for Project NAME by default. It's only used in your single project, so it's safe to edit it. Edit the workflow to have these three states: Pending Approved Denied The rules of the workflow must meet these requirements: New created issues should be in Pending state. It should be possible to move from Pending to Approved It should be possible to move from Pending to Declined. You can choose to make the workflow strict and restrict moving requests from Approved state to Declined state and vice versa, or leave that flexible. Teleport will only change the request status once, i.e. the first time the request is approved or denied on your Jira board. With Workflow editor you can setup who can approve or deny the request based on their Jira user permissions. We won't cover that in this guide as it mostly relates to Jira settings. By default Teleport will allow anyone who can use the workflow to approve or deny the request. Go to your Project Settings -> Workflows, and make sure that your workflow that you just created or edited is applied to the project you'll use for Teleport integration. Setting up the webhook Teleport Jira Plugin will listen for a webhook that Jira Server sends when a request is approved or denied. Go to Settings -> System -> Webhooks to setup the webhook. The webhook needs to be sent when issues are updated or deleted. Configuring the Teleport Jira Plugin for Jira Server Installing We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-jira-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-jira-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-jira/ $ sudo ./install # Teleport Jira Plugin binaries have been copied to /usr/local/bin # You can run teleport-jira configure > /etc/teleport-jira.toml to bootstrap your config file. $ which teleport-jira /usr/local/bin/teleport-jira Run sudo ./install in from 'teleport-jira' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation. Configuration file Teleport Jira Plugin uses a config file in TOML format. Generate a boilerplate config by running the following command: $ teleport-jira configure > teleport-jira.toml $ sudo mv teleport-jira.toml /etc By default, Jira Teleport Plugin will use a config in /etc/teleport-jira.toml , and you can override it with -c config/file/path.toml flag. { ! examples / resources / plugins / teleport-jira . toml ! } The [teleport] section describes where is the teleport service running, and what keys should the plugin use to authenticate itself. Use the keys that you've generated above in exporting your Certificate section . The [jira] section requires a few things: 1. Your Jira Cloud or Jira Server URL. For Jira Cloud, it looks something like yourcompany.atlassian.net. 2. Your username on Jira, i.e. benarent Note: Not your email address. 3. Your Jira API token. For Jira Server, this is a password. it's a good idea to create a separate user record with permissions limited to accessing this particular project board, and use this with the bot. 4. And the Jira Project key, available in Project settings. [http] setting block describes how the Plugin's HTTP server works. The HTTP server is responsible for listening for updates from Jira, and processing updates, like when someone drags a task from Inbox to Approved column. You must provide an address the server should listen on, and a certificate to use, unless you plan on running with --insecure-no-tls , which we don't recommend in production. Testing You should be able to run the Teleport plugin now! teleport-jira start INFO Starting Teleport Access JIRAbot 0 .1.0-alpha.3:teleport-jira-v0.1.0-alpha.3-0-gea1ef8e jira/app.go:74 # DEBU Checking Teleport server version jira/app.go:150 # DEBU Starting JIRA API health check... jira/app.go:111 # DEBU Checking out JIRA project... jira/bot.go:145 # DEBU Found project \"TEL1\": \"Tel-kb\" jira/bot.go:150 # DEBU Checking out JIRA project permissions... jira/bot.go:152 # DEBU JIRA API health check finished ok jira/app.go:117 # DEBU Starting secure HTTPS server on 66.66.66.66:8081 utils/http.go:235 # DEBU Watcher connected access/service_job.go:62 The log output should look familiar to what Teleport service logs. You should see that it connected to Teleport, and is listening for new Teleport requests and Jira webhooks. Go ahead and test it: tsh login --request-roles = admin That should create a new permission request on Teleport (you can test if it did with tctl request ls ), and you should see a new task on your Jira project board. Setup with SystemD In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-jira.service! } Save this as teleport-jira.service . Audit Log The plugin will let anyone with access to the Jira board approve or deny requests, so it's important to review Teleport's audit log. Feedback If you have any issues with this plugin please create an issue here .","title":"Teleport Jira Server Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-jira-server/#ssh-login-approvals-using-jira-server","text":"","title":"SSH login approvals using Jira Server"},{"location":"enterprise/workflow/ssh-approval-jira-server/#teleport-jira-server-plugin-setup","text":"This guide will talk through how to setup Teleport with Jira Server. Teleport to Jira Server integration allows you to treat Teleport access and permission requests as Jira Tasks. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles. Note Teleport's tsh request workflow is synchronous and needs to be approved within 1 hour of the request. Your browser does not support the video tag.","title":"Teleport Jira Server Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-jira-server/#setup","text":"","title":"Setup"},{"location":"enterprise/workflow/ssh-approval-jira-server/#prerequisites","text":"An Enterprise or Pro Teleport Cluster Admin Privileges with access and control of tctl Jira Server installation with owner privileges, specifically to setup webhooks, issue types, and workflows. This plugin has been tested with Jira Software 8.8.0","title":"Prerequisites"},{"location":"enterprise/workflow/ssh-approval-jira-server/#create-an-access-plugin-role-and-user-within-teleport","text":"First off, using an existing Teleport Cluster, we are going to create a new Teleport User and Role to access Teleport.","title":"Create an access-plugin role and user within Teleport"},{"location":"enterprise/workflow/ssh-approval-jira-server/#setting-up-your-jira-server-instance","text":"","title":"Setting up your Jira Server instance"},{"location":"enterprise/workflow/ssh-approval-jira-server/#setting-up-the-webhook","text":"Teleport Jira Plugin will listen for a webhook that Jira Server sends when a request is approved or denied. Go to Settings -> System -> Webhooks to setup the webhook. The webhook needs to be sent when issues are updated or deleted.","title":"Setting up the webhook"},{"location":"enterprise/workflow/ssh-approval-jira-server/#installing","text":"We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-jira-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-jira-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-jira/ $ sudo ./install # Teleport Jira Plugin binaries have been copied to /usr/local/bin # You can run teleport-jira configure > /etc/teleport-jira.toml to bootstrap your config file. $ which teleport-jira /usr/local/bin/teleport-jira Run sudo ./install in from 'teleport-jira' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation.","title":"Installing"},{"location":"enterprise/workflow/ssh-approval-jira-server/#configuration-file","text":"Teleport Jira Plugin uses a config file in TOML format. Generate a boilerplate config by running the following command: $ teleport-jira configure > teleport-jira.toml $ sudo mv teleport-jira.toml /etc By default, Jira Teleport Plugin will use a config in /etc/teleport-jira.toml , and you can override it with -c config/file/path.toml flag. { ! examples / resources / plugins / teleport-jira . toml ! } The [teleport] section describes where is the teleport service running, and what keys should the plugin use to authenticate itself. Use the keys that you've generated above in exporting your Certificate section . The [jira] section requires a few things: 1. Your Jira Cloud or Jira Server URL. For Jira Cloud, it looks something like yourcompany.atlassian.net. 2. Your username on Jira, i.e. benarent Note: Not your email address. 3. Your Jira API token. For Jira Server, this is a password. it's a good idea to create a separate user record with permissions limited to accessing this particular project board, and use this with the bot. 4. And the Jira Project key, available in Project settings. [http] setting block describes how the Plugin's HTTP server works. The HTTP server is responsible for listening for updates from Jira, and processing updates, like when someone drags a task from Inbox to Approved column. You must provide an address the server should listen on, and a certificate to use, unless you plan on running with --insecure-no-tls , which we don't recommend in production.","title":"Configuration file"},{"location":"enterprise/workflow/ssh-approval-jira-server/#testing","text":"You should be able to run the Teleport plugin now! teleport-jira start INFO Starting Teleport Access JIRAbot 0 .1.0-alpha.3:teleport-jira-v0.1.0-alpha.3-0-gea1ef8e jira/app.go:74 # DEBU Checking Teleport server version jira/app.go:150 # DEBU Starting JIRA API health check... jira/app.go:111 # DEBU Checking out JIRA project... jira/bot.go:145 # DEBU Found project \"TEL1\": \"Tel-kb\" jira/bot.go:150 # DEBU Checking out JIRA project permissions... jira/bot.go:152 # DEBU JIRA API health check finished ok jira/app.go:117 # DEBU Starting secure HTTPS server on 66.66.66.66:8081 utils/http.go:235 # DEBU Watcher connected access/service_job.go:62 The log output should look familiar to what Teleport service logs. You should see that it connected to Teleport, and is listening for new Teleport requests and Jira webhooks. Go ahead and test it: tsh login --request-roles = admin That should create a new permission request on Teleport (you can test if it did with tctl request ls ), and you should see a new task on your Jira project board.","title":"Testing"},{"location":"enterprise/workflow/ssh-approval-jira-server/#setup-with-systemd","text":"In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-jira.service! } Save this as teleport-jira.service .","title":"Setup with SystemD"},{"location":"enterprise/workflow/ssh-approval-jira-server/#audit-log","text":"The plugin will let anyone with access to the Jira board approve or deny requests, so it's important to review Teleport's audit log.","title":"Audit Log"},{"location":"enterprise/workflow/ssh-approval-jira-server/#feedback","text":"If you have any issues with this plugin please create an issue here .","title":"Feedback"},{"location":"enterprise/workflow/ssh-approval-mattermost/","text":"Teleport Mattermost Plugin Setup This guide will talk through how to setup Teleport with Mattermost. Teleport to Mattermost integration allows teams to approve or deny Teleport access requests using Mattermost an open source messaging platform. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles. Example Mattermost Request Your browser does not support the video tag. Setup Prerequisites This guide assumes that you have: Teleport Enterprise 4.2.8 or newer Admin privileges with access to tctl Mattermost account with admin privileges. This plugin has been tested with Mattermost 5.x Setting up Mattermost to work with the bot In Mattermost, go to System Console \u2192 Integrations \u2192 Enable Bot Account Creation \u2192 Set to True. This will allow us to create a new bot account that the Teleport bot will use. Go back to your team, then Integrations \u2192 Bot Accounts \u2192 Add Bot Account. The new bot account will need Post All permission. App Icon: Download Teleport Bot Icon Create an OAuth 2.0 Application In Mattermost, go to System Console \u2192 Integrations \u2192 OAuth 2.0 Applications. Set Callback URLs to the location of your Teleport Proxy The confirmation screen after you've created the bot will give you the access token. We'll use this in the config later. Create User and Role for access. Log into Teleport Authentication Server, this is where you normally run tctl . Create a new user and role that only has API access to the access_request API. The below script will create a yaml resource file for a new user and role. # This command will create two Teleport Yaml resources, a new Teleport user and a # Role for that users that can only approve / list requests. $ cat > rscs.yaml <<EOF kind : user metadata : name : access-plugin-mattermost spec : roles : [ 'access-plugin-mattermost' ] version : v2 --- kind : role metadata : name : access-plugin-mattermost spec : allow : rules : - resources : [ 'access_request' ] verbs : [ 'list' , 'read' , 'update' ] # teleport currently refuses to issue certs for a user with 0 logins, # this restriction may be lifted in future versions. logins : [ 'access-plugin-mattermost' ] version : v3 EOF # Run this to create the user and role in Teleport. $ tctl create -f rscs.yaml Export access-plugin Certificate Teleport Plugin use the access-plugin-mattermost role and user to perform the approval. We export the identity files, using tctl auth sign . $ tctl auth sign --format = tls --user = access-plugin-mattermost --out = auth --ttl = 8760h # ... The above sequence should result in three PEM encoded files being generated: auth.crt, auth.key, and auth.cas (certificate, private key, and CA certs respectively). We'll reference the auth.crt, auth.key, and auth.cas files later when configuring the plugins . Certificate Lifetime By default, tctl auth sign produces certificates with a relatively short lifetime. For production deployments, the --ttl flag can be used to ensure a more practical certificate lifetime. --ttl=8760h exports a 1 year token Downloading and installing the plugin We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-mattermost-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-mattermost-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-mattermost $ ./install $ which teleport-mattermost /usr/local/bin/teleport-mattermost Run ./install in from 'teleport-mattermost' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation. Configuring Mattermost bot Mattermost Bot uses a config file in TOML format. Generate a boilerplate config by running the following command: $ teleport-mattermost configure > teleport-mattermost.toml $ sudo mv teleport-mattermost.toml /etc Then, edit the config as needed. { !examples/resources/plugins/teleport-mattermost.toml! } Testing the Plugin With the config above, you should be able to run the bot invoking teleport-mattermost start -d . The will provide some debug information to make sure the bot can connect to Mattermost. $ teleport-mattermost start -d DEBU DEBUG logging enabled logrus/exported.go:117 INFO Starting Teleport Access Mattermost Bot {{ teleport.plugin.version }} -dev.1: mattermost/main.go:140 DEBU Checking Teleport server version mattermost/main.go:234 DEBU Starting a request watcher... mattermost/main.go:296 DEBU Starting Mattermost API health check... mattermost/main.go:186 DEBU Starting secure HTTPS server on :8081 utils/http.go:146 DEBU Watcher connected mattermost/main.go:260 DEBU Mattermost API health check finished ok mattermost/main.go:19 Setup with SystemD In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-mattermost.service! } Save this as teleport-mattermost.service . Audit Log The plugin will let anyone with access to the Mattermost channel requests so it's important to review Teleport's audit log. Feedback If you have any issues with this plugin please create an issue here .","title":"Teleport Mattermost Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-mattermost/#teleport-mattermost-plugin-setup","text":"This guide will talk through how to setup Teleport with Mattermost. Teleport to Mattermost integration allows teams to approve or deny Teleport access requests using Mattermost an open source messaging platform. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles.","title":"Teleport Mattermost Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-mattermost/#setup","text":"","title":"Setup"},{"location":"enterprise/workflow/ssh-approval-mattermost/#prerequisites","text":"This guide assumes that you have: Teleport Enterprise 4.2.8 or newer Admin privileges with access to tctl Mattermost account with admin privileges. This plugin has been tested with Mattermost 5.x","title":"Prerequisites"},{"location":"enterprise/workflow/ssh-approval-mattermost/#downloading-and-installing-the-plugin","text":"We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-mattermost-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-mattermost-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-mattermost $ ./install $ which teleport-mattermost /usr/local/bin/teleport-mattermost Run ./install in from 'teleport-mattermost' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation.","title":"Downloading and installing the plugin"},{"location":"enterprise/workflow/ssh-approval-mattermost/#configuring-mattermost-bot","text":"Mattermost Bot uses a config file in TOML format. Generate a boilerplate config by running the following command: $ teleport-mattermost configure > teleport-mattermost.toml $ sudo mv teleport-mattermost.toml /etc Then, edit the config as needed. { !examples/resources/plugins/teleport-mattermost.toml! }","title":"Configuring Mattermost bot"},{"location":"enterprise/workflow/ssh-approval-mattermost/#testing-the-plugin","text":"With the config above, you should be able to run the bot invoking teleport-mattermost start -d . The will provide some debug information to make sure the bot can connect to Mattermost. $ teleport-mattermost start -d DEBU DEBUG logging enabled logrus/exported.go:117 INFO Starting Teleport Access Mattermost Bot {{ teleport.plugin.version }} -dev.1: mattermost/main.go:140 DEBU Checking Teleport server version mattermost/main.go:234 DEBU Starting a request watcher... mattermost/main.go:296 DEBU Starting Mattermost API health check... mattermost/main.go:186 DEBU Starting secure HTTPS server on :8081 utils/http.go:146 DEBU Watcher connected mattermost/main.go:260 DEBU Mattermost API health check finished ok mattermost/main.go:19","title":"Testing the Plugin"},{"location":"enterprise/workflow/ssh-approval-mattermost/#setup-with-systemd","text":"In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-mattermost.service! } Save this as teleport-mattermost.service .","title":"Setup with SystemD"},{"location":"enterprise/workflow/ssh-approval-mattermost/#audit-log","text":"The plugin will let anyone with access to the Mattermost channel requests so it's important to review Teleport's audit log.","title":"Audit Log"},{"location":"enterprise/workflow/ssh-approval-mattermost/#feedback","text":"If you have any issues with this plugin please create an issue here .","title":"Feedback"},{"location":"enterprise/workflow/ssh-approval-pagerduty/","text":"SSH Login Approval using PagerDuty Teleport Pagerduty Plugin Setup This guide will talk through how to setup Teleport with Pagerduty. Teleport to Pagerduty integration allows you to treat Teleport access and permission requests as Pagerduty incidents \u2014 notifying the appropriate team, and approve or deny the requests via Pagerduty special action. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles. Setup Prerequisites This guide assumes that you have: Teleport Enterprise 4.2.8 or newer Admin privileges with access to tctl Pagerduty account already set, with access to creating a new API token. A node to run the plugin, we recommend running it alongside the Teleport Proxy for convenience. Create User and Role for access. Log into Teleport Authentication Server, this is where you normally run tctl . Create a new user and role that only has API access to the access_request API. The below script will create a yaml resource file for a new user and role. # This command will create two Teleport Yaml resources, a new Teleport user and a # Role for that users that can only approve / list requests. $ cat > rscs.yaml <<EOF kind : user metadata : name : access-plugin-pagerduty spec : roles : [ 'access-plugin-pagerduty' ] version : v2 --- kind : role metadata : name : access-plugin-pagerduty spec : allow : rules : - resources : [ 'access_request' ] verbs : [ 'list' , 'read' , 'update' ] # teleport currently refuses to issue certs for a user with 0 logins, # this restriction may be lifted in future versions. logins : [ 'access-plugin-pagerduty' ] version : v3 EOF # Run this to create the user and role in Teleport. $ tctl create -f rscs.yaml Export access-plugin Certificate Teleport Plugin use the access-plugin-pagerduty role and user to perform the approval. We export the identity files, using tctl auth sign . $ tctl auth sign --format = tls --user = access-plugin-pagerduty --out = auth --ttl = 8760h # ... The above sequence should result in three PEM encoded files being generated: auth.crt, auth.key, and auth.cas (certificate, private key, and CA certs respectively). We'll reference the auth.crt, auth.key, and auth.cas files later when configuring the plugins . Certificate Lifetime By default, tctl auth sign produces certificates with a relatively short lifetime. For production deployments, the --ttl flag can be used to ensure a more practical certificate lifetime. --ttl=8760h exports a 1 year token Setting up Pagerduty API key In your Pagerduty dashboard, go to Configuration \u2192 API Access \u2192 Create New API Key , add a key description, and save the key. We'll use the key in the plugin config file later. Create Pager Duty API Key Create Service Account Downloading and installing the plugin We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-pagerduty-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-pagerduty-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-pagerduty/ $ ./install $ which teleport-pagerduty /usr/local/bin/teleport-pagerduty Run ./install in from 'teleport-pagerduty' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation. Config file Teleport Pagerduty plugin has its own configuration file in TOML format. Before starting the plugin for the first time, you'll need to generate and edit that config file. $ teleport-pagerduty configure > teleport-pagerduty.toml $ sudo mv teleport-pagerduty.toml /etc Editing the config file After generating the config, edit it as follows: # Example PagerDuty config file { ! examples / resources / plugins / teleport-pagerduty . toml ! } Testing the Plugin With the config above, you should be able to run the plugin invoking teleport-pagerduty start -d . The will provide some debug information to make sure the bot can connect to Pagerduty. $ teleport-pagerduty start -d DEBU DEBUG logging enabled logrus/exported.go:117 INFO Starting Teleport Access PagerDuty extension 0 .1.0-dev.1: pagerduty/main.go:124 DEBU Checking Teleport server version pagerduty/main.go:226 DEBU Starting a request watcher... pagerduty/main.go:288 DEBU Starting PagerDuty API health check... pagerduty/main.go:170 DEBU Starting secure HTTPS server on :8081 utils/http.go:146 DEBU Watcher connected pagerduty/main.go:252 DEBU PagerDuty API health check finished ok pagerduty/main.go:176 DEBU Setting up the webhook extensions pagerduty/main.go:178 By default, teleport-pagerduty will assume its config is in /etc/teleport-pagerduty.toml , but you can override it with --config option. Setup with SystemD In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-pagerduty.service! } Save this as teleport-pagerduty.service . Example PagerDuty Request Your browser does not support the video tag. Audit Log The plugin will let anyone with access to the PagerDuty account so it's important to review Teleport's audit log. Feedback If you have any issues with this plugin please create an issue here .","title":"Teleport Pagerduty Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#ssh-login-approval-using-pagerduty","text":"","title":"SSH Login Approval using PagerDuty"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#teleport-pagerduty-plugin-setup","text":"This guide will talk through how to setup Teleport with Pagerduty. Teleport to Pagerduty integration allows you to treat Teleport access and permission requests as Pagerduty incidents \u2014 notifying the appropriate team, and approve or deny the requests via Pagerduty special action. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles.","title":"Teleport Pagerduty Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#setup","text":"","title":"Setup"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#prerequisites","text":"This guide assumes that you have: Teleport Enterprise 4.2.8 or newer Admin privileges with access to tctl Pagerduty account already set, with access to creating a new API token. A node to run the plugin, we recommend running it alongside the Teleport Proxy for convenience.","title":"Prerequisites"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#setting-up-pagerduty-api-key","text":"In your Pagerduty dashboard, go to Configuration \u2192 API Access \u2192 Create New API Key , add a key description, and save the key. We'll use the key in the plugin config file later. Create Pager Duty API Key Create Service Account","title":"Setting up Pagerduty API key"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#downloading-and-installing-the-plugin","text":"We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-pagerduty-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-pagerduty-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-pagerduty/ $ ./install $ which teleport-pagerduty /usr/local/bin/teleport-pagerduty Run ./install in from 'teleport-pagerduty' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation.","title":"Downloading and installing the plugin"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#config-file","text":"Teleport Pagerduty plugin has its own configuration file in TOML format. Before starting the plugin for the first time, you'll need to generate and edit that config file. $ teleport-pagerduty configure > teleport-pagerduty.toml $ sudo mv teleport-pagerduty.toml /etc","title":"Config file"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#testing-the-plugin","text":"With the config above, you should be able to run the plugin invoking teleport-pagerduty start -d . The will provide some debug information to make sure the bot can connect to Pagerduty. $ teleport-pagerduty start -d DEBU DEBUG logging enabled logrus/exported.go:117 INFO Starting Teleport Access PagerDuty extension 0 .1.0-dev.1: pagerduty/main.go:124 DEBU Checking Teleport server version pagerduty/main.go:226 DEBU Starting a request watcher... pagerduty/main.go:288 DEBU Starting PagerDuty API health check... pagerduty/main.go:170 DEBU Starting secure HTTPS server on :8081 utils/http.go:146 DEBU Watcher connected pagerduty/main.go:252 DEBU PagerDuty API health check finished ok pagerduty/main.go:176 DEBU Setting up the webhook extensions pagerduty/main.go:178 By default, teleport-pagerduty will assume its config is in /etc/teleport-pagerduty.toml , but you can override it with --config option.","title":"Testing the Plugin"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#setup-with-systemd","text":"In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-pagerduty.service! } Save this as teleport-pagerduty.service .","title":"Setup with SystemD"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#audit-log","text":"The plugin will let anyone with access to the PagerDuty account so it's important to review Teleport's audit log.","title":"Audit Log"},{"location":"enterprise/workflow/ssh-approval-pagerduty/#feedback","text":"If you have any issues with this plugin please create an issue here .","title":"Feedback"},{"location":"enterprise/workflow/ssh-approval-slack/","text":"Teleport Slack Plugin Setup This guide will talk through how to setup Teleport with Slack. Teleport to Slack integration allows you to treat Teleport access and permission requests via Slack message and inline interactive components. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles. Example Slack Request Your browser does not support the video tag. Setup Prerequisites This guide assumes that you have: Teleport Enterprise 4.2.8 or newer Admin privileges with access to tctl Slack Admin Privileges to create an app and install it to your workspace. Create User and Role for access. Log into Teleport Authentication Server, this is where you normally run tctl . Create a new user and role that only has API access to the access_request API. The below script will create a yaml resource file for a new user and role. # Copy and Paste the below on the Teleport Auth server. $ cat > rscs.yaml <<EOF kind: user metadata: name: access-plugin-slack spec: roles: ['access-plugin-slack'] version: v2 --- kind: role metadata: name: access-plugin-slack spec: allow: rules: - resources: ['access_request'] verbs: ['list','read','update'] # teleport currently refuses to issue certs for a user with 0 logins, # this restriction may be lifted in future versions. logins: ['access-plugin-slack'] version: v3 EOF # Use tctl to create the user and role within Teleport. $ tctl create -f rscs.yaml Tip If you're using other plugins, you might want to create different users and roles for different plugins Export access-plugin Certificate Teleport Plugin use the access-plugin-slack role and user to perform the approval. We export the identity files, using tctl auth sign . $ tctl auth sign --format = tls --user = access-plugin-slack --out = auth --ttl = 8760h # ... The above sequence should result in three PEM encoded files being generated: auth.crt, auth.key, and auth.cas (certificate, private key, and CA certs respectively). We'll reference the auth.crt, auth.key, and auth.cas files later when configuring the plugins . Certificate Lifetime By default, tctl auth sign produces certificates with a relatively short lifetime. For production deployments, the --ttl flag can be used to ensure a more practical certificate lifetime. --ttl=8760h exports a 1 year token Create Slack App We'll create a new Slack app and setup auth tokens and callback URLs, so that Slack knows how to notify the Teleport plugin when Approve / Deny buttons are clicked. You'll need to: Create a new app, pick a name and select a workspace it belongs to. Select \u201capp features\u201d: we'll enable interactivity and setup the callback URL here. Add OAuth Scopes. This is required by Slack for the app to be installed \u2014 we'll only need a single scope to post messages to your Slack account. Obtain OAuth token and callback signing secret for the Teleport plugin config. Creating a New Slack app Visit https://api.slack.com/apps to create a new Slack App. App Name: Teleport Development Slack Workspace: Pick the workspace you'd like the requests to show up in. App Icon: Download Teleport Bot Icon Setup Interactive Components This URL must match the URL setting in Teleport Plugin settings file (we'll cover that later), and be publicly accessible. For now, just think of the URL you'll use and set it in the Slack App's settings screen in Features > Interactive Components > Request URL. Selecting OAuth Scopes On the App screen, go to \u201cOAuth and Permissions\u201d under Features in the sidebar menu. Then scroll to Scopes, and add chat:write, incoming-webhook, users:read, users:read.email scopes so that our plugin can post messages to your Slack channels. Obtain OAuth Token Getting the secret signing token In the sidebar of the app screen, click on Basic. Scroll to App Credentials section, and grab the app's Signing Secret. We'll use it in the config file later. Add to Workspace After adding to the workspace, you still need to invite the bot to the channel. Do this by using the @ command, and inviting them to the channel. Installing the Teleport Slack Plugin We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-slack-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-slack-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-slack/ $ ./install $ which teleport-slack /usr/local/bin/teleport-slack Run ./install in from 'teleport-slack' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation. Configuring Teleport Slack Teleport Slack uses a config file in TOML format. Generate a boilerplate config by running the following command: $ teleport-slack configure > teleport-slack.toml $ sudo mv teleport-slack.toml /etc Then, edit the config as needed. { !examples/resources/plugins/teleport-slack.toml! } Editing the config file In the Teleport section, use the certificates you've generated with tctl auth sign before. The plugin installer creates a folder for those certificates in /var/lib/teleport/plugins/slack/ \u2014 so just move the certificates there and make sure the config points to them. In Slack section, use the OAuth token, signing token, setup the desired channel name. The listen URL is the URL the plugin will listen for Slack callbacks. Then set the plugin callback (where Slack sends its requests) to an address you like, and provide the TLS certificates for that http server to use. By default, Teleport Slack plugin will run with TLS on. {!examples/resources/plugins/teleport-slack.toml!} Test Run Assuming that Teleport is running, and you've created the Slack app, the plugin config, and provided all the certificates \u2014 you can now run the plugin and test the workflow! $ teleport-slack start If everything works fine, the log output should look like this: $ teleport-slack start INFO Starting Teleport Access Slack {{ teleport.plugin.version }} .1-0-slack/main.go:145 INFO Starting a request watcher... slack/main.go:330 INFO Starting insecure HTTP server on 0 .0.0.0:8081 utils/http.go:64 INFO Watcher connected slack/main.go:298 Testing the approval workflow You can create a test permissions request with tctl and check if the plugin works as expected like this: Create a test permissions request behalf of a user. # Replace USERNAME with a Teleport local user, and TARGET_ROLE with a Teleport Role $ tctl request create USERNAME --roles = TARGET_ROLE A user can also try using --request-roles flag. # Example with a user trying to request a role DBA. $ tsh login --request-roles = dba Approve or deny the request on Slack The messages should automatically get updated to reflect the action you just clicked. You can also check the request status with tctl : $ tctl request ls TSH User Login and Request Admin Role. You can also test the full workflow from the user's perspective using tsh : # tsh login --request-roles=REQUESTED_ROLE Seeking request approval... ( id: 8f77d2d1-2bbf-4031-a300-58926237a807 ) You should now see a new request in Teleport, and a message about the request on Slack. You can approve or deny it and tsh should login successfully or error out right after you click an action button on Slack. Setup with SystemD In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-slack.service! } Save this as teleport-slack.service . Audit Log The plugin will let anyone with access to the Slack Channel so it's important to review Teleport' audit log. Feedback If you have any issues with this plugin please create an issue here .","title":"Teleport Slack Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-slack/#teleport-slack-plugin-setup","text":"This guide will talk through how to setup Teleport with Slack. Teleport to Slack integration allows you to treat Teleport access and permission requests via Slack message and inline interactive components. Warning The Approval Workflow only works with Teleport Enterprise as it requires several roles.","title":"Teleport Slack Plugin Setup"},{"location":"enterprise/workflow/ssh-approval-slack/#setup","text":"","title":"Setup"},{"location":"enterprise/workflow/ssh-approval-slack/#prerequisites","text":"This guide assumes that you have: Teleport Enterprise 4.2.8 or newer Admin privileges with access to tctl Slack Admin Privileges to create an app and install it to your workspace.","title":"Prerequisites"},{"location":"enterprise/workflow/ssh-approval-slack/#create-slack-app","text":"We'll create a new Slack app and setup auth tokens and callback URLs, so that Slack knows how to notify the Teleport plugin when Approve / Deny buttons are clicked. You'll need to: Create a new app, pick a name and select a workspace it belongs to. Select \u201capp features\u201d: we'll enable interactivity and setup the callback URL here. Add OAuth Scopes. This is required by Slack for the app to be installed \u2014 we'll only need a single scope to post messages to your Slack account. Obtain OAuth token and callback signing secret for the Teleport plugin config.","title":"Create Slack App"},{"location":"enterprise/workflow/ssh-approval-slack/#installing-the-teleport-slack-plugin","text":"We recommend installing the Teleport Plugins alongside the Teleport Proxy. This is an ideal location as plugins have a low memory footprint, and will require both public internet access and Teleport Auth access. We currently only provide linux-amd64 binaries, you can also compile these plugins from source . $ wget https://get.gravitational.com/teleport-access-slack-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ tar -xzf teleport-access-slack-v {{ teleport.plugin.version }} -linux-amd64-bin.tar.gz $ cd teleport-access-slack/ $ ./install $ which teleport-slack /usr/local/bin/teleport-slack Run ./install in from 'teleport-slack' or place the executable in the appropriate /usr/bin or /usr/local/bin on the server installation.","title":"Installing the Teleport Slack Plugin"},{"location":"enterprise/workflow/ssh-approval-slack/#configuring-teleport-slack","text":"Teleport Slack uses a config file in TOML format. Generate a boilerplate config by running the following command: $ teleport-slack configure > teleport-slack.toml $ sudo mv teleport-slack.toml /etc Then, edit the config as needed. { !examples/resources/plugins/teleport-slack.toml! }","title":"Configuring Teleport Slack"},{"location":"enterprise/workflow/ssh-approval-slack/#test-run","text":"Assuming that Teleport is running, and you've created the Slack app, the plugin config, and provided all the certificates \u2014 you can now run the plugin and test the workflow! $ teleport-slack start If everything works fine, the log output should look like this: $ teleport-slack start INFO Starting Teleport Access Slack {{ teleport.plugin.version }} .1-0-slack/main.go:145 INFO Starting a request watcher... slack/main.go:330 INFO Starting insecure HTTP server on 0 .0.0.0:8081 utils/http.go:64 INFO Watcher connected slack/main.go:298","title":"Test Run"},{"location":"enterprise/workflow/ssh-approval-slack/#testing-the-approval-workflow","text":"You can create a test permissions request with tctl and check if the plugin works as expected like this:","title":"Testing the approval workflow"},{"location":"enterprise/workflow/ssh-approval-slack/#tsh-user-login-and-request-admin-role","text":"You can also test the full workflow from the user's perspective using tsh : # tsh login --request-roles=REQUESTED_ROLE Seeking request approval... ( id: 8f77d2d1-2bbf-4031-a300-58926237a807 ) You should now see a new request in Teleport, and a message about the request on Slack. You can approve or deny it and tsh should login successfully or error out right after you click an action button on Slack.","title":"TSH User Login and Request Admin Role."},{"location":"enterprise/workflow/ssh-approval-slack/#setup-with-systemd","text":"In production, we recommend starting teleport plugin daemon via an init system like systemd . Here's the recommended Teleport Plugin service unit file for systemd: { !examples/systemd/plugins/teleport-slack.service! } Save this as teleport-slack.service .","title":"Setup with SystemD"},{"location":"enterprise/workflow/ssh-approval-slack/#audit-log","text":"The plugin will let anyone with access to the Slack Channel so it's important to review Teleport' audit log.","title":"Audit Log"},{"location":"enterprise/workflow/ssh-approval-slack/#feedback","text":"If you have any issues with this plugin please create an issue here .","title":"Feedback"},{"location":"features/enhanced-session-recording/","text":"Enhanced Session Recording Teleport SSH and Kubernetes session recording feature captures what is echoed to a terminal. This has inherent advantages, for example because no input is captured, Teleport session recordings typically do not contain passwords that were entered into a terminal. The disadvantage is that session recordings can by bypassed using several techniques: Obfuscation . For example, even though the command echo Y3VybCBodHRwOi8vd3d3LmV4YW1wbGUuY29tCg== | base64 --decode | sh does not contain curl http://www.example.com , when decoded, that is what is run. Shell scripts . For example, if a user uploads and executes a script, the commands run within the script are not captured, simply the output. Terminal controls . Terminals support a wide variety of controls including the ability for users to disable terminal echo. This is frequently used when requesting credentials. Disabling terminal echo allows commands to be run without being captured. Furthermore, due to their unstructured nature, session recordings are difficult to ingest and perform monitoring/alerting on. Note Enhanced Session Recording requires all parts of the Teleport system to be running 4.2+. Requirements: 1. Check / Patch Kernel Teleport 4.2+ with Enhanced Session Recording requires Linux kernel 4.18 (or above) as well as kernel headers. Tip Our Standard Session Recording works with older Linux Kernels. View our audit log docs for more details. You can check your kernel version using the uname command. The output should look something like the following. $ uname -a Linux ip-172-31-43-104.ec2.internal 4 .19.72-25.58.amzn2.x86_64 x86_64 x86_64 x86_64 GNU/Linux Operating System and Kernel Status table Ubuntu Kernel Version 18.04 Bionic Beaver 4.18+ \u2705 20.04 Focal Fossa 5.4 \u2705 CentOS Kernel Version 8.0-1905 4.18.0.80 \u2705 Debian Kernel Version 9 Debian Stretch 4.9.0-6 Patch Kernel 10 Buster 4.19 \u2705 Red Hat Kernel Version Enterprise Linux 8 4.18.0-147 \u2705 Amazon Linux We recommend using Amazon Linux 2 to install and use Linux kernel 4.19 using sudo amazon-linux-extras install kernel-ng and rebooting your instance. archlinux Kernel Version 2019.12.01 5.3.13 \u2705 2. Install BCC Tools Run the following script to download the prerequisites to build BCC tools, building LLVM and Clang targeting BPF byte code, and then building and installing BCC tools. Note We plan to soon support installing bcc-tools from packages instead of compiling them yourself to make taking advantage of enhanced session recording easier. Script to Install BCC Tools Ubuntu and Debian #!/bin/bash # Download LLVM and Clang from the Trusty Repos. VER = trusty echo \"deb http://llvm.org/apt/ $VER / llvm-toolchain- $VER -3.7 main deb-src http://llvm.org/apt/ $VER / llvm-toolchain- $VER -3.7 main\" | \\ sudo tee /etc/apt/sources.list.d/llvm.list wget -O - http://llvm.org/apt/llvm-snapshot.gpg.key | sudo apt-key add - sudo apt-get update sudo apt-get -y install bison build-essential cmake flex git libedit-dev \\ libllvm6.0 llvm-6.0-dev libclang-6.0-dev python zlib1g-dev libelf-dev # Install Linux Kernel Headers sudo apt-get install linux-headers- $( uname -r ) # Install additional tools. sudo apt install arping iperf3 netperf git # Install BCC. export MAKEFLAGS = \"-j16\" git clone https://github.com/iovisor/bcc.git ( cd bcc && git checkout v0.11.0 ) mkdir bcc/build ; cd bcc/build cmake .. -DCMAKE_INSTALL_PREFIX = /usr make sudo make install # Install is done. echo \"Install is complete, try running /usr/share/bcc/tools/execsnoop to verify install.\" CentOS Example Script to install relevant bcc packages for CentOS Follow bcc documentation on how to install the relevant tooling for other operating systems. #!/bin/bash set -e if [[ $EUID -ne 0 ]] ; then echo \"Please run this script as root or sudo.\" exit 1 fi # Create a temporary to build tooling in. BUILD_DIR = $( mktemp -d ) cd $BUILD_DIR echo \"Building in $BUILD_DIR .\" # Install Extra Packages for Enterprise Linux (EPEL) yum install -y epel-release yum update -y # Install development tools. yum groupinstall -y \"Development tools\" yum install -y elfutils-libelf-devel cmake3 git bison flex ncurses-devel # Download and install LLVM and Clang. Build them with BPF target. curl -LO http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz curl -LO http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar -xf cfe-7.0.1.src.tar.xz tar -xf llvm-7.0.1.src.tar.xz mkdir clang-build mkdir llvm-build cd llvm-build cmake3 -G \"Unix Makefiles\" -DLLVM_TARGETS_TO_BUILD = \"BPF;X86\" \\ -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = /usr ../llvm-7.0.1.src make make install cd .. cd clang-build cmake3 -G \"Unix Makefiles\" -DLLVM_TARGETS_TO_BUILD = \"BPF;X86\" \\ -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = /usr ../cfe-7.0.1.src make make install cd .. # Install BCC. git clone https://github.com/iovisor/bcc.git cd bcc && git checkout v0.11.0 mkdir bcc/build ; cd bcc/build cmake3 .. -DCMAKE_INSTALL_PREFIX = /usr make make install # Install is done. rm -fr $BUILD_DIR echo \"Install is complete, try running /usr/share/bcc/tools/execsnoop to verify install.\" Amazon Linux Example Script to install relevant bcc packages for Amazon 2 Linux Make sure the the machine is at Kernel 4.19+/5+ ( uname -r ). Run the following script, sourced from BCC github , to enable BCC in Amazon Linux Extras, install required kernel-devel package for the Kernel version and install the BCC tools. #!/bin/bash #Enable BCC within the Amazon Linux Extras sudo amazon-linux-extras enable BCC #Install the kernal devel package for this kernel sudo yum install -y kernel-devel- $( uname -r ) # Install BCC sudo yum install -y bcc You should see output similar to below: Installed: bcc.x86_64 0:0.10.0-1.amzn2.0.1 Dependency Installed: bcc-tools.x86_64 0:0.10.0-1.amzn2.0.1 python2-bcc.x86_64 0:0.10.0-1.amzn2.0.1 3. Install & Configure Teleport Node Follow our installation instructions to install Teleport Auth, Proxy and Nodes. Set up the Teleport node with this etc/teleport.yaml . See our configuration file setup for more instructions. # Example Config to be saved as etc/teleport.yaml teleport : nodename : graviton-node auth_token : exampletoken auth_servers : # Replace with IP of Teleport Auth server. - 127.0.0.1:3025 data_dir : /var/lib/teleport proxy_service : enabled : false auth_service : enabled : false ssh_service : enabled : true enhanced_recording : # Enable or disable enhanced auditing for this node. Default value: false. enabled : true # Optional: command_buffer_size is optional with a default value of 8 pages. command_buffer_size : 8 # Optional: disk_buffer_size is optional with default value of 128 pages. disk_buffer_size : 128 # Optional: network_buffer_size is optional with default value of 8 pages. network_buffer_size : 8 # Optional: Controls where cgroupv2 hierarchy is mounted. Default value: # /cgroup2. cgroup_path : /cgroup2 4. Test by logging into node via Teleport Session with Enhanced Session Recording will be marked as 'true' in the logs. { \"code\" : \"T2004I\" , \"ei\" : 23 , \"enhanced_recording\" : true , \"event\" : \"session.end\" , \"interactive\" : true , \"namespace\" : \"default\" , \"participants\" : [ \"benarent\" ], \"server_id\" : \"585fc225-5cf9-4e9f-8ff6-1b0fd6885b09\" , \"sid\" : \"ca82b98d-1d30-11ea-8244-cafde5327a6c\" , \"time\" : \"2019-12-12T22:44:46.218Z\" , \"uid\" : \"83e67464-a93a-4c7c-8ce6-5a3d8802c3b2\" , \"user\" : \"benarent\" } 5. Inspect Logs The resulting enhanced session recording will be shown in Teleport's Audit Log . $ teleport-auth ~: tree /var/lib/teleport/log /var/lib/teleport/log \u251c\u2500\u2500 1048a649-8f3f-4431-9529-0c53339b65a5 \u2502 \u251c\u2500\u2500 2020 -01-13.00:00:00.log \u2502 \u2514\u2500\u2500 sessions \u2502 \u2514\u2500\u2500 default \u2502 \u251c\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324-0.chunks.gz \u2502 \u251c\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324-0.events.gz \u2502 \u251c\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324-0.session.command-events.gz \u2502 \u251c\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324-0.session.network-events.gz \u2502 \u2514\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324.index \u251c\u2500\u2500 events.log -> /var/lib/teleport/log/1048a649-8f3f-4431-9529-0c53339b65a5/2020-01-13.00:00:00.log \u251c\u2500\u2500 playbacks \u2502 \u2514\u2500\u2500 sessions \u2502 \u2514\u2500\u2500 default \u2514\u2500\u2500 upload \u2514\u2500\u2500 sessions \u2514\u2500\u2500 default To quickly check the status of the audit log, you can simply tail the logs with tail -f /var/lib/teleport/log/events.log , the resulting capture from Teleport will be a JSON log for each command and network request. json { \"argv\" :[ \"google.com\" ], \"cgroup_id\" : 4294968064 , \"code\" : \"T4000I\" , \"ei\" : 5 , \"event\" : \"session.command\" , \"login\" : \"root\" , \"namespace\" : \"default\" , \"path\" : \"/bin/ping\" , \"pid\" : 2653 , \"ppid\" : 2660 , \"program\" : \"ping\" , \"return_code\" : 0 , \"server_id\" : \"96f2bed2-ebd1-494a-945c-2fd57de41644\" , \"sid\" : \"44c6cea8-362f-11ea-83aa-125400432324\" , \"time\" : \"2020-01-13T18:05:53.919Z\" , \"uid\" : \"734930bb-00e6-4ee6-8798-37f1e9473fac\" , \"user\" : \"benarent\" } json formatted { \"argv\" :[ \"google.com\" ], \"cgroup_id\" : 4294968064 , \"code\" : \"T4000I\" , \"ei\" : 5 , \"event\" : \"session.command\" , \"login\" : \"root\" , \"namespace\" : \"default\" , \"path\" : \"/bin/ping\" , \"pid\" : 2653 , \"ppid\" : 2660 , \"program\" : \"ping\" , \"return_code\" : 0 , \"server_id\" : \"96f2bed2-ebd1-494a-945c-2fd57de41644\" , \"sid\" : \"44c6cea8-362f-11ea-83aa-125400432324\" , \"time\" : \"2020-01-13T18:05:53.919Z\" , \"uid\" : \"734930bb-00e6-4ee6-8798-37f1e9473fac\" , \"user\" : \"benarent\" }","title":"Enhanced Session Rec."},{"location":"features/enhanced-session-recording/#enhanced-session-recording","text":"Teleport SSH and Kubernetes session recording feature captures what is echoed to a terminal. This has inherent advantages, for example because no input is captured, Teleport session recordings typically do not contain passwords that were entered into a terminal. The disadvantage is that session recordings can by bypassed using several techniques: Obfuscation . For example, even though the command echo Y3VybCBodHRwOi8vd3d3LmV4YW1wbGUuY29tCg== | base64 --decode | sh does not contain curl http://www.example.com , when decoded, that is what is run. Shell scripts . For example, if a user uploads and executes a script, the commands run within the script are not captured, simply the output. Terminal controls . Terminals support a wide variety of controls including the ability for users to disable terminal echo. This is frequently used when requesting credentials. Disabling terminal echo allows commands to be run without being captured. Furthermore, due to their unstructured nature, session recordings are difficult to ingest and perform monitoring/alerting on. Note Enhanced Session Recording requires all parts of the Teleport system to be running 4.2+.","title":"Enhanced Session Recording"},{"location":"features/enhanced-session-recording/#requirements","text":"","title":"Requirements:"},{"location":"features/enhanced-session-recording/#1-check-patch-kernel","text":"Teleport 4.2+ with Enhanced Session Recording requires Linux kernel 4.18 (or above) as well as kernel headers. Tip Our Standard Session Recording works with older Linux Kernels. View our audit log docs for more details. You can check your kernel version using the uname command. The output should look something like the following. $ uname -a Linux ip-172-31-43-104.ec2.internal 4 .19.72-25.58.amzn2.x86_64 x86_64 x86_64 x86_64 GNU/Linux","title":"1. Check / Patch Kernel"},{"location":"features/enhanced-session-recording/#operating-system-and-kernel-status-table","text":"Ubuntu Kernel Version 18.04 Bionic Beaver 4.18+ \u2705 20.04 Focal Fossa 5.4 \u2705 CentOS Kernel Version 8.0-1905 4.18.0.80 \u2705 Debian Kernel Version 9 Debian Stretch 4.9.0-6 Patch Kernel 10 Buster 4.19 \u2705 Red Hat Kernel Version Enterprise Linux 8 4.18.0-147 \u2705 Amazon Linux We recommend using Amazon Linux 2 to install and use Linux kernel 4.19 using sudo amazon-linux-extras install kernel-ng and rebooting your instance. archlinux Kernel Version 2019.12.01 5.3.13 \u2705","title":"Operating System and Kernel Status table"},{"location":"features/enhanced-session-recording/#2-install-bcc-tools","text":"Run the following script to download the prerequisites to build BCC tools, building LLVM and Clang targeting BPF byte code, and then building and installing BCC tools. Note We plan to soon support installing bcc-tools from packages instead of compiling them yourself to make taking advantage of enhanced session recording easier.","title":"2. Install BCC Tools"},{"location":"features/enhanced-session-recording/#script-to-install-bcc-tools","text":"Ubuntu and Debian #!/bin/bash # Download LLVM and Clang from the Trusty Repos. VER = trusty echo \"deb http://llvm.org/apt/ $VER / llvm-toolchain- $VER -3.7 main deb-src http://llvm.org/apt/ $VER / llvm-toolchain- $VER -3.7 main\" | \\ sudo tee /etc/apt/sources.list.d/llvm.list wget -O - http://llvm.org/apt/llvm-snapshot.gpg.key | sudo apt-key add - sudo apt-get update sudo apt-get -y install bison build-essential cmake flex git libedit-dev \\ libllvm6.0 llvm-6.0-dev libclang-6.0-dev python zlib1g-dev libelf-dev # Install Linux Kernel Headers sudo apt-get install linux-headers- $( uname -r ) # Install additional tools. sudo apt install arping iperf3 netperf git # Install BCC. export MAKEFLAGS = \"-j16\" git clone https://github.com/iovisor/bcc.git ( cd bcc && git checkout v0.11.0 ) mkdir bcc/build ; cd bcc/build cmake .. -DCMAKE_INSTALL_PREFIX = /usr make sudo make install # Install is done. echo \"Install is complete, try running /usr/share/bcc/tools/execsnoop to verify install.\" CentOS Example Script to install relevant bcc packages for CentOS Follow bcc documentation on how to install the relevant tooling for other operating systems. #!/bin/bash set -e if [[ $EUID -ne 0 ]] ; then echo \"Please run this script as root or sudo.\" exit 1 fi # Create a temporary to build tooling in. BUILD_DIR = $( mktemp -d ) cd $BUILD_DIR echo \"Building in $BUILD_DIR .\" # Install Extra Packages for Enterprise Linux (EPEL) yum install -y epel-release yum update -y # Install development tools. yum groupinstall -y \"Development tools\" yum install -y elfutils-libelf-devel cmake3 git bison flex ncurses-devel # Download and install LLVM and Clang. Build them with BPF target. curl -LO http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz curl -LO http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar -xf cfe-7.0.1.src.tar.xz tar -xf llvm-7.0.1.src.tar.xz mkdir clang-build mkdir llvm-build cd llvm-build cmake3 -G \"Unix Makefiles\" -DLLVM_TARGETS_TO_BUILD = \"BPF;X86\" \\ -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = /usr ../llvm-7.0.1.src make make install cd .. cd clang-build cmake3 -G \"Unix Makefiles\" -DLLVM_TARGETS_TO_BUILD = \"BPF;X86\" \\ -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = /usr ../cfe-7.0.1.src make make install cd .. # Install BCC. git clone https://github.com/iovisor/bcc.git cd bcc && git checkout v0.11.0 mkdir bcc/build ; cd bcc/build cmake3 .. -DCMAKE_INSTALL_PREFIX = /usr make make install # Install is done. rm -fr $BUILD_DIR echo \"Install is complete, try running /usr/share/bcc/tools/execsnoop to verify install.\" Amazon Linux Example Script to install relevant bcc packages for Amazon 2 Linux Make sure the the machine is at Kernel 4.19+/5+ ( uname -r ). Run the following script, sourced from BCC github , to enable BCC in Amazon Linux Extras, install required kernel-devel package for the Kernel version and install the BCC tools. #!/bin/bash #Enable BCC within the Amazon Linux Extras sudo amazon-linux-extras enable BCC #Install the kernal devel package for this kernel sudo yum install -y kernel-devel- $( uname -r ) # Install BCC sudo yum install -y bcc You should see output similar to below: Installed: bcc.x86_64 0:0.10.0-1.amzn2.0.1 Dependency Installed: bcc-tools.x86_64 0:0.10.0-1.amzn2.0.1 python2-bcc.x86_64 0:0.10.0-1.amzn2.0.1","title":"Script to Install BCC Tools"},{"location":"features/enhanced-session-recording/#3-install-configure-teleport-node","text":"Follow our installation instructions to install Teleport Auth, Proxy and Nodes. Set up the Teleport node with this etc/teleport.yaml . See our configuration file setup for more instructions. # Example Config to be saved as etc/teleport.yaml teleport : nodename : graviton-node auth_token : exampletoken auth_servers : # Replace with IP of Teleport Auth server. - 127.0.0.1:3025 data_dir : /var/lib/teleport proxy_service : enabled : false auth_service : enabled : false ssh_service : enabled : true enhanced_recording : # Enable or disable enhanced auditing for this node. Default value: false. enabled : true # Optional: command_buffer_size is optional with a default value of 8 pages. command_buffer_size : 8 # Optional: disk_buffer_size is optional with default value of 128 pages. disk_buffer_size : 128 # Optional: network_buffer_size is optional with default value of 8 pages. network_buffer_size : 8 # Optional: Controls where cgroupv2 hierarchy is mounted. Default value: # /cgroup2. cgroup_path : /cgroup2","title":"3. Install &amp; Configure Teleport Node"},{"location":"features/enhanced-session-recording/#4-test-by-logging-into-node-via-teleport","text":"Session with Enhanced Session Recording will be marked as 'true' in the logs. { \"code\" : \"T2004I\" , \"ei\" : 23 , \"enhanced_recording\" : true , \"event\" : \"session.end\" , \"interactive\" : true , \"namespace\" : \"default\" , \"participants\" : [ \"benarent\" ], \"server_id\" : \"585fc225-5cf9-4e9f-8ff6-1b0fd6885b09\" , \"sid\" : \"ca82b98d-1d30-11ea-8244-cafde5327a6c\" , \"time\" : \"2019-12-12T22:44:46.218Z\" , \"uid\" : \"83e67464-a93a-4c7c-8ce6-5a3d8802c3b2\" , \"user\" : \"benarent\" }","title":"4. Test by logging into node via Teleport"},{"location":"features/enhanced-session-recording/#5-inspect-logs","text":"The resulting enhanced session recording will be shown in Teleport's Audit Log . $ teleport-auth ~: tree /var/lib/teleport/log /var/lib/teleport/log \u251c\u2500\u2500 1048a649-8f3f-4431-9529-0c53339b65a5 \u2502 \u251c\u2500\u2500 2020 -01-13.00:00:00.log \u2502 \u2514\u2500\u2500 sessions \u2502 \u2514\u2500\u2500 default \u2502 \u251c\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324-0.chunks.gz \u2502 \u251c\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324-0.events.gz \u2502 \u251c\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324-0.session.command-events.gz \u2502 \u251c\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324-0.session.network-events.gz \u2502 \u2514\u2500\u2500 fad07202-35bb-11ea-83aa-125400432324.index \u251c\u2500\u2500 events.log -> /var/lib/teleport/log/1048a649-8f3f-4431-9529-0c53339b65a5/2020-01-13.00:00:00.log \u251c\u2500\u2500 playbacks \u2502 \u2514\u2500\u2500 sessions \u2502 \u2514\u2500\u2500 default \u2514\u2500\u2500 upload \u2514\u2500\u2500 sessions \u2514\u2500\u2500 default To quickly check the status of the audit log, you can simply tail the logs with tail -f /var/lib/teleport/log/events.log , the resulting capture from Teleport will be a JSON log for each command and network request. json { \"argv\" :[ \"google.com\" ], \"cgroup_id\" : 4294968064 , \"code\" : \"T4000I\" , \"ei\" : 5 , \"event\" : \"session.command\" , \"login\" : \"root\" , \"namespace\" : \"default\" , \"path\" : \"/bin/ping\" , \"pid\" : 2653 , \"ppid\" : 2660 , \"program\" : \"ping\" , \"return_code\" : 0 , \"server_id\" : \"96f2bed2-ebd1-494a-945c-2fd57de41644\" , \"sid\" : \"44c6cea8-362f-11ea-83aa-125400432324\" , \"time\" : \"2020-01-13T18:05:53.919Z\" , \"uid\" : \"734930bb-00e6-4ee6-8798-37f1e9473fac\" , \"user\" : \"benarent\" } json formatted { \"argv\" :[ \"google.com\" ], \"cgroup_id\" : 4294968064 , \"code\" : \"T4000I\" , \"ei\" : 5 , \"event\" : \"session.command\" , \"login\" : \"root\" , \"namespace\" : \"default\" , \"path\" : \"/bin/ping\" , \"pid\" : 2653 , \"ppid\" : 2660 , \"program\" : \"ping\" , \"return_code\" : 0 , \"server_id\" : \"96f2bed2-ebd1-494a-945c-2fd57de41644\" , \"sid\" : \"44c6cea8-362f-11ea-83aa-125400432324\" , \"time\" : \"2020-01-13T18:05:53.919Z\" , \"uid\" : \"734930bb-00e6-4ee6-8798-37f1e9473fac\" , \"user\" : \"benarent\" }","title":"5. Inspect Logs"},{"location":"features/ssh-pam/","text":"Pluggable Authentication Modules (PAM) Teleport's node service can be configured to integrate with PAM . This allows Teleport to create user sessions using PAM session profiles. Teleport only supports the auth , account and session stack. The auth stack is optional and not used by default. Introduction to Pluggable Authentication Modules Pluggable Authentication Modules (PAM) date back to 1995 when Sun Microsystems implemented a generic authentication framework for Solaris. Since then most GNU/Linux distributions have adopted PAM. $ man pam The Pluggable Authentication Modules (PAM) library abstracts a number of common authentication-related operations and provides a framework for dynamically loaded modules that implement these operations in various ways. Terminology In PAM parlance, the application that uses PAM to authenticate a user is the server, and is identified for configuration purposes by a service name, which is often (but not necessarily) the program name. The user requesting authentication is called the applicant, while the user (usually, root) charged with verifying his identity and granting them the requested credentials is called the arbitrator. The sequence of operations the server goes through to authenticate a user and perform whatever task their requested is a PAM transaction; the context within which the server performs the requested task is called a session. The functionality embodied by PAM is divided into six primitives grouped into four facilities: authentication, account management, session management and password management. Teleport currently supports account management and session management. Setting up PAM on a Linux Machine running Teleport To enable PAM on a given Linux machine, update /etc/teleport.yaml with: ssh_service : pam : # \"no\" by default enabled : true # use /etc/pam.d/sshd configuration (the default) service_name : \"sshd\" # use the \"auth\" modules in the PAM config # \"false\" by default use_pam_auth : true Please note that most Linux distributions come with a number of PAM services in /etc/pam.d and Teleport will try to use sshd by default, which will be removed if you uninstall the openssh-server package. We recommend creating your own PAM service file like /etc/pam.d/teleport and specifying it as service_name above. Setting Message of the Day (motd) with Teleport The file /etc/motd is normally displayed by login(1) after a user has logged in but before the shell is run. It is generally used for important system-wide announcements. This feature can help you inform users that activity on the node is being audited and recorded. Example node with PAM turned off teleport : nodename : graviton-node-1 auth_token : hello auth_servers : - 10.2.1.230:5070 data_dir : /var/lib/teleport proxy_service : enabled : false auth_service : enabled : false ssh_service : enabled : true # configures PAM integration. see below for more details. pam : enabled : false Example node with PAM enabled teleport : nodename : graviton-node-1 auth_token : hello auth_servers : - 10.2.1.230:5070 data_dir : /var/lib/teleport proxy_service : enabled : false auth_service : enabled : false ssh_service : enabled : true # configures PAM integration. see below for more details. pam : enabled : true When PAM is enabled it will use the default sshd config file. This can differ per distro. $ cat /etc/pam.d/sshd # PAM configuration for the Secure Shell service # Standard Un*x authentication. @include common-auth # Disallow non-root logins when /etc/nologin exists. account required pam_nologin.so # Uncomment and edit /etc/security/access.conf if you need to set complex # access limits that are hard to express in sshd_config. # account required pam_access.so # Standard Un*x authorization. @include common-account # SELinux needs to be the first session rule. This ensures that any # lingering context has been cleared. Without this it is possible that a # module could execute code in the wrong domain. session [ success = ok ignore = ignore module_unknown = ignore default = bad ] pam_selinux.so close # Set the loginuid process attribute. session required pam_loginuid.so # Create a new session keyring. session optional pam_keyinit.so force revoke # Standard Un*x session setup and teardown. @include common-session # Print the message of the day upon successful login. # This includes a dynamically generated part from /run/motd.dynamic # and a static (admin-editable) part from /etc/motd. session optional pam_motd.so motd = /run/motd.dynamic session optional pam_motd.so noupdate # Print the status of the user's mailbox upon successful login. session optional pam_mail.so standard noenv # [1] # Set up user limits from /etc/security/limits.conf. session required pam_limits.so # Read environment variables from /etc/environment and # /etc/security/pam_env.conf. session required pam_env.so # [1] # In Debian 4.0 (etch), locale-related environment variables were moved to # /etc/default/locale, so read that as well. session required pam_env.so user_readenv = 1 envfile = /etc/default/locale # SELinux needs to intervene at login time to ensure that the process starts # in the proper default security context. Only sessions which are intended # to run in the user's context should be run after this. session [ success = ok ignore = ignore module_unknown = ignore default = bad ] pam_selinux.so open # Standard Un*x password updating. @include common-password The default sshd will call two pam_motd files, one dynamic. That prints the machine info, and a static MOTD that can be set by an admin. session optional pam_motd.so motd=/run/motd.dynamic session optional pam_motd.so noupdate Below, we show the default admin MOTD. $ cat /etc/motd The programs included with the Debian GNU/Linux system are free software ; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. I've updated this to provide a message to users of Teleport, so they know they are being audited. $ cat /etc/motd WARNING: All activity on this node is being recorded by Teleport Creating local users with Teleport Teleport 4.3 introduced the ability to create local (UNIX) users on login. This is very helpful if you're a large organization and want to provision local users and home directories on the fly. Teleport added the ability to read in PAM environment variables from PAM handle and pass environment variables to PAM modules: TELEPORT_USERNAME , TELEPORT_LOGIN , and TELEPORT_ROLES . Here are some details on the contents of these environment variables which will be set by Teleport: TELEPORT_USERNAME : The Teleport username of the user who is logging into the node. This is usually an email address (such as user@example.com ) if using SAML/OIDC identities with Teleport Enterprise, or a more standard exampleuser if using local Teleport users. TELEPORT_LOGIN : The name of the Linux/UNIX principal which the Teleport user is logging into the Teleport node as - for example root , developer , ubuntu , ec2-user or similar. TELEPORT_ROLES : A space-separated list of Teleport roles which the Teleport user has - for example: developer tester admin . This PAM module creates the user and home directory before attempting to launch a shell for said user. Examples Using pam_exec.so Using pam_exec.so is the easiest way to use the PAM stack to create a user if the user does not already exist. pam_exec.so usually ships with the operating system. You can either add pam_exec.so to the existing PAM stack for your application or write a new one for Teleport. In this example, we'll write a new one to simplify how to use pam_exec.so with Teleport. Start by creating a file /etc/pam.d/teleport with the following contents: account required pam_exec.so /etc/pam-exec.d/teleport_acct session required pam_motd.so session required pam_permit.so Note Pay attention to the inclusion of pam_motd.so under the session facility. While pam_motd.so is not required for user creation, Teleport requires at least one module to be set under both the account and session facilities for it to work. Next, create the script that will be run by pam_exec.so like below. This script will check if the user passed in TELEPORT_LOGIN exists and if it does not, it will create it. Any error from useradd will be written to /tmp/pam.error . Note the additional environment variables TELEPORT_USERNAME , TELEPORT_ROLES , and TELEPORT_LOGIN . These can be used to write richer scripts that may change the system in other ways based on identity information. mkdir -p /etc/pam-exec.d cat > /etc/pam-exec.d/teleport_acct <<EOF #!/bin/sh COMMENT=\"User ${TELEPORT_USERNAME} with roles ${TELEPORT_ROLES} created by Teleport.\" id -u \"${TELEPORT_LOGIN}\" &>/dev/null || /sbin/useradd -m -c \"${COMMENT}\" \"${TELEPORT_LOGIN}\" 2> /tmp/pam.error exit 0 EOF chmod +x /etc/pam-exec.d/teleport_acct Next, update /etc/teleport.yaml to call the above PAM stack by both enabling PAM and setting the service_name. ssh_service : pam : enabled : true service_name : \"teleport\" Now attempting to login as an existing user should result in the creation of the user and a successful login. Additional authentication steps Using the PAM auth modules, it is possible to add additional authentication steps during user login. These can include passwords, 2nd factor or even biometrics. Note that Teleport enables strong SSH authentication out of the box using certificates. For most users, hardening the initial Teleport authentication (e.g. tsh login ) is preferred. By default, auth modules are not used to avoid the default system behavior (usually using local Unix passwords). You can enable them by setting use_pam_auth in the pam section of your teleport.yaml .","title":"Using Teleport with PAM"},{"location":"features/ssh-pam/#pluggable-authentication-modules-pam","text":"Teleport's node service can be configured to integrate with PAM . This allows Teleport to create user sessions using PAM session profiles. Teleport only supports the auth , account and session stack. The auth stack is optional and not used by default.","title":"Pluggable Authentication Modules (PAM)"},{"location":"features/ssh-pam/#introduction-to-pluggable-authentication-modules","text":"Pluggable Authentication Modules (PAM) date back to 1995 when Sun Microsystems implemented a generic authentication framework for Solaris. Since then most GNU/Linux distributions have adopted PAM. $ man pam The Pluggable Authentication Modules (PAM) library abstracts a number of common authentication-related operations and provides a framework for dynamically loaded modules that implement these operations in various ways. Terminology In PAM parlance, the application that uses PAM to authenticate a user is the server, and is identified for configuration purposes by a service name, which is often (but not necessarily) the program name. The user requesting authentication is called the applicant, while the user (usually, root) charged with verifying his identity and granting them the requested credentials is called the arbitrator. The sequence of operations the server goes through to authenticate a user and perform whatever task their requested is a PAM transaction; the context within which the server performs the requested task is called a session. The functionality embodied by PAM is divided into six primitives grouped into four facilities: authentication, account management, session management and password management. Teleport currently supports account management and session management.","title":"Introduction to Pluggable Authentication Modules"},{"location":"features/ssh-pam/#setting-up-pam-on-a-linux-machine-running-teleport","text":"To enable PAM on a given Linux machine, update /etc/teleport.yaml with: ssh_service : pam : # \"no\" by default enabled : true # use /etc/pam.d/sshd configuration (the default) service_name : \"sshd\" # use the \"auth\" modules in the PAM config # \"false\" by default use_pam_auth : true Please note that most Linux distributions come with a number of PAM services in /etc/pam.d and Teleport will try to use sshd by default, which will be removed if you uninstall the openssh-server package. We recommend creating your own PAM service file like /etc/pam.d/teleport and specifying it as service_name above.","title":"Setting up PAM on a Linux Machine running Teleport"},{"location":"features/ssh-pam/#setting-message-of-the-day-motd-with-teleport","text":"The file /etc/motd is normally displayed by login(1) after a user has logged in but before the shell is run. It is generally used for important system-wide announcements. This feature can help you inform users that activity on the node is being audited and recorded.","title":"Setting Message of the Day (motd) with Teleport"},{"location":"features/ssh-pam/#example-node-with-pam-turned-off","text":"teleport : nodename : graviton-node-1 auth_token : hello auth_servers : - 10.2.1.230:5070 data_dir : /var/lib/teleport proxy_service : enabled : false auth_service : enabled : false ssh_service : enabled : true # configures PAM integration. see below for more details. pam : enabled : false","title":"Example node with PAM turned off"},{"location":"features/ssh-pam/#example-node-with-pam-enabled","text":"teleport : nodename : graviton-node-1 auth_token : hello auth_servers : - 10.2.1.230:5070 data_dir : /var/lib/teleport proxy_service : enabled : false auth_service : enabled : false ssh_service : enabled : true # configures PAM integration. see below for more details. pam : enabled : true When PAM is enabled it will use the default sshd config file. This can differ per distro. $ cat /etc/pam.d/sshd # PAM configuration for the Secure Shell service # Standard Un*x authentication. @include common-auth # Disallow non-root logins when /etc/nologin exists. account required pam_nologin.so # Uncomment and edit /etc/security/access.conf if you need to set complex # access limits that are hard to express in sshd_config. # account required pam_access.so # Standard Un*x authorization. @include common-account # SELinux needs to be the first session rule. This ensures that any # lingering context has been cleared. Without this it is possible that a # module could execute code in the wrong domain. session [ success = ok ignore = ignore module_unknown = ignore default = bad ] pam_selinux.so close # Set the loginuid process attribute. session required pam_loginuid.so # Create a new session keyring. session optional pam_keyinit.so force revoke # Standard Un*x session setup and teardown. @include common-session # Print the message of the day upon successful login. # This includes a dynamically generated part from /run/motd.dynamic # and a static (admin-editable) part from /etc/motd. session optional pam_motd.so motd = /run/motd.dynamic session optional pam_motd.so noupdate # Print the status of the user's mailbox upon successful login. session optional pam_mail.so standard noenv # [1] # Set up user limits from /etc/security/limits.conf. session required pam_limits.so # Read environment variables from /etc/environment and # /etc/security/pam_env.conf. session required pam_env.so # [1] # In Debian 4.0 (etch), locale-related environment variables were moved to # /etc/default/locale, so read that as well. session required pam_env.so user_readenv = 1 envfile = /etc/default/locale # SELinux needs to intervene at login time to ensure that the process starts # in the proper default security context. Only sessions which are intended # to run in the user's context should be run after this. session [ success = ok ignore = ignore module_unknown = ignore default = bad ] pam_selinux.so open # Standard Un*x password updating. @include common-password The default sshd will call two pam_motd files, one dynamic. That prints the machine info, and a static MOTD that can be set by an admin. session optional pam_motd.so motd=/run/motd.dynamic session optional pam_motd.so noupdate Below, we show the default admin MOTD. $ cat /etc/motd The programs included with the Debian GNU/Linux system are free software ; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. I've updated this to provide a message to users of Teleport, so they know they are being audited. $ cat /etc/motd WARNING: All activity on this node is being recorded by Teleport","title":"Example node with PAM enabled"},{"location":"features/ssh-pam/#creating-local-users-with-teleport","text":"Teleport 4.3 introduced the ability to create local (UNIX) users on login. This is very helpful if you're a large organization and want to provision local users and home directories on the fly. Teleport added the ability to read in PAM environment variables from PAM handle and pass environment variables to PAM modules: TELEPORT_USERNAME , TELEPORT_LOGIN , and TELEPORT_ROLES . Here are some details on the contents of these environment variables which will be set by Teleport: TELEPORT_USERNAME : The Teleport username of the user who is logging into the node. This is usually an email address (such as user@example.com ) if using SAML/OIDC identities with Teleport Enterprise, or a more standard exampleuser if using local Teleport users. TELEPORT_LOGIN : The name of the Linux/UNIX principal which the Teleport user is logging into the Teleport node as - for example root , developer , ubuntu , ec2-user or similar. TELEPORT_ROLES : A space-separated list of Teleport roles which the Teleport user has - for example: developer tester admin . This PAM module creates the user and home directory before attempting to launch a shell for said user.","title":"Creating local users with Teleport"},{"location":"features/ssh-pam/#examples","text":"Using pam_exec.so Using pam_exec.so is the easiest way to use the PAM stack to create a user if the user does not already exist. pam_exec.so usually ships with the operating system. You can either add pam_exec.so to the existing PAM stack for your application or write a new one for Teleport. In this example, we'll write a new one to simplify how to use pam_exec.so with Teleport. Start by creating a file /etc/pam.d/teleport with the following contents: account required pam_exec.so /etc/pam-exec.d/teleport_acct session required pam_motd.so session required pam_permit.so Note Pay attention to the inclusion of pam_motd.so under the session facility. While pam_motd.so is not required for user creation, Teleport requires at least one module to be set under both the account and session facilities for it to work. Next, create the script that will be run by pam_exec.so like below. This script will check if the user passed in TELEPORT_LOGIN exists and if it does not, it will create it. Any error from useradd will be written to /tmp/pam.error . Note the additional environment variables TELEPORT_USERNAME , TELEPORT_ROLES , and TELEPORT_LOGIN . These can be used to write richer scripts that may change the system in other ways based on identity information. mkdir -p /etc/pam-exec.d cat > /etc/pam-exec.d/teleport_acct <<EOF #!/bin/sh COMMENT=\"User ${TELEPORT_USERNAME} with roles ${TELEPORT_ROLES} created by Teleport.\" id -u \"${TELEPORT_LOGIN}\" &>/dev/null || /sbin/useradd -m -c \"${COMMENT}\" \"${TELEPORT_LOGIN}\" 2> /tmp/pam.error exit 0 EOF chmod +x /etc/pam-exec.d/teleport_acct Next, update /etc/teleport.yaml to call the above PAM stack by both enabling PAM and setting the service_name. ssh_service : pam : enabled : true service_name : \"teleport\" Now attempting to login as an existing user should result in the creation of the user and a successful login.","title":"Examples"},{"location":"features/ssh-pam/#additional-authentication-steps","text":"Using the PAM auth modules, it is possible to add additional authentication steps during user login. These can include passwords, 2nd factor or even biometrics. Note that Teleport enables strong SSH authentication out of the box using certificates. For most users, hardening the initial Teleport authentication (e.g. tsh login ) is preferred. By default, auth modules are not used to avoid the default system behavior (usually using local Unix passwords). You can enable them by setting use_pam_auth in the pam section of your teleport.yaml .","title":"Additional authentication steps"}]}